[["index.html", "Data Wrangling with R Syllabus Learning Objectives Material Class Structure Schedule Conventions used in this book Feedback Acknowledgements", " Data Wrangling with R Bradley Boehmke Syllabus This repository contains additional resources for the UC BANA 7025 Data Wrangling course. The following is a truncated syllabus; for the full syllabus along with complete course content please visit the online course content in Canvas. Welcome to Data Wrangling with R! This course provides an intensive, hands-on introduction to Data Wrangling with the R programming language. You will learn the fundamental skills required to acquire, munge, transform, manipulate, and visualize data in a computing environment that fosters reproducibility. Data wrangling, which is also commonly referred to as data munging, transformation, manipulation, janitor work, etc. can be a painstakingly laborious process. In fact, it has been stated that up to 80% of data analysis is spent on the process of cleaning and preparing data (Wickham 2014; Dasu and Johnson 2003). However, being a prerequisite to the rest of the data analysis workflow (visualization, modeling, reporting), it’s essential that you become fluent and efficient in data wrangling techniques. Learning Objectives This course will guide you through the data wrangling process along with give you a solid foundation of the basics of working with data in R. My goal is to teach you how to easily wrangle your data so you can spend more time focused on understanding the content of your data via visualization, modeling, and reporting your results. Upon successfully completing this course, you will be able to: Perform your data analysis in a literate programming environment Manage different types of data Manage different data structures Import and export data Index, subset, reshape and transform your data Compute descriptive statistics Visualize data Make your code efficient by using control statements &amp; iteration Write your own functions …all with R! This course assumes no prior knowledge of R. Experience with programming concepts or another programming language will help, but is not required to understand the material. Material The bulk of the classroom material will be provided via this book, the recorded lectures, and class notes. In some cases there are additional recommended readings, all of which are readily available online. Class Structure Modules: For this class each module is covered over the course of week. In the “Overview” section for each module you will find overall learning objectives, a short description of the learning content covered in that module, along with all tasks that are required of you for that module (i.e. quizzes, lab). Each module will have two or more primary lessons and associated quizzes along with a lab. Lessons: For each lesson you will read and work through the tutorial. Short videos will be sprinkled throughout the lesson to further discuss and reinforce lesson concepts. Each lesson will have various “Your Turn” exercises throughout, along with end-of-lesson exercises. I highly recommend you work through these exercises as they will prepare you for the quizzes, labs, and project work. Quizzes: There will be a short quiz associated with each lesson. These quizzes will be hosted in the course website on Canvas. Please check Canvas for due dates for these quizzes. Labs: There will be a lab associated with each module. For these labs students will be guided through a case study step-by-step. The aim is to provide a detailed view on how to manage a variety of complex real-world data; how to convert real problems into data wrangling and analysis problems; and to apply R to address these problems and extract insights from the data. Submission of these labs will be done through the course website on Canvas. Please check Canvas for due dates for these labs. Project: TBD Schedule See the Canvas course webpage for a detailed schedule with due dates for quizzes, labs, etc. Module Description 1 Introduction Intro to data wrangling, R, and this course R fundamentals &amp; the Rstudio IDE 2 Reproducible Documents and Importing Data Managing your workflow and reproducibility Importing data and understanding the basics of it 3 Tidy Data and Data Manipulation Tidying &amp; preparing data for analysis Data manipulation 4 Relational Data and More Tidyverse Packages Relational data Leveraging the Tidyverse to simplify data wrangling 5 Data Visualization &amp; Exploration Data visualization Exploratory data analysis 6 Creating Efficient Code in R Control statements &amp; iteration Writing functions 7 Introduction to Applied Modeling Correlation &amp; pattern discovery Introduction to machine learning Conventions used in this book The following typographical conventions are used in this book: strong italic: indicates new terms, bold: indicates package &amp; file names, inline code: monospaced highlighted text indicates functions or other commands that could be typed literally by the user, code chunk: indicates commands or other text that could be typed literally by the user 1 + 2 ## [1] 3 In addition to the general text used throughout, you will notice the following cells that provide additional context for improved learning: A tip or suggestion that will likely produce better results. A general note that could improve your understanding but is not required for the course requirements. Warning or caution to look out for. Knowledge check exercises to gauge your learning progress. Feedback To report errors or bugs that you find in this course material please post an issue at https://github.com/bradleyboehmke/uc-bana-7025/issues. For all other communication be sure to use Canvas or the university email. When communicating with me via email, please always include BANA7025 in the subject line. Acknowledgements This course and its materials have been influenced by the following resources: Jenny Bryan, STAT 545: Data wrangling, exploration, and analysis with R Garrett Grolemund &amp; Hadley Wickham, R for Data Science Stephanie Hicks, Statistical Computing Chester Ismay &amp; Albert Kim, ModernDive Alex Douglas et al., An Introduction to R References "],["overview.html", "1 Overview 1.1 Learning objectives 1.2 Tasks 1.3 Course readings", " 1 Overview Welcome to module 1! The focus of this module is to get you acquainted with the concept of data wrangling, the R programming language, and to get you performing some foundational programming tasks with R. 1.1 Learning objectives By the end of this module you should be able to: Explain the key components of data wrangling. Discuss the background and benefits of the R programming language. Understand how to install, open, and start using R with the RStudio IDE. Perform fundamental tasks in R such as assignment, evaluation, using R as a calculator, and installing packages. 1.2 Tasks TBD 1.3 Course readings TBD "],["lesson-1a-course-overview.html", "2 Lesson 1a: Course overview 2.1 Learning objectives 2.2 Purpose of this course 2.3 Assumptions &amp; Pre-requisites 2.4 Course Staff 2.5 Course Logistics 2.6 Grading 2.7 Getting Help 2.8 Fine Print 2.9 Exercises", " 2 Lesson 1a: Course overview TBD – complete this lesson towards the end of course dev 2.1 Learning objectives By the end of this lesson you will: Be able to explain the purpose and objectives of this course. Understand the assumptions and prerequisites that are expected for this course. Understand how this course flows and how to complete the required tasks (i.e. quizzes, labs). Know the grading philosophy and relative weights across the various tasks. Know how, when, and where to get help for course specific concerns. 2.2 Purpose of this course 2.3 Assumptions &amp; Pre-requisites 2.4 Course Staff 2.5 Course Logistics 2.6 Grading 2.7 Getting Help 2.8 Fine Print 2.9 Exercises "],["lesson-1b-r-rstudio.html", "3 Lesson 1b: R &amp; RStudio 3.1 Learning objectives 3.2 R vs. RStudio 3.3 Installing R and RStudio 3.4 Understanding the RStudio IDE 3.5 Getting help 3.6 Errors, warnings, and messages 3.7 Exercises", " 3 Lesson 1b: R &amp; RStudio This lesson is designed to help you get up and running with R and RStudio. It will also provide you with some tips for getting help as you start writing R code and differentiating errors, warnings, and messages (because we will experience these daily!). 3.1 Learning objectives Upon completing this module you will: Understand the difference between R and RStudio. Have R and RStudio installed on your computer. Be able to navigate the RStudio IDE. Know how to get help as you first start learning R. Differentiate errors, warnings, and messages. Know what additional resources are available to go deeper. 3.2 R vs. RStudio When students first learn about R and RStudio, they are often confused on how the two differentiate. R is a programming language that runs computations, while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. An analogy used by Ismay and Kim (2019) helps put this into perspective. “R is like a car’s engine while RStudio is like a car’s dashboard…just as the way of having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well.” Figure 3.1: Analogy of difference between R and RStudio (Ismay and Kim 2019) It is important to note that one can write R and do nearly everything you are going to do in this course without using RStudio. However, RStudio has lots of features that make writing, executing, and organizing your R code so much easier. Moreover, realize that there are other IDEs and platforms that you can use (i.e. VS code, Radiant, jupyter) to write and execute R code; however, RStudio has become the defacto IDE for R and is heavily used throughout the R community. Since RStudio has lots of features, it takes time to learn them. A good resource to learn more about RStudio are the R Studio Essentials collection of videos. 3.3 Installing R and RStudio It is important that you install R first and then install RStudio. You can download and install R from CRAN, the Comprehensive R Archive Network. It is highly recommended to install a precompiled binary distribution for your operating system (OS); follow these instructions: Go to https://cloud.r-project.org/ Select the installer for your operating system: If you are a Windows user: Click on “Download R for Windows”, then click on “base”, then click on the Download link. If you are macOS user: Click on “Download R for (Mac) OS X”, then under “Latest release:” click on R-X.X.X.pkg, where R-X.X.X is the version number. For example, the latest version of R as of April 25, 2022 was R-4.2.0. If you are a Linux user: Click on “Download R for Linux” and choose your distribution for more information on installing R for your setup. Follow the instructions of the installer. Next, you can download RStudio’s IDE by following these instructions: Go to RStudio for desktop https://www.rstudio.com/products/rstudio/download/ Scroll down to “Installers for Supported Platforms” near the bottom of the page. Select the install file for your OS. Follow the instructions of the installer. Installing R and RStudio should be fairly straightforward. However, a great set of detailed step-by-step instructions is in Rafael Irizarry’s Introduction to Data Science book (Irizarry 2019). After you install R and RStudio on your computer, you’ll have two new programs (also called applications) you can open. The R application icon looks like the left image below while the RStudio icon looks like the right image. We’ll always work in RStudio and not in the R application so be sure to select the correct icon. Figure 3.2: Use the RStudio icon (right) and not the R icon (left) to launch your R project. (Ismay and Kim 2019) If your OS allows, pin the RStudio icon to your desktop or Dock and keep the R icon in another location (i.e. Applications for Mac users). This makes it natural and efficient to always go to the RStudio icon on your desktop for launching the program. After you open RStudio, you should see something similar to the following image. Note that slight differences might exist depending on the version of your RStudio interface. Figure 3.3: RStudio interface to R. You are now ready to start programming! 3.4 Understanding the RStudio IDE The RStudio IDE is where all the action happens. There are four fundamental windows in the IDE, each with their own purpose. I discuss each briefly below but I highly suggest Oscar Torres-Reyna’s Introduction to RStudio for a thorough understanding of the console1. Figure 3.4: The four fundamental windows within the RStudio IDE 3.4.1 Script Editor The top left window is where your script files will display. There are multiple forms of script files but the basic one to start with is the .R file. To create a new file you use the File » New File menu. To open an existing file you use either the File » Open File… menu or the Recent Files menu to select from recently opened files. RStudio’s source editor includes a variety of productivity enhancing features including syntax highlighting, code completion, multiple-file editing, and find/replace. A good introduction to the script editor was written by RStudio’s Josh Paulson2. The script editor is a great place to put code you care about. Keep experimenting in the console, but once you have written code that works and does what you want, put it in the script editor. RStudio will automatically save the contents of the editor when you quit RStudio, and will automatically load it when you re-open. Nevertheless, it’s a good idea to save your scripts regularly and to back them up. To execute the code in the script editor you have two options: Place the cursor on the line that you would like to execute and execute Cmd/Ctrl + Enter. Alternatively, you could hit the Run button in the toolbar. If you want to run all lines of code in the script then you can highlight the lines you want to run and then execute one of the options in #1. Figure 3.5: Execute lines of code in your script with Cmd/Ctrl + Enter or using the Run button. 3.4.2 Workspace Environment The top right window is the workspace environment which captures much of your your current R working environment and includes any user-defined objects (vectors, matrices, data frames, lists, functions). When saving your R working session, these are the components along with the script files that will be saved in your working directory, which is the default location for all file inputs and outputs. To get or set your working directory so you can direct where your files are saved: # returns path for the current working directory getwd() # set the working directory to a specified directory setwd(&quot;path/of/directory&quot;) For example, if I call getwd() the file path “/Users/b294776/Desktop” is returned. If I want to set the working directory to the “Workspace” folder within the “Desktop” directory I would use setwd(\"/Users/b294776/Desktop/Workspace\"). Now if I call getwd() again it returns “/Users/b294776/Desktop/Workspace”. An alternative solution is to go to the following location in your toolbar Session » Set Working Directory » Choose Directory and select the directory of choice (much easier!). The workspace environment will also list your user defined objects such as vectors, matrices, data frames, lists, and functions. For example, if you type the following in your console: x &lt;- 2 y &lt;- 3 You will now see x and y listed in your workspace environment. To identify or remove the objects (i.e. vectors, data frames, user defined functions, etc.) in your current R environment: # list all objects ls() # identify if an R object with a given name is present exists(&quot;x&quot;) # remove defined object from the environment rm(x) # you can remove multiple objects rm(x, y) # basically removes everything in the working environment -- use with caution! rm(list = ls()) If you have a lot of objects in your workspace environment you can use the 🧹 icon in the workspace environment tab to clear out everything. You can also view previous commands in the workspace environment by clicking the History tab, by simply pressing the up arrow on your keyboard, or by typing into the console: # default shows 25 most recent commands history() # show 100 most recent commands history(100) # show entire saved history history(Inf) 3.4.3 Console The bottom left window contains the console. You can code directly in this window but it will not save your code. It is best to use this window when you are simply wanting to perform calculator type functions. This is also where your outputs will be presented when you run code in your script. Go ahead and type the following in your console: 2 * 3 + 8 / 2 3.4.4 Misc. Displays The bottom right window contains multiple tabs. The Files tab allows you to see which files are available in your working directory. The Plots tab will display any plots/graphics that are produced by your code. The Packages tab will list all packages downloaded to your computer and also the ones that are loaded (more on this later). And the Help tab allows you to search for topics you need help on and will also display any help responses (more on this later as well). 3.4.5 Workspace Options &amp; Shortcuts There are multiple options available for you to set and customize both R and your RStudio console. For R, you can read about, and set, available options for the current R session with the following code. For now you don’t need to worry about making any adjustments, just know that many options do exist. # learn about available options help(options) # view current option settings options() # change a specific option (i.e. number of digits to print on output) options(digits=3) For a thorough tutorial regarding the RStudio console and how to customize different components check out this tutorial. You can also find the RStudio console cheatsheet shown below by going to Help menu » Cheatsheets. As with most computer programs, there are numerous keyboard shortcuts for working with the console. To access a menu displaying all the shortcuts in RStudio you can use option + shift + k. Within RStudio you can also access them in the Help menu » Keyboard Shortcuts Help. Figure 3.6: RStudio IDE cheat sheet. 3.4.6 Knowledge check Identify which working directory you are working out of. Create a folder on your computer titled Learning R. Within RStudio, set your working directory to this folder. Type pi in the console. Set the option to show 8 digits. Re-type pi in the console. Type ?pi in the console. Note that documentation on this object pops up in the Help tab in the Misc. Display. Now check out your code History tab. Create a new .R file and save this as my-first-script (note how this now appears in your Learning R folder). Type pi in line 1 of this script, option(digits = 8) in line 2, and pi again in line three. Execute this code one line at a time and then re-execute all lines at once. 3.5 Getting help The help documentation and support in R is comprehensive and easily accessible from the the console. 3.5.1 General Help To leverage general help resources you can use: # provides general help links help.start() # searches the help system for documentation matching a given character string help.search(&quot;linear regression&quot;) Note that the help.search(\"some text here\") function requires a character string enclosed in quotation marks. So if you are in search of time series functions in R, using help.search(\"time series\") will pull up a healthy list of vignettes and code demonstrations that illustrate packages and functions that work with time series data. For more direct help on functions that are installed on your computer you can use the following. Test these out in your console: help(mean) # provides details for specific function ?mean # provides same information as help(functionname) example(mean) # provides examples for said function Note that the help() and ? function calls only work for functions within loaded packages. You’ll understand what this means shortly. 3.5.2 Getting Help From the Web Typically, a problem you may be encountering is not new and others have faced, solved, and documented the same issue online. The following resources can be used to search for online help. Although, I typically just Google the problem and find answers relatively quickly. RSiteSearch(\"key phrase\"): searches for the key phrase in help manuals and archived mailing lists on the R Project website. Stack Overflow: a searchable Q&amp;A site oriented toward programming issues. 75% of my answers typically come from Stack Overflow. Cross Validated: a searchable Q&amp;A site oriented toward statistical analysis. RStudio Community: a community for all things R and RStudio where you can get direct answers to your problems and also give back by helping to solve and answer other’s questions. R-seek: a Google custom search that is focused on R-specific websites R-bloggers: a central hub of content collected from over 500 bloggers who provide news and tutorials about R. Twitter has a thriving R community (#rstats) and is definitely worth following. However, it is not the place to ask for help on specific code functionality. 3.5.3 Knowledge check Does help.start() provide a link to an introduction to R manual? Does R’s mode function compute the statistical mode? Review at least 5 Stack Overflow questions about R. 3.6 Errors, warnings, and messages One thing that intimidates new R users (and new programmers in general) is when they run into errors, warnings, and messages. R reports errors, warnings, and messages in the following way. Don’t worry about what the code is doing, just get familiar with the differences in how errors, warnings, and messages are reported. Errors: When the red text is a legitimate error, it will be prefaced with “Error in…” and will try to explain what went wrong. Generally when there’s an error, the code will not run. Think of errors as a red traffic light: something is wrong! For references on common errors, check out the following links by Noam Ross, David Smith, and Chester Ismay. # Example of an error 1 + &#39;a&#39; ## Error in 1 + &quot;a&quot;: non-numeric argument to binary operator Warnings: Warning messages will be prefaced with “Warning:” and R will try to explain why there’s a warning. Generally your code will still work, but with some caveats. Think of warnings as a yellow traffic light: everything is working, but watch out/pay attention. Typically when you get a warning you want to figure out what is throwing the warning because the end result may not be what you had intended! x &lt;- 1:2 y &lt;- 1:3 # Example of a warning x + y ## Warning in x + y: longer object length is not a multiple of shorter object length ## [1] 2 4 4 Messages: When the text doesn’t start with either “Error” or “Warning”, it’s just a friendly message. You’ll often see these messages when you load R packages or make certain function calls. Think of messages as a green traffic light: everything is working fine and keep on going! # Example of a message library(completejourney) ## Welcome to the completejourney package! Learn more about these data ## sets at http://bit.ly/completejourney. 3.7 Exercises What differentiates R and RStudio? What are the four main panes in RStudio? What function tells you what directory your are operating in? How could you change the directory to a new folder? How could you access R documentation to learn more about the lm() function? References "],["lesson-1c-r-fundamentals.html", "4 Lesson 1c: R fundamentals 4.1 Learning objectives 4.2 Assignment &amp; evaluation 4.3 R as a calculator 4.4 Working with packages 4.5 Style guide 4.6 Exercises", " 4 Lesson 1c: R fundamentals This lesson serves to introduce you to many of the basics of R to get you comfortable. This includes understanding how to assign and evaluate expressions, performing basic calculator-like activities, the idea of vectorization and packages. Finally, I offer some basic styling guidelines to help you write code that is easier to digest by others. 4.1 Learning objectives Upon completing this module you will be able to: Assign values to variables and evaluate them. Perform basic mathematical operations. Explain what packages and be able to install and load them. Understand and apply basic styling guidelines to your code. 4.2 Assignment &amp; evaluation 4.2.1 Assignment The first operator you’ll run into is the assignment operator. The assignment operator is used to assign a value. For instance we can assign the value 3 to the variable x using the &lt;- assignment operator. # idiomatic assignment x &lt;- 3 R is a dynamically typed programming language which means it will perform the process of verifying and enforcing the constraints of types at run-time. If you are unfamiliar with dynamically versus statically-typed languages then do not worry about this detail. Just realize that dynamically typed languages allow for the simplicity of running the above command and R automatically inferring that 3 should be a numeric type rather than a character string. Interestingly, R actually allows for five assignment operators3: # leftward assignment x &lt;- value x = value x &lt;&lt;- value # rightward assignment value -&gt; x value -&gt;&gt; x The original assignment operator in R was &lt;- and has continued to be the preferred among R users. The = assignment operator was added in 2001 primarily because it is the accepted assignment operator in many other languages and beginners to R coming from other languages were so prone to use it. Using = is not wrong, just realize that most R programmers prefer to keep = reserved for argument association and use &lt;- for assignment. The operators &lt;&lt;- is normally only used in functions or looping constructs which we will not get into the details. And the rightward assignment operators perform the same as their leftward counterparts, they just assign the value in an opposite direction. Overwhelmed yet? Don’t be. This is just meant to show you that there are options and you will likely come across them sooner or later. My suggestion is to stick with the tried, true, and idiomatic &lt;- operator. This is the most conventional assignment operator used and is what you will find in all the base R source code…which means it should be good enough for you. 4.2.2 Evaluation We can then evaluate the variable by simply typing x at the command line which will return the value of x. Note that prior to the value returned you’ll see ## [1] in the console. This simply implies that the output returned is the first output. Note that you can type any comments in your code by preceding the comment with the hash tag (#) symbol. Any values, symbols, and texts following # will not be evaluated. # evaluation x ## [1] 3 4.2.3 Case Sensitivity Lastly, note that R is a case sensitive programming language. Meaning all variables, functions, and objects must be called by their exact spelling: x &lt;- 1 y &lt;- 3 z &lt;- 4 x * y * z ## [1] 12 x * Y * z ## Error in eval(expr, envir, enclos): object &#39;Y&#39; not found 4.2.4 Knowledge check Assign the value 5 to variable x (note how this shows up in your Global Environment). Assign the character “abc” to variable y. Evaluate the value of x and y at in the console. Now use the rm() function to remove these objects from you working environment. 4.3 R as a calculator 4.3.1 Basic Arithmetic At its most basic function R can be used as a calculator. When applying basic arithmetic, the PEMDAS order of operations applies: parentheses first followed by exponentiation, multiplication and division, and finally addition and subtraction. 8 + 9 / 5 ^ 2 ## [1] 8.36 8 + 9 / (5 ^ 2) ## [1] 8.36 8 + (9 / 5) ^ 2 ## [1] 11.24 (8 + 9) / 5 ^ 2 ## [1] 0.68 By default R will display seven digits but this can be changed using options() as previously outlined. 1 / 7 ## [1] 0.14286 options(digits = 3) 1 / 7 ## [1] 0.143 Also, large numbers will be expressed in scientific notation which can also be adjusted using options(). 888888 * 888888 ## [1] 7.9e+11 options(digits = 10) 888888 * 888888 ## [1] 790121876544 Note that the largest number of digits that can be displayed is 22. Requesting any larger number of digits will result in an error message. pi ## [1] 3.141592654 options(digits = 22) pi ## [1] 3.141592653589793115998 options(digits = 23) ## Error in options(digits = 23): invalid &#39;digits&#39; parameter, allowed 1...22 pi ## [1] 3.141592653589793115998 We can also perform integer divide (%/%) and modulo (%%) functions. The integer divide function will give the integer part of a fraction while the modulo will provide the remainder. # regular division 42 / 4 ## [1] 10.5 # integer division 42 %/% 4 ## [1] 10 # modulo (remainder) 42 %% 4 ## [1] 2 4.3.2 Miscellaneous Mathematical Functions There are many built-in functions to be aware of. These include but are not limited to the following. Go ahead and run this code in your console. x &lt;- 10 abs(x) # absolute value sqrt(x) # square root exp(x) # exponential transformation log(x) # logarithmic transformation cos(x) # cosine and other trigonometric functions 4.3.3 Infinite, and NaN Numbers When performing undefined calculations, R will produce Inf (infinity) and NaN (not a number) outputs. These can easily pop up in regular data wrangling tasks and later modules will discuss how to work with these types of outputs along with missing values. # infinity 1 / 0 ## [1] Inf # infinity minus infinity Inf - Inf ## [1] NaN # negative infinity -1 / 0 ## [1] -Inf # not a number 0 / 0 ## [1] NaN # square root of -9 sqrt(-9) ## Warning in sqrt(-9): NaNs produced ## [1] NaN 4.3.4 Knowledge check Assign the values 1000, 5, and 0.05 to variables D, K, and h respectively. Compute \\(2 \\times D \\times K\\). Compute \\(\\frac{2 \\times D \\times K}{h}\\). Now put this together to compute the Economic Order Quantity, which is \\(\\sqrt{\\frac{2 \\times D \\times K}{h}}\\). Save the output as Q. 4.4 Working with packages In R, the fundamental unit of shareable code is the package. A package bundles together code, data, documentation, and tests and provides an easy method to share with others (Wickham 2015). As of April 2022 there were nearly 19,000 packages available on CRAN, 2,000 on Bioconductor, and countless more available through GitHub. This huge variety of packages is one of the reasons that R is so successful: chances are that someone has already solved a problem that you’re working on, and you can benefit from their work by downloading and using their package. 4.4.1 Installing Packages The most common place to get packages is from CRAN. To install packages from CRAN you use install.packages(\"packagename\"). For instance, if you want to install the ggplot2 package, which is a very popular visualization package you would type the following in the console: # install package from CRAN install.packages(&quot;ggplot2&quot;) As previously stated, packages are also available through Bioconductor and GitHub. Bioconductor provides R packages primarily for genomic data analyses and packages on GitHub are usually under development but have not gone through all the checks and balances to be loaded onto CRAN (aka download and use these packages at your discretion). You can learn how to install Bioconductor packages here and GitHub packages here. 4.4.2 Loading Packages Once the package is downloaded to your computer you can access the functions and resources provided by the package in two different ways: # load the package to use in the current R session library(packagename) # use a particular function within a package without loading the package packagename::functionname For instance, if you want to have full access to the dplyr package you would use library(dplyr); however, if you just wanted to use the filter() function which is provided by the dplyr package without fully loading dplyr you can use dplyr::filter(...)4. 4.4.3 Getting Help on Packages For more direct help on packages that are installed on your computer you can use the help and vignette functions. Here we can get help on the ggplot2 package with the following: help(package = &quot;ggplot2&quot;) # provides details regarding contents of a package vignette(package = &quot;ggplot2&quot;) # list vignettes available for a specific package vignette(&quot;ggplot2-specs&quot;) # view specific vignette vignette() # view all vignettes on your computer Note that some packages will have multiple vignettes. For instance vignette(package = \"grid\") will list the 13 vignettes available for the grid package. To access one of the specific vignettes you simply use vignette(\"vignettename\"). 4.4.4 Useful Packages There are thousands of helpful R packages for you to use, but navigating them all can be a challenge. To help you out, RStudio compiled a guide to some of the best packages for loading, manipulating, visualizing, analyzing, and reporting data. In addition, their list captures packages that specialize in spatial data, time series and financial data, increasing spead and performance, and developing your own R packages. 4.4.5 Knowledge check Install the completejourney package. Load the completejourney package. Access the help documentation for the completejourney package. Check out the vignette(s) for completejourney. Call the get_transactions() function provided by completejourney and save the output to a transactions variable (note that this takes a little time as you are trying to download 1.5 million transactions!). 4.4.6 Tidyverse The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures; and this group of packages has become extremely popular and common across the data science community. You will learn about many of these packages throughout this course. Take a little time to familiarize yourself with the Tidyverse. Not only will you find high-level descriptions of the different tidyverse packages but you will also find a lot of educational content that you can and should take advantage of! Figure 4.1: Tidyverse is a collection of packages designed to simplify many tasks throughout the data analysis process. Let’s go ahead and install the tidyverse package, which will actually install a bunch of other packages for us. install.packages(&quot;tidyverse&quot;) The single line of code above 👆… is equivalent to running the 29 lines of code below 👇! install.packages(&quot;ggplot2&quot;) install.packages(&quot;tibble&quot;) install.packages(&quot;tidyr&quot;) install.packages(&quot;readr&quot;) install.packages(&quot;purrr&quot;) install.packages(&quot;dplyr&quot;) install.packages(&quot;stringr&quot;) install.packages(&quot;forcats&quot;) install.packages(&quot;cli&quot;) install.packages(&quot;crayon&quot;) install.packages(&quot;dbplyr&quot;) install.packages(&quot;dtplyr&quot;) install.packages(&quot;googledrive&quot;) install.packages(&quot;googlesheets4&quot;) install.packages(&quot;haven&quot;) install.packages(&quot;hms&quot;) install.packages(&quot;httr&quot;) install.packages(&quot;jsonlite&quot;) install.packages(&quot;lubridate&quot;) install.packages(&quot;magrittr&quot;) install.packages(&quot;modelr&quot;) install.packages(&quot;pillar&quot;) install.packages(&quot;readxl&quot;) install.packages(&quot;reprex&quot;) install.packages(&quot;rlang&quot;) install.packages(&quot;rstudioapi&quot;) install.packages(&quot;rvest&quot;) install.packages(&quot;xml2&quot;) If we load the tidyverse package we will see that it loads 8 packages for us: ggplot2_, tibble, tidyr, readr, purrr, dplyr, stringr, and forcats. These are packages that we will tend to use in almost any data analysis project. library(tidyverse) The single line of code above 👆… is equivalent to running the 8 lines of code below 👇! library(ggplot2) library(tibble) library(tidyr) library(readr) library(purrr) library(dplyr) library(stringr) library(forcats) 4.5 Style guide “Good coding style is like using correct punctuation. You can manage without it, but it sure makes things easier to read.” - Hadley Wickham As a medium of communication, its important to realize that the readability of code does in fact make a difference. Well styled code has many benefits to include making it easy to i) read, ii) extend, and iii) debug. Unfortunately, R does not come with official guidelines for code styling but such is an inconvenient truth of most open source software. However, this should not lead you to believe there is no style to be followed and over time implicit guidelines for proper code styling have been documented. What follows are a few of the basic guidelines from the tidyverse style guide. These suggestions will help you get started with good styling suggestions as you begin this book but as you progress you should leverage the far more detailed tidyverse style guide along with useful packages such as lintr and styler to help enforce good code syntax on yourself. 4.5.1 Notation and Naming File names should be meaningful and end with a .R extension. # Good weather-analysis.R emerson-text-analysis.R lesson-1b-homework.R # Bad basic-stuff.r detail.r If files need to be run in sequence, prefix them with numbers: 0-download.R 1-preprocessing.R 2-explore.R 3-fit-model.R In R, naming conventions for variables and functions are famously muddled. They include the following: namingconvention # all lower case; no separator naming.convention # period separator naming_convention # underscore separator (aka snake case) namingConvention # lower camel case NamingConvention # upper camel case Historically, there has been no clearly preferred approach with multiple naming styles sometimes used within a single package. Bottom line, your naming convention will be driven by your preference but the ultimate goal should be consistency. Vast majority of the R community uses lowercase with an underscore (“_“) to separate words within a variable/function name (‘snake_case’). Furthermore, variable names should be nouns and function names should be verbs to help distinguish their purpose. Also, refrain from using existing names of functions (i.e. mean, sum, true). 4.5.2 Organization Organization of your code is also important. There’s nothing like trying to decipher 500 lines of code that has no organization. The easiest way to achieve organization is to comment your code. When you have large sections within your script you should separate them to make it obvious of the distinct purpose of the code. # Download Data ------------------------------------------------------------------- lines of code here # Preprocess Data ----------------------------------------------------------------- lines of code here # Exploratory Analysis ------------------------------------------------------------ lines of code here You can easily add these section breaks within RStudio wth Cmd+Shift+R. Then comments for specific lines of code can be done as follows: code_1 # short comments can be placed to the right of code code_2 # blah code_3 # blah # or comments can be placed above a line of code code_4 # Or extremely long lines of commentary that go beyond the suggested 80 # characters per line can be broken up into multiple lines. Just don&#39;t forget # to use the hash on each. code_5 You can easily comment or uncomment lines by highlighting the line and then pressing Cmd+Shift+C. 4.5.3 Syntax The maximum number of characters on a single line of code should be 80 or less. If you are using RStudio you can have a margin displayed so you know when you need to break to a new line5. Also, when indenting your code use two spaces rather than using tabs. The only exception is if a line break occurs inside parentheses. In this case it is common to do either of the following: # option 1 super_long_name &lt;- seq(ymd_hm(&quot;2015-1-1 0:00&quot;), ymd_hm(&quot;2015-1-1 12:00&quot;), by = &quot;hour&quot;) # option 2 super_long_name &lt;- seq( ymd_hm(&quot;2015-1-1 0:00&quot;), ymd_hm(&quot;2015-1-1 12:00&quot;), by = &quot;hour&quot; ) Proper spacing within your code also helps with readability. Place spaces around all infix operators (=, +, -, &lt;-, etc.). The same rule applies when using = in function calls. Always put a space after a comma, and never before. # Good average &lt;- mean(feet / 12 + inches, na.rm = TRUE) # Bad average&lt;-mean(feet/12+inches,na.rm=TRUE) There’s a small exception to this rule: :, :: and ::: don’t need spaces around them. # Good x &lt;- 1:10 base::get # Bad x &lt;- 1 : 10 base :: get It is important to think about style when communicating any form of language. Writing code is no exception and is especially important if your code will be read by others. Following these basic style guides will get you on the right track for writing code that can be easily communicated to others. 4.5.4 Knowledge check Review chapters 1 &amp; 2 in the Tidyverse Style Guide. Go back through the script you’ve been writing to execute the exercises in this module and make sure your naming conventions are consistent, your code is nicely organized and annotated, your syntax includes proper spacing. 4.6 Exercises Say you have a 12” pizza. Compute the area of the pizza and assign that value to the variable area. Now say the cost of the pizza was $8. Compute the cost per square inch and assign that value to a variable ppsi. Based on the style guide section rename the ppsi variable in question 1 to be more meaningful. If you did not already do so, install the tidyverse package. How many vignettes does the dplyr package have? Where can you go to learn more about the tidyverse packages? When you load the tidyverse packages what other packages is it automatically loading for you? Using the resource in #5, explain at a high-level what the packages in #6 do. References "],["lesson-1d-vectors.html", "5 Lesson 1d: Vectors 5.1 Learning objectives 5.2 Creating vectors 5.3 Extracting elements 5.4 Replacing elements 5.5 Operations 5.6 Missing data 5.7 Vectorization 5.8 Exercises", " 5 Lesson 1d: Vectors In the last module we started to explore how to use R as a calculator. This is great; however, we were only working with individual values. Often, we want to work on several values at once so we need a structure that will hold multiple pieces of data. We will discuss data structures more in the next module but in this lesson we introduce the vector, which is the fundamental data structure in R. Once you have a good grasp of working with vectors, then working with other data structures because much easier. 5.1 Learning objectives By the end of this lesson you will be able to: Create vectors. Extract and replace elements within a vector. Perform basic operations on a vector (i.e. compute the mean). Work with missing data in a vector. Explain and take advantage of vectorization. 5.2 Creating vectors There are multiple ways to create vectors but the first one you’ll be introduced to is by using c(). The c() function is short for concatenate and we use it to join together a series of values and store them in a vector. my_vec &lt;- c(2, 3, 1, 6, 4, 3, 3, 7) To examine the value of our new object we can simply type out the name of the object as we did before: my_vec ## [1] 2 3 1 6 4 3 3 7 5.2.1 Creating sequences Sometimes it can be useful to create a vector that contains a regular sequence of values in steps of one. Here we can make use of a shortcut using the : symbol. my_seq &lt;- 1:10 # create regular sequence my_seq ## [1] 1 2 3 4 5 6 7 8 9 10 my_seq2 &lt;- 10:1 # in decending order my_seq2 ## [1] 10 9 8 7 6 5 4 3 2 1 Other useful functions for generating vectors of sequences include the seq() and rep() functions. For example, to generate a sequence from 1 to 5 in steps of 0.5 my_seq2 &lt;- seq(from = 1, to = 5, by = 0.5) my_seq2 ## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Here we’ve used the arguments from = and to = to define the limits of the sequence and the by = argument to specify the increment of the sequence. Play around with other values for these arguments to see their effect. The rep() function allows you to replicate (repeat) values a specified number of times. To repeat the value 2, 10 times # repeats 2, 10 times my_seq3 &lt;- rep(2, times = 10) my_seq3 ## [1] 2 2 2 2 2 2 2 2 2 2 You can also repeat non-numeric values # repeats ‘abc’ 3 times my_seq4 &lt;- rep(&quot;abc&quot;, times = 3) my_seq4 ## [1] &quot;abc&quot; &quot;abc&quot; &quot;abc&quot; or each element of a series # repeats the series 1 to 5, 3 times my_seq5 &lt;- rep(1:5, times = 3) my_seq5 ## [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 or elements of a series # repeats each element of the series 3 times my_seq6 &lt;- rep(1:5, each = 3) my_seq6 ## [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 We can also repeat a non-sequential series # repeats each element of the series 3 times my_seq7 &lt;- rep(c(3, 1, 10, 7), each = 3) my_seq7 ## [1] 3 3 3 1 1 1 10 10 10 7 7 7 Note in the code above how we’ve used the c() function inside the seq() function. Nesting functions allows us to build quite complex commands within a single line of code and is a very common practice when using R. However, care needs to be taken as too many nested functions can make your code quite difficult for others to understand (or yourself some time in the future!). We could rewrite the code above to explicitly separate the two different steps to generate our vector. Either approach will give the same result, you just need to use your own judgement as to which is more readable. in_vec &lt;- c(3, 1, 10, 7) my_seq7 &lt;- rep(in_vec, each = 3) my_seq7 ## [1] 3 3 3 1 1 1 10 10 10 7 7 7 5.2.2 Knowledge check Use c() to create a vector called weight containing the weight (in lbs) of 10 children: 75, 95, 92, 89, 101, 87, 79, 92, 70, 99. Use the seq() function to create a sequence of numbers ranging from 0 to 1 in steps of 0.1. Generate the following sequences. You will need to experiment with the arguments to the rep() function to generate these sequences: 1 2 3 1 2 3 1 2 3 “a” “a” “a” “c” “c” “c” “e” “e” “e” “g” “g” “g” “a” “c” “e” “g” “a” “c” “e” “g” “a” “c” “e” “g” 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 1 1 2 2 2 2 3 3 3 4 4 5 5.3 Extracting elements To extract (also known as indexing) one or more values (more generally known as elements) from a vector we use the square bracket [ ] notation. The general approach is to name the object you wish to extract from, then a set of square brackets with an index of the element you wish to extract contained within the square brackets. This index can be a position or the result of a logical test. 5.3.1 Positional indexing # extract the 3rd value my_vec[3] ## [1] 1 # if you want to assign this value in another object val_3 &lt;- my_vec[3] val_3 ## [1] 1 Note that the positional index starts at 1 rather than 0 like some other other programming languages (i.e. Python). We can also extract more than one value by using the c() function inside the square brackets. Here we extract the 1st, 5th, 6th and 8th element from the my_vec object my_vec[c(1, 5, 6, 8)] ## [1] 2 4 3 7 Or we can extract a range of values using the : notation. To extract the values from the 3rd to the 8th elements my_vec[3:8] ## [1] 1 6 4 3 3 7 5.3.2 Logical indexing Another really useful way to extract data from a vector is to use a logical expression as an index. For example, to extract all elements with a value greater than 4 in the vector my_vec my_vec[my_vec &gt; 4] ## [1] 6 7 Here, the logical expression is my_vec &gt; 4 and R will only extract those elements that satisfy this logical condition. So how does this actually work? If we look at the output of just the logical expression without the square brackets you can see that R returns a vector containing either TRUE or FALSE which correspond to whether the logical condition is satisfied for each element. In this case only the 4th and 8th elements return a TRUE as their value is greater than 4. my_vec &gt; 4 ## [1] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE So what R is actually doing under the hood is equivalent to my_vec[c(FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE)] ## [1] 6 7 and only those element that are TRUE will be extracted. In addition to the &lt; and &gt; operators you can also use composite operators to increase the complexity of your expressions. For example the expression for ‘greater or equal to’ is &gt;=. To test whether a value is equal to a value we need to use a double equals symbol == and for ‘not equal to’ we use != (the ! symbol means ‘not’). my_vec[my_vec &gt;= 4] # values greater or equal to 4 ## [1] 6 4 7 my_vec[my_vec &lt; 4] # values less than 4 ## [1] 2 3 1 3 3 my_vec[my_vec &lt;= 4] # values less than or equal to 4 ## [1] 2 3 1 4 3 3 my_vec[my_vec == 4] # values equal to 4 ## [1] 4 my_vec[my_vec != 4] # values not equal to 4 ## [1] 2 3 1 6 3 3 7 We can also combine multiple logical expressions using Boolean expressions. In R the &amp; symbol means AND and the | symbol means OR. For example, to extract values in my_vec which are less than 6 AND greater than 2 val26 &lt;- my_vec[my_vec &lt; 6 &amp; my_vec &gt; 2] val26 ## [1] 3 4 3 3 or extract values in my_vec that are greater than 6 OR less than 3 val63 &lt;- my_vec[my_vec &gt; 6 | my_vec &lt; 3] val63 ## [1] 2 1 7 5.3.3 Knowledge check Use c() to create a vector called weight containing the weight (in lbs) of 10 children: 75, 95, 92, 89, 101, 87, 79, 92, 70, 99. Extract the weights for the first and last child. Extract the weights for the first five children. Extract the weights for those children that weigh over 90lbs. 5.4 Replacing elements We can change the values of some elements in a vector using our [ ] notation in combination with the assignment operator &lt;-. For example, to replace the 4th value of our my_vec object from 6 to 500 my_vec[4] &lt;- 500 my_vec ## [1] 2 3 1 500 4 3 3 7 We can also replace more than one value or even replace values based on a logical expression # replace the 6th and 7th element with 100 my_vec[c(6, 7)] &lt;- 100 my_vec ## [1] 2 3 1 500 4 100 100 7 # replace element that are less than or equal to 4 with 1000 my_vec[my_vec &lt;= 4] &lt;- 1000 my_vec ## [1] 1000 1000 1000 500 1000 100 100 7 5.4.1 Knowledge check Use c() to create a vector called weight containing the weight (in lbs) of 10 children: 75, 95, 92, 89, 101, 87, 79, 92, 70, 99. Say we made a mistake and the third child actually weighs 96, change the value for that element. Say the minimum weight allowed to be recorded is 80lbs. Change the value for all elements that are less than 80 to be equal to 80. 5.5 Operations We can use other functions to perform useful operations on vectors. For example, we can calculate the mean, variance, standard deviation and number of elements in our vector by using the mean(), var(), sd() and length() functions mean(my_vec) # returns the mean of my_vec ## [1] 588.38 var(my_vec) # returns the variance of my_vec ## [1] 214367 sd(my_vec) # returns the standard deviation of my_vec ## [1] 463 length(my_vec) # returns the number of elements in my_vec ## [1] 8 If we wanted to use any of these values later on in our analysis we can just assign the resulting value to another object vec_mean &lt;- mean(my_vec) vec_mean ## [1] 588.38 We can also sort values in our element: # ascending order sort(my_vec) ## [1] 7 100 100 500 1000 1000 1000 1000 # decending order sort(my_vec, decreasing = TRUE) ## [1] 1000 1000 1000 1000 500 100 100 7 There are a lot operations that we can perform on vectors. As we progress through this course we’ll see many of these in action. 5.5.1 Knowledge check Use c() to create a vector called weight containing the weight (in lbs) of 10 children: 75, 95, 92, 89, 101, 87, 79, 92, 70, 99. Identify functions that will compute the minimum and maximum values in this vector. What is the mean, median, and standard deviation of child weights? Apply the summary() function on this vector. What does this return? 5.6 Missing data In R, missing data is usually represented by an NA symbol meaning ‘Not Available’. Data may be missing for a whole bunch of reasons, maybe your machine broke down, maybe the weather was too bad to collect data on a particular day, etc. Missing data can be a pain in the proverbial both from an R perspective and also a statistical perspective. From an R perspective missing data can be problematic as different functions deal with missing data in different ways. For example, let’s say we collected air temperature readings over 10 days, but our thermometer broke on day 2 and again on day 9 so we have no data for those days temp &lt;- c(7.2, NA, 7.1, 6.9, 6.5, 5.8, 5.8, 5.5, NA, 5.5) temp ## [1] 7.2 NA 7.1 6.9 6.5 5.8 5.8 5.5 NA 5.5 We now want to calculate the mean temperature over these days using the mean() function mean_temp &lt;- mean(temp) mean_temp ## [1] NA Flippin heck, what’s happened here? Why does the mean() function return an NA? Actually, R is doing something very sensible (at least in our opinion!). If a vector has a missing value then the only possible value to return when calculating a mean is NA. R doesn’t know that you perhaps want to ignore the NA values. Happily, if we look at the help file via help(\"mean\") we can see there is an argument na.rm = which is set to FALSE by default. Most statistical operators will have an na.rm parameter that takes a TRUE or FALSE argument indicating whether NA values should be stripped before the computation proceeds. If we change this argument to na.rm = TRUE when we use the mean() function this will allow us to ignore the NA values when calculating the mean mean_temp &lt;- mean(temp, na.rm = TRUE) mean_temp ## [1] 6.2875 It’s important to note that the NA values have not been removed from our temp object (that would be bad practice), rather the mean() function has just ignored them. The point of the above is to highlight how we can change the default behaviour of a function using an appropriate argument. 5.6.1 Knowledge check Use c() to create a vector called weight containing the weight (in lbs) of 10 children: 75, 95, 92, 89, 101, NA, 79, 92, 70, 99. Compute the min, max, mean, median, and standard deviation of child weights? Apply the summary() function on this vector. How does this differ from before? 5.7 Vectorization 5.7.1 Looping versus Vectorization A key difference between R and many other languages is a topic known as vectorization. What does this mean? It means that many functions that are to be applied individually to each element in a vector of numbers require a loop assessment to evaluate; however, in R many of these functions have been coded in C to perform much faster than a for loop would perform. For example, let’s say you want to add the elements of two separate vectors of numbers (x and y). x &lt;- c(1, 3, 4) y &lt;- c(1, 2, 4) x ## [1] 1 3 4 y ## [1] 1 2 4 In other languages you might have to run a loop to add two vectors together. In this for loop I print each iteration to show that the loop calculates the sum for the first elements in each vector, then performs the sum for the second elements, etc. Don’t worry if you don’t understand each piece of this code. This is just for illustration purposes! You will learn about looping procedures in a later module. # empty vector to store results z &lt;- as.vector(NULL) # `for` loop to add corresponding elements in each vector for (i in seq_along(x)) { z[i] &lt;- x[i] + y[i] print(z) } ## [1] 2 ## [1] 2 5 ## [1] 2 5 8 Instead, in R, + is a vectorized function which can operate on entire vectors at once. So rather than creating for loops for many functions, you can just use simple syntax to perform element-wise operations with both vectors: # add each element in x and y x + y ## [1] 2 5 8 # multiply each element in x and y x * y ## [1] 1 6 16 # compare each element in x to y x &gt; y ## [1] FALSE TRUE FALSE 5.7.2 Recycling When performing vector operations in R, it is important to know about recycling. When performing an operation on two or more vectors of unequal length, R will recycle elements of the shorter vector(s) to match the longest vector. For example: long &lt;- 1:10 short &lt;- 1:5 long ## [1] 1 2 3 4 5 6 7 8 9 10 short ## [1] 1 2 3 4 5 # R will recycle (reuse) the short vector until it reaches # the end of the long vector long + short ## [1] 2 4 6 8 10 7 9 11 13 15 The elements of long and short are added together starting from the first element of both vectors. When R reaches the end of the short vector, it starts again at the first element of short and continues until it reaches the last element of the long vector. This functionality is very useful when you want to perform the same operation on every element of a vector. For example, say we want to multiply every element of our vector long by 3: long &lt;- 1:10 c &lt;- 3 long * c ## [1] 3 6 9 12 15 18 21 24 27 30 There are no scalars in R, so c is actually a vector of length 1; in order to add its value to every element of long, it is recycled to match the length of long. Don’t get hung up with some of the verbiage used here (i.e. vectors vs. scalars), we will cover what this means in later a module. When the length of the longer object is a multiple of the shorter object length, the recycling occurs silently. When the longer object length is not a multiple of the shorter object length, a warning is given: even_length &lt;- 1:10 odd_length &lt;- 1:3 even_length + odd_length ## Warning in even_length + odd_length: longer object length is not a multiple of shorter object ## length ## [1] 2 4 6 5 7 9 8 10 12 11 5.7.3 Knowledge check Create this vector my_vec &lt;- 1:10. Add 1 to every element in my_vec. Divide every element in my_vec by 2. Create a second vector my_vec2 &lt;- 10:18 and add my_vec to my_vec2. 5.8 Exercises Create a vector called weight containing the weight (in kg) of 10 children: 69, 62, 57, 59, 59, 64, 56, 66, 67, 66. Create a vector called height containing the height (in cm) of the same 10 children: 112, 102, 83, 84, 99, 90, 77, 112, 133, 112. Use the summary() function to summarize these data. Extract the height of the 2nd, 3rd, 9th and 10th child and assign these heights to a variable called some_child. Also extract all the heights of children less than or equal to 99 cm and assign to a variable called shorter_child. Use the information in your weight and height variables to calculate the body mass index (BMI) for each child. The BMI is calculated as weight (in kg) divided by the square of the height (in meters). Store the results of this calculation in a variable called bmi. Note: you don’t need to do this calculation for each child individually, you can leverage vectorization to do this! "],["lab.html", "6 Lab", " 6 Lab TBD "],["overview-1.html", "7 Overview 7.1 Learning objectives 7.2 Tasks 7.3 Course readings", " 7 Overview Welcome to module 2! This module will focus on … 7.1 Learning objectives By the end of this module you should be able to: Use RStudio Projects to organize a data wrangling project. Understand how to use R scripts, R Markdown, and R Notebook documents to make your data wrangling projects and outputs reproducible. Import data from a variety of sources and perform basic tasks to understand simple attributes about your imported dataset. Describe key differences between different data structures. 7.2 Tasks TBD 7.3 Course readings TBD "],["lesson-2a-workflow.html", "8 Lesson 2a: Workflow 8.1 Learning objectives 8.2 R Projects 8.3 R Markdown 8.4 R Notebooks 8.5 Exercises 8.6 Computing environment", " 8 Lesson 2a: Workflow This lesson serves to introduce you to workflow options that will serve to organize your projects and make them reproducible. You’ll learn how to have a project-oriented environment along with how to create R Markdown and Notebook scripts for efficient and reproducible deliverables. 8.1 Learning objectives Upon completing this module you will be able to: Explain the benefits of an R project and new ones. Explain the similarities and differences between R Markdown files and R Notebooks. Create both R Markdown and R Notebook deliverables. 8.2 R Projects “Organization is what you do before you do something, so that when you do it, it is not all mixed up.” - A.A. Milne If you are not careful your data analyses can become an explosion of data files, R scripts, ggplot graphs, and final reports. Each project evolves and mutates in its own way and keeping all the files associated with a project organized together is a wise practice. In fact, it is such a wise practice that RStudio has built-in support to manage your projects. This built-in capability is called…wait for it…RStudio projects. RStudio projects make it straightforward to divide your work into multiple contexts, each with their own working directory, workspace, history, and source documents. 8.2.1 Creating Projects RStudio projects are associated with R working directories. You can create an RStudio project: In a new directory In an existing directory where you already have R code and data By cloning a version control (Git or Subversion) repository by selecting File » New Project and then completing the following set-up tasks: 8.2.2 So What’s Different? When a new project is created RStudio: Creates a project file (with an .Rproj extension) within the project directory. This file contains various project options (discussed below) and can also be used as a shortcut for opening the project directly from the filesystem. Creates a hidden directory (named .Rproj.user) where project-specific temporary files (e.g. auto-saved source documents, window-state, etc.) are stored. This directory is also automatically added to .Rbuildignore, .gitignore, etc. if required. Loads the project into RStudio and display its name in the Projects toolbar (which is located on the far right side of the main toolbar) When a project is opened (File » Open Project or by clicking on the .Rproj file directly for the project): A new R session is started The .Rprofile file in the project’s main directory is sourced by R The .RData file in the project’s main directory is loaded (if any) The history for the project is loaded into the History panel The working directory is set to the project’s directory. Previously edited source documents are restored into editor tabs Other RStudio settings are restored to where they were the last time the project was closed As you write and execute code in the project all updates and outputs created will be saved to the project directory. And when you close out of the project the .RData and .Rhistory files will be saved (if these options are selected in the global options) and the list of open source documents are saved so that they can be restored the next time you open the project. There are additional project options you can choose from to customize the project at Tools » Project Options. These project options are overrides for existing global options. To inherit the default global behavior for a project you can specify (Default) as the option value. 8.2.3 Knowledge check Go ahead and create an R Project for this class. Make sure the following RStudio preference settings are set: General: Set “Save workspace to .RData on exit: Never”. Code: In the display tab check the “Show margin” option and set “Margin Column: 80”. Code &gt;&gt; Diagnostics: Make sure the “Provide R style diagnostics” is checked. 8.3 R Markdown R Markdown provides an easy way to produce a rich, fully-documented reproducible analysis. It allows the user to share a single file that contains all of the prose, code, and metadata needed to reproduce the analysis from beginning to end. R Markdown allows for “chunks” of R code to be included along with Markdown text to produce a nicely formatted HTML, PDF, or Word file without having to know any HTML or LaTeX code or have to fuss with difficult formatting issues. One R Markdown file can generate a variety of different formats and all of this is done in a single text file with a few bits of formatting. So how does it work? Creating documents with R Markdown starts with an .Rmd file that contains a combination of text and R code chunks. The .Rmd file is fed to knitr, which executes all of the R code chunks and creates a new markdown (.md) document with the output. Pandoc then processes the .md file to create a finished report in the form of a web page, PDF, Word document, slide show, etc. Sounds confusing you say, don’t fret. Much of what takes place happens behind the scenes. You primarily need to worry only about the syntax required in the .Rmd file. You then press a button and out comes your report. 8.3.1 Creating an R Markdown File To create an R Markdown file you can select File » New File » R Markdown or you can select the shortcut for creating a new document in the top left-hand corner of the RStudio window. You will be given an option to create an HTML, PDF, or Word document; however, R Markdown let’s you change seamlessly between these options after you’ve created your document so I tend to just select the default HTML option. There are additional options such as creating Presentations (HTML or PDF), Shiny documents, or other template documents but for now we will focus on the initial HTML, PDF, or Word document options. 8.3.2 Components of an R Markdown File There are three general components of an R Markdown file that you will eventually become accustomed to. This includes the YAML, the general markdown (or text) component, and code chunks. 8.3.2.1 YAML Header The first few lines you see in the R Markdown report are known as the YAML. --- title: &quot;R Markdown Demo&quot; author: &quot;Brad Boehmke&quot; date: &quot;2016-08-15&quot; output: html_document --- These lines will generate a generic heading at the top of the final report. There are several YAML options to enhance your reports such as the following: You can include hyperlinks around the title or author name: --- title: &quot;R Markdown Demo&quot; author: &quot;[Brad Boehmke](http://bradleyboehmke.github.io)&quot; date: &quot;2016-08-15&quot; output: html_document --- If you don’t want the date to be hard-coded you can include R code so that anytime you re-run the report the current date will print off at the top. You can also exclude the date (or author and title information) by including NULL or simply by deleting that line: --- title: &quot;R Markdown Demo&quot; author: &quot;[Brad Boehmke](http://bradleyboehmke.github.io)&quot; date: &quot;2022-05-26&quot; output: html_document --- By default, your report will not include a table of contents (TOC). However, you can easily generate one by including the toc: true argument. There are several TOC options such as the level of headers to include in the TOC, whether to have a fixed or floating TOC, to have a collapsable TOC, etc. You can find many of the TOC options here. --- title: &quot;R Markdown Demo&quot; author: &quot;[Brad Boehmke](http://bradleyboehmke.github.io)&quot; date: &quot;2022-05-26&quot; output: html_document: toc: true toc_float: true --- When knitr processes an R Markdown input file it creates a markdown (.md) file which is subsequently transformed into HTML by pandoc. If you want to keep a copy of the markdown file after rendering you can do so using the keep_md: true option. This will likely not be a concern at first but when (if) you start doing a lot of online writing you will find that keeping the .md file is extremely beneficial. --- title: &quot;R Markdown Demo&quot; author: &quot;[Brad Boehmke](http://bradleyboehmke.github.io)&quot; date: &quot;2022-05-26&quot; output: html_document: keep_md: true --- There are many YAML options which you can read more about at: HTML reports: http://rmarkdown.rstudio.com/html_document_format.html PDF (LaTex) reports: http://rmarkdown.rstudio.com/pdf_document_format.html Word reports: http://rmarkdown.rstudio.com/word_document_format.html 8.3.2.2 Text Formatting The beauty of R Markdown is the ability to easily combine prose (text) and code. For the text component, much of your writing is similar to when you type a Word document; however, to perform many of the basic text formatting you use basic markdown code such as: There are many additional formatting options which can be viewed here and here; however, this should get you well on your way. 8.3.2.3 Code Chunks R code chunks can be used as a means to render R output into documents or to simply display code for illustration. Code chunks start with the following line: {r chunk_name}&lt;/code&gt; and end with &lt;code&gt;. You can quickly insert chunks into your R Markdown file with the keyboard shortcut Cmd + Option + I (Windows Ctrl + Alt + I). Here is a simple R code chunk that will result in both the code and it’s output being included: ```{r} head(iris) ``` Chunk output can be customized with many knitr options which are arguments set in the {} of a chunk header. Examples include: 1. echo=FALSE hides the code but displays results: ```{r echo=FALSE} x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) cor(x, y) ``` 2. results='hide' hides the results but shows the code ```{r results=&#39;hide&#39;} x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) cor(x, y) ``` 3. eval=FALSE displays the code but does not evaluate it ```{r eval=FALSE} x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) cor(x, y) ``` 4. include=FALSE evaluates the code but does not display code or output ```{r include=FALSE} x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) cor(x, y) ``` 5. warning=FALSE and message=FALSE are useful for suppressing any messages produced when loading packages ```{r, warning=FALSE, message=FALSE} library(dplyr) ``` 6. collapse=TRUE will collapse your output to be contained within the code chunk ```{r, collapse=TRUE} head(iris) ``` 7. fig... options are available to align and size figure outputs ```{r, fig.align=&#39;center&#39;, fig.height=3, fig.width=4} library(ggplot2) ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() ``` 8.3.2.4 Inline code chunks A key motivation for reproducible research is to link any results reported directly to the data and functions used to create them. Consequently, you should never manual insert numbers such as “The average miles per gallon is 20.1.” Rather, code results can be inserted directly into the text of a .Rmd file by enclosing the code with `r ` such as: “The average miles per gallon is `r mean(mtcars$mpg)`.” Now if the underlying data changes you do not need to remember all the inline values you manually entered. You may not like the fact that the output is reporting all the decimals. You could include the round function in the inline code: `r round(mean(mtcars$mpg), 1)`. 8.3.2.5 Dealing with Tables By default, the table outputs produced in R Markdown will look like the output you would see in your console. However, if you prefer that data be displayed with additional formatting you can use the knitr::kable() function. For example: ```{r, results=&#39;asis&#39;} knitr::kable(iris) ``` To include captions: ```{r} knitr::kable(head(iris), caption = &#39;Example caption for the iris data frame&#39;) ``` The simplest approach to print nice looking tables is to use the printr package which can be installed from CRAN. ```{r} library(printr) head(iris) ``` There are several packages that can be used to make very nice tables: printr xtable stargazer tables pander 8.3.3 Knitting the R Markdown File When you are all done writing your .Rmd document you have two options to render the output. The first is to call the following function in your console: render(\"document_name.Rmd\", output_format = \"html_document\"). Alternatively you can click the drop down arrow next to the knit button on the RStudio toolbar, select the document format (HTML, PDF, Word) and your report will be developed. The following output formats are available to use with R Markdown. Documents: html_notebook - Interactive R Notebooks html_document - HTML document w/ Bootstrap CSS pdf_document - PDF document (via LaTeX template) word_document - Microsoft Word document (docx) odt_document - OpenDocument Text document rtf_document - Rich Text Format document md_document - Markdown document (various flavors) Presentations (slides): ioslides_presentation - HTML presentation with ioslides revealjs::revealjs_presentation - HTML presentation with reveal.js slidy_presentation - HTML presentation with W3C Slidy beamer_presentation - PDF presentation with LaTeX Beamer More: flexdashboard::flex_dashboard - Interactive dashboards tufte::tufte_handout - PDF handouts in the style of Edward Tufte tufte::tufte_html - HTML handouts in the style of Edward Tufte tufte::tufte_book - PDF books in the style of Edward Tufte html_vignette - R package vignette (HTML) github_document - GitHub Flavored Markdown document bookdown - Write HTML, PDF, ePub, and Kindle books with R Markdown 8.3.4 Additional Resources R Markdown is an incredible tool for reproducible research and there are a lot of resources available. Here are just a few of the available resources to learn more about R Markdown. Rstudio tutorials R Markdown course by DataCamp Karl Browman’s tutorial Daring Fireball Reproducible Research course on Coursera Chester Ismay’s book Also, you can find the R Markdown cheatsheet within the RStudio console at Help menu » Cheatsheets. 8.3.5 Knowledge check Create a new R Markdown document with File &gt; New File &gt; R Markdown… Knit it by clicking the appropriate button. Knit it by using the appropriate keyboard short cut. Verify that you can modify the input and see the output update. Practice what you’ve learned by creating a brief CV. The title should be your name, and you should include headings for (at least) education or employment. Each of the sections should include a bulleted list of jobs/degrees. Highlight the year in bold. Using the R Markdown quick reference, figure out how to: Add a footnote. Add a horizontal rule. Add a block quote. 8.4 R Notebooks An R Notebook is an R Markdown document that allows for independent and interactive execution of the code chunks. This allows you to visually assess the output as you develop your R Markdown document without having to knit the entire document to see the output. R Notebooks can be thought of as a unique execution mode for R Markdown documents as any R Markdown document can be used as a notebook, and all R Notebooks can be rendered to other R Markdown document types. The interactive capabilities of the notebook mode makes it extremely useful for writing R Markdown documents and iterating on code. 8.4.1 Creating an R Notebook Creating an R Notebook is similar to creating an R Markdown document - you’ll notice a new option for creating an R Notebook. When you create a new R Notebook the primary differece you will notice at first is the YAML which will look like: --- title: &quot;R Notebook&quot; output: html_notebook --- The default notebook mode allows inline output on all R Markdown documents. If you prefer to use the traditional console method of interaction, you can disable notebook mode by clicking the gear in the editor toolbar and choosing Chunk Output in Console. You can also toggle between previewing the document in the Viewer Pane versus in a Window. 8.4.2 Interactiveness of an R Notebook Writing an R Notebook document is no different than writing an R Markdown document. The text and code chunk syntax does not differ from what you learned in the previous section of this lesson. The primary difference is in the interactiveness of an R Notebook. Primarily that when executing chunks in an R Markdown document, all the code is sent to the console at once, but in an R Notebook, only one line at a time is sent. This allows execution to stop if a line raises an error. There are couple options for executing code chunks. You can execute code chunks individually by: Having the cursor within the code chunk and selecting ⌘ + enter Clicking the Run Current Chunk button in the first line (far right-hand side) of the code chunk Or selecting the Run Current Chunk option from the Run menu in the RStudio console toolbar You can also run all chunks in your document by: Selecting the Run All option from the Run menu in the RStudio console toolbar Using the keyboard shortcut ⌥ + ⌘ + R When a code chunk is waiting to be executed, you’ll notice a progress meter that appears to the left of the code chunk plus there will be a status in the editor’s status bar indicating the number of chunks remaining to be executed. You can click on this meter at any time to jump to the currently executing chunk. 8.4.3 Saving, Sharing, Previewing &amp; Knitting an R Notebook When a notebook .Rmd is saved, an .nb.html file is created alongside it. This file is a self-contained HTML file which contains all current code chunks (collapsable/expandable) and their respective outputs. You can view this .nb.html file directly in any browser along with sharing it with others who can also view it in any browser. Ordinary R Markdown documents are “knit”, but notebooks are “previewed”. So by default, when you select the preview option in the editor toolbar your document will be previewed in the Viewer Pane. You can preview your document in a window by selecting the desired option in the gear in the editor toolbar. When you are ready to publish the document, you can share the .nb.html directly, or render it to a publication format by knitting the document to the desired format. 8.4.4 Additional Resources Learn more about R Notebook at RStudio’s tutorial page. 8.4.5 Knowledge check Create a new notebook using File &gt; New File &gt; R Notebook. Read the instructions. Practice running the chunks. Verify that you can modify the code, re-run it, and see modified output. Compare and contrast the R notebook you just created with the R markdown file you created in the previous knowledge check. How are the outputs similar? How are they different? How are the inputs similar? How are they different? What happens if you copy the YAML header from one to the other? 8.5 Exercises If you have not already done so create an R project for this course so that all future scripts, inputs, and outputs are organized. Make sure the following RStudio preference settings are set: General: Set “Save workspace to .RData on exit: Never”. Code: In the display tab check the “Show margin” option and set “Margin Column: 80”. Code &gt;&gt; Diagnostics: Make sure the “Provide R style diagnostics” is checked. Create a new R Markdown document and include code and text from some of the activities we covered in lesson 3 of module 1. Now knit this document using each of the three built-in formats: HTML, PDF and Word. How does the output differ? You may need to install LaTeX in order to build the PDF output — RStudio will prompt you if this is necessary. 8.6 Computing environment At the end of of any notebook you make, you should include information about the computing environment including the version numbers of all packages you use. We can do that with the following. sessioninfo() will provide information on the operating system, version of R along with any specified packages. Here, we specify pkgs = 'attached' which means it will only list those packages that have been attached to the R search path with library(pkg_name). An alternative is to explicitly pass a vector of package names used. For this book you’ll find the computing environment consolidated into one notebook at the end of the book. sessioninfo::session_info(pkgs = &#39;attached&#39;) ## ─ Session info ────────────────────────────────────────────────────────────────────────────── ## setting value ## version R version 4.2.0 (2022-04-22) ## os macOS Monterey 12.2 ## system x86_64, darwin17.0 ## ui RStudio ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/New_York ## date 2022-05-26 ## rstudio 2022.02.2+485 Prairie Trillium (desktop) ## pandoc 2.18 @ /usr/local/bin/ (via rmarkdown) ## ## ─ Packages ────────────────────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## completejourney * 1.1.0 2019-09-28 [1] CRAN (R 4.2.0) ## dbplyr * 2.1.1 2021-04-06 [1] CRAN (R 4.2.0) ## dplyr * 1.0.9 2022-04-28 [1] CRAN (R 4.2.0) ## forcats * 0.5.1 2021-01-27 [1] CRAN (R 4.2.0) ## ggplot2 * 3.3.6 2022-05-03 [1] CRAN (R 4.2.0) ## here * 1.0.1 2020-12-13 [1] CRAN (R 4.2.0) ## jsonlite * 1.8.0 2022-02-22 [1] CRAN (R 4.2.0) ## magrittr * 2.0.3 2022-03-30 [1] CRAN (R 4.2.0) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.2.0) ## readr * 2.1.2 2022-01-30 [1] CRAN (R 4.2.0) ## readxl * 1.4.0 2022-03-28 [1] CRAN (R 4.2.0) ## RSQLite * 2.2.14 2022-05-07 [1] CRAN (R 4.2.0) ## stringr * 1.4.0 2019-02-10 [1] CRAN (R 4.2.0) ## tibble * 3.1.7 2022-05-03 [1] CRAN (R 4.2.0) ## tidyr * 1.2.0 2022-02-01 [1] CRAN (R 4.2.0) ## tidyverse * 1.3.1 2021-04-15 [1] CRAN (R 4.2.0) ## ## [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library ## ## ───────────────────────────────────────────────────────────────────────────────────────────── "],["lesson-2b-data-types-structures.html", "9 Lesson 2b: Data types &amp; structures 9.1 Learning objectives 9.2 Data types 9.3 Data structures 9.4 Exercises", " 9 Lesson 2b: Data types &amp; structures Until now, you’ve created fairly simple data in R and stored it as a vector! However, when wrangling data we often come across a variety of data types and require different data structures to manage them. This lesson serves to introduce you to the basic data types and structures in R that you’ll most commonly use. 9.1 Learning objectives Upon completing this module you will be able to: Explain the benefits of an R project and new ones. Explain the similarities and differences between R Markdown files and R Notebooks. Create both R Markdown and R Notebook deliverables. 9.2 Data types R has six basic types of data: numeric, integer, logical, character, complex and raw. However, it is very unlikely that in your time as a data analyst/scientist you’ll need to work with complex and raw data types so we’ll focus on the first four. Numeric data are numbers that contain a decimal. Actually they can also be whole numbers but we’ll gloss over that. Integers are whole numbers (those numbers without a decimal point). Logical data take on the value of either TRUE or FALSE. There’s also another special type of logical called NA to represent missing values. Character data are used to represent string values. You can think of character strings as something like a word (or multiple words). A special type of character string is a factor, which is a string but with additional attributes (like levels or an order). We’ll cover factors later. 9.2.1 Determining the type R is (usually) able to automatically distinguish between different classes of data by their nature and the context in which they’re used although you should bear in mind that R can’t actually read your mind and you may have to explicitly tell R how you want to treat a data type. You can find out the type (or class) of any object using the class() function. num &lt;- 2.2 class(num) ## [1] &quot;numeric&quot; char &lt;- &quot;hello world&quot; class(char) ## [1] &quot;character&quot; logi &lt;- TRUE class(logi) ## [1] &quot;logical&quot; Alternatively, you can ask if an object is a specific class using a logical test. The is.xxxx() family of functions will return either a TRUE or a FALSE. is.numeric(num) ## [1] TRUE is.character(num) ## [1] FALSE is.character(char) ## [1] TRUE is.logical(logi) ## [1] TRUE 9.2.2 Type conversion It can sometimes be useful to be able to change the class of a variable using the as.xxxx() family of coercion functions, although you need to be careful when doing this as you might receive some unexpected results (see what happens below when we try to convert a character string to a numeric). # coerce numeric to character class(num) ## [1] &quot;numeric&quot; num_char &lt;- as.character(num) num_char ## [1] &quot;2.2&quot; class(num_char) ## [1] &quot;character&quot; # coerce character to numeric! class(char) ## [1] &quot;character&quot; char_num &lt;- as.numeric(char) ## Warning: NAs introduced by coercion Type Logical test Coercing Character is.character as.character Numeric is.numeric as.numeric Logical is.logical as.logical Factor is.factor as.factor Complex is.complex as.complex In later modules we will learn how to wrangle these different data types plus other special data types that are built on top of these classes (i.e. date-time stamps, missing values). For now, I just want you to understand the foundational data types built into R. 9.2.3 Knowledge check Check out the built-in object pi. What class is this object? What happens when you coerce this object to a character? What happens when you coerce it to a logical? Is there a coercion function that could convert this to an integer? What happens when you do this? 9.3 Data structures Now that you’ve been introduced to some of the most important classes of data in R, let’s have a look at some of main structures that we have for storing these data. 9.3.1 Scalars and vectors Perhaps the simplest type of data structure is the vector. You’ve already been introduced to vectors in module 1. Vectors that have a single value (length 1) are often referred to as scalars. Vectors can contain numbers, characters, factors or logicals, but the key thing to remember is that all the elements inside a vector must be of the same class. In other words, vectors can contain either numbers, characters or logicals but not mixtures of these types of data. There is one important exception to this, you can include NA (this is special type of logical) to denote missing data in vectors with other data types. All the elements inside a vector must be of the same class 9.3.2 Matrices and arrays Another useful data structure used in many disciplines such as population ecology, theoretical and applied statistics is the matrix. A matrix is simply a vector that has additional attributes called dimensions. Arrays are just multidimensional matrices. Again, matrices and arrays must contain elements all of the same data class. 9.3.2.1 Creating A convenient way to create a matrix or an array is to use the matrix() and array() functions respectively. Below, we will create a matrix from a sequence 1 to 16 in four rows (nrow = 4) and fill the matrix row-wise (byrow = TRUE) rather than the default column-wise. When using the array() function we define the dimensions using the dim = argument, in our case 2 rows, 4 columns in 2 different matrices. my_mat &lt;- matrix(1:16, nrow = 4, byrow = TRUE) my_mat ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 ## [4,] 13 14 15 16 my_array &lt;- array(1:16, dim = c(2, 4, 2)) my_array ## , , 1 ## ## [,1] [,2] [,3] [,4] ## [1,] 1 3 5 7 ## [2,] 2 4 6 8 ## ## , , 2 ## ## [,1] [,2] [,3] [,4] ## [1,] 9 11 13 15 ## [2,] 10 12 14 16 Sometimes it’s also useful to define row and column names for your matrix but this is not a requirement. To do this use the rownames() and colnames() functions. rownames(my_mat) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) colnames(my_mat) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) my_mat ## a b c d ## A 1 2 3 4 ## B 5 6 7 8 ## C 9 10 11 12 ## D 13 14 15 16 9.3.2.2 Indexing Similar to vectors, we can extract elements from our matrix using [] notation. The main difference is we now have to specify two dimensions in our indexing matrix[row, col]: # element located at the intersection of the # third row and second column my_mat[3, 2] ## [1] 10 We can also leave one dimension empty if we want to retrieve all elements for that particular dimension. # all elements in the second row my_mat[2, ] ## a b c d ## 5 6 7 8 # all elements in the third column my_mat[, 3] ## A B C D ## 3 7 11 15 And when rows and columns are named we can also index based on those names: # Element located in row &#39;A&#39; and column &#39;b&#39; my_mat[&#39;A&#39;, &#39;b&#39;] ## [1] 2 9.3.2.3 Operators Once you’ve created your matrices you can do useful stuff with them and as you’d expect. Many of the functions we used in the vector lesson can be applied across an entire matrix: # mean of all elements mean(my_mat) ## [1] 8.5 However, there are also unique functions that work on matrices but not vectors. For example, we can compute the mean of each column in a matrix: colMeans(my_mat) ## a b c d ## 7 8 9 10 R has numerous built in functions to perform matrix operations. Some of the most common are given below. For example, to transpose a matrix we use the transposition function t(): my_mat_t &lt;- t(my_mat) my_mat_t ## A B C D ## a 1 5 9 13 ## b 2 6 10 14 ## c 3 7 11 15 ## d 4 8 12 16 To extract the diagonal elements of a matrix and store them as a vector we can use the diag() function: my_mat_diag &lt;- diag(my_mat) my_mat_diag ## [1] 1 6 11 16 The usual matrix addition, multiplication etc can be performed. Note the use of the %*% operator to perform matrix multiplication. mat.1 &lt;- matrix(c(2, 0, 1, 1), nrow = 2) mat.1 ## [,1] [,2] ## [1,] 2 1 ## [2,] 0 1 mat.2 &lt;- matrix(c(1, 1, 0, 2), nrow = 2) mat.2 ## [,1] [,2] ## [1,] 1 0 ## [2,] 1 2 mat.1 + mat.2 # matrix addition ## [,1] [,2] ## [1,] 3 1 ## [2,] 1 3 mat.1 * mat.2 # element by element products ## [,1] [,2] ## [1,] 2 0 ## [2,] 0 2 mat.1 %*% mat.2 # matrix multiplication ## [,1] [,2] ## [1,] 3 2 ## [2,] 1 2 9.3.2.4 Knowledge check Check out the built-in VADeaths data matrix? Subset this matrix for only male death rates. Subset for males death rates over the age of 60. Calculate averages for each column and row. 9.3.3 Lists The next data structure we will quickly take a look at is a list. Whilst vectors and matrices are constrained to contain data of the same type, lists are able to store mixtures of data types. In fact we can even store other data structures such as vectors and arrays within a list or even have a list of a list. Lists a very flexible data structures which is ideal for storing irregular or non-rectangular data. Many statistical outputs are provided as a list as well; therefore, its critical to understand how to work with lists. 9.3.3.1 Creating To create a list we can use the list() function. Note how each of the three list elements are of different classes (character, logical, and numeric) and are of different lengths. list_1 &lt;- list(c(&quot;black&quot;, &quot;yellow&quot;, &quot;orange&quot;), c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE), matrix(1:6, nrow = 3)) list_1 ## [[1]] ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; ## ## [[2]] ## [1] TRUE TRUE FALSE TRUE FALSE FALSE ## ## [[3]] ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 Elements of the list can be named during the construction of the list list_2 &lt;- list(colours = c(&quot;black&quot;, &quot;yellow&quot;, &quot;orange&quot;), evaluation = c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE), time = matrix(1:6, nrow = 3)) list_2 ## $colours ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; ## ## $evaluation ## [1] TRUE TRUE FALSE TRUE FALSE FALSE ## ## $time ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 or after the list has been created using the names() function names(list_1) &lt;- c(&quot;colors&quot;, &quot;evaluation&quot;, &quot;time&quot;) list_1 ## $colors ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; ## ## $evaluation ## [1] TRUE TRUE FALSE TRUE FALSE FALSE ## ## $time ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 We can always get a quick glimpse of the structure of a list using str(): str(list_1) ## List of 3 ## $ colors : chr [1:3] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; ## $ evaluation: logi [1:6] TRUE TRUE FALSE TRUE FALSE FALSE ## $ time : int [1:3, 1:2] 1 2 3 4 5 6 9.3.3.2 Indexing To subset lists we can utilize the single bracket [ ], double brackets [[ ]], and dollar sign $ operators. Each approach provides a specific purpose and can be combined in different ways to achieve the following subsetting objectives: Subset list and preserve output as a list Subset list and simplify output Subset list to get elements out of a list To extract one or more list items while preserving the output in list format use the [ ] operator. Its important to understand the difference between simplifying and preserving subsetting. Simplifying subsets returns the simplest possible data structure that can represent the output. Preserving subsets keeps the structure of the output the same as the input. See Hadley Wickham’s section on Simplifying vs. Preserving Subsetting to learn more. # extract first list item list_1[1] ## $colors ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; # same as above but using the item&#39;s name list_1[&#39;colors&#39;] ## $colors ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; # extract multiple list items list_1[c(&#39;colors&#39;, &#39;time&#39;)] ## $colors ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; ## ## $time ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 To extract one or more list items while simplifying the output use the [[ ]] or $ operator: # extract first list item list_1[[1]] ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; # same as above but using the item&#39;s name list_1[[&#39;colors&#39;]] ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; # same as above but using $ list_1$colors ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; One thing that differentiates the [[ operator from the \\(&lt;/code&gt; is that the &lt;code&gt;[[&lt;/code&gt; operator can be used with computed indices. The &lt;code&gt;\\) operator can only be used with literal names. To extract individual elements out of a specific list item combine the [[ (or $) operator with the [ operator: # extract the third element of the first list item list_1[[&#39;colors&#39;]][3] ## [1] &quot;orange&quot; 9.3.3.3 Operators There are less operators that you typically use directly on a list. Most of the time you are trying to extract items out of a list. However, a few useful functions that are applied to a list include: # how many items are in a list length(list_1) ## [1] 3 # the name of the list items names(list_1) ## [1] &quot;colors&quot; &quot;evaluation&quot; &quot;time&quot; # the overall structure of a list str(list_1) ## List of 3 ## $ colors : chr [1:3] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; ## $ evaluation: logi [1:6] TRUE TRUE FALSE TRUE FALSE FALSE ## $ time : int [1:3, 1:2] 1 2 3 4 5 6 9.3.3.4 Knowledge check Install and load the nycflights13 package: install.packages(&#39;nycflights13&#39;) Using the flights data provided by this package create the following regression model: This line of code is performing a linear regression model and saving the results in a list called flight_lm. We’ll discuss linear regression and modeling in later modules. flight_lm &lt;- lm(arr_delay ~ dep_delay + month + carrier, data = flights) How many items are in this list? What are the names of these list items? Extract the coefficients of this model. Extract the departure delay (dep_delay) coefficient. 9.3.4 Data frames By far the most commonly used data structure to store data is the data frame. A data frame is a powerful two-dimensional object made up of rows and columns which looks superficially very similar to a matrix. However, whilst matrices are restricted to containing data all of the same type, data frames can contain a mixture of different types of data. Typically, in a data frame each row corresponds to an individual observation and each column corresponds to a different measured or recorded variable. This setup may be familiar to those of you who use Microsoft Excel to manage and store your data. Perhaps a useful way to think about data frames is that they are essentially made up of a bunch of vectors (columns) with each vector containing its own data type but the data type can be different between vectors. As an example, the data frame below contains total quantity and sales for a grocery product category (i.e. potatoes, popcorn, frozen pizza) for each household. The data frame has four variables (columns) and each row represents an individual household. The variables household_id, total_quantity, and total_sales are numeric, product_category is a character, and multiple_items is a Boolean representing if the household bought more than one item. household_id product_category total_quantity total_sales multiple_items 1333 WATER - CARBONATED/FLVRD DRINK 1 1.19 FALSE 1372 BAG SNACKS 2 0.60 TRUE 1430 BAKED SWEET GOODS 1 1.25 FALSE 1434 SOFT DRINKS 5 14.70 TRUE 1535 SUGARS/SWEETNERS 1 2.99 FALSE 1567 BEANS - CANNED GLASS &amp; MW 2 1.98 TRUE 1650 GARDEN CENTER 1 1.29 FALSE 1693 SEAFOOD-FRESH 1 5.94 FALSE 1724 CANNED JUICES 1 2.00 FALSE 1748 BACON 1 2.50 FALSE 1750 TROPICAL FRUIT 1 0.49 FALSE 1785 VEGETABLES SALAD 1 0.99 FALSE 1950 PAPER HOUSEWARES 1 1.00 FALSE 2017 ISOTONIC DRINKS 2 2.00 TRUE 2034 APPLES 1 3.49 FALSE 2266 MUSHROOMS 1 1.99 FALSE 324 BEERS/ALES 1 4.99 FALSE 597 PASTA SAUCE 1 1.50 FALSE 852 BAKING MIXES 1 1.67 FALSE 921 LUNCHMEAT 1 0.99 FALSE There are a couple of important things to bear in mind about data frames. These types of objects are known as rectangular data as each column must have the same number of observations. Also, any missing data should be recorded as an NA just as we did with our vectors. 9.3.4.1 Creating Data frames are usually created by reading in a data set, which we’ll cover in a later lesson. However, data frames can also be created explicitly with the data.frame() function or they can be coerced from other types of objects like lists. In this case I’ll create a simple data frame df and assess its basic structure: df &lt;- data.frame(col1 = 1:3, col2 = c(&quot;this&quot;, &quot;is&quot;, &quot;text&quot;), col3 = c(TRUE, FALSE, TRUE), col4 = c(2.5, 4.2, pi)) # assess the structure of a data frame str(df) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ col1: int 1 2 3 ## $ col2: chr &quot;this&quot; &quot;is&quot; &quot;text&quot; ## $ col3: logi TRUE FALSE TRUE ## $ col4: num 2.5 4.2 3.14 # number of rows nrow(df) ## [1] 3 # number of columns ncol(df) ## [1] 4 Note how col2 in df was converted to a column of factors. This is because there is a default setting in data.frame() that converts character columns to factors. We can turn this off by setting the stringsAsFactors = FALSE argument: df &lt;- data.frame(col1 = 1:3, col2 = c(&quot;this&quot;, &quot;is&quot;, &quot;text&quot;), col3 = c(TRUE, FALSE, TRUE), col4 = c(2.5, 4.2, pi), stringsAsFactors = FALSE) # note how col2 now is of a character class str(df) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ col1: int 1 2 3 ## $ col2: chr &quot;this&quot; &quot;is&quot; &quot;text&quot; ## $ col3: logi TRUE FALSE TRUE ## $ col4: num 2.5 4.2 3.14 We can also convert pre-existing structures to a data frame. The following illustrates how we can turn multiple vectors into a data frame: v1 &lt;- 1:3 v2 &lt;-c(&quot;this&quot;, &quot;is&quot;, &quot;text&quot;) v3 &lt;- c(TRUE, FALSE, TRUE) # convert same length vectors to a data frame using data.frame() data.frame(col1 = v1, col2 = v2, col3 = v3) ## col1 col2 col3 ## 1 1 this TRUE ## 2 2 is FALSE ## 3 3 text TRUE 9.3.4.2 Indexing Data frames possess the characteristics of both lists and matrices: if you index with a single vector, they behave like lists and will return the selected columns with all rows; if you subset with two vectors, they behave like matrices and can be subset by row and column: df ## col1 col2 col3 col4 ## 1 1 this TRUE 2.5000 ## 2 2 is FALSE 4.2000 ## 3 3 text TRUE 3.1416 # subsetting by row numbers df[2:3, ] ## col1 col2 col3 col4 ## 2 2 is FALSE 4.2000 ## 3 3 text TRUE 3.1416 # subsetting by row names df[c(&quot;row2&quot;, &quot;row3&quot;), ] ## col1 col2 col3 col4 ## NA NA &lt;NA&gt; NA NA ## NA.1 NA &lt;NA&gt; NA NA # subset for both rows and columns df[1:2, c(1, 3)] ## col1 col3 ## 1 1 TRUE ## 2 2 FALSE You can also subset data frames based on conditional statements. To illustrate we’ll use the built-in mtcars data frame: head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 # all rows where mpg is greater than 20 mtcars[mtcars$mpg &gt; 20, ] ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 Its good to know that we can index and filter data frames in this fashion but later lessons will demonstrate an alternative, and more common approach to wrangle data frames. In fact, most of the lessons that follow are all focused on working with data frames! 9.4 Exercises Check out the built-in mtcars data set. What type of object is this? Apply the head() and summary() functions to mtcars, what do these functions return? Index for just the ‘mpg’ column in mtcars using three different approaches: single brackets ([), double brackets [[, and dollar sign $. How do the results differ? Use one of the methods in #3 to save the ‘mpg’ column as a vector. Now compute the mean of this vector. "],["lesson-2c-importing-data.html", "10 Lesson 2c: Importing data 10.1 Learning objectives 10.2 Data &amp; memory 10.3 Delimited files 10.4 Excel files 10.5 SQL databases 10.6 Many other file types 10.7 Exercises", " 10 Lesson 2c: Importing data The first step to any data analysis process is to get the data. Data can come from many sources but two of the most common include delimited and Excel files. This section covers how to import data from these common files; plus we cover other important topics such as understanding file paths, connecting to SQL databases, and to load data from saved R object files. 10.1 Learning objectives Upon completing this module you will be able to: Describe how imported data affects computer memory. Import tabular data with R. Assess some basic attributes of your imported data. Import alternative data files such as SQL tables and Rdata files. 10.2 Data &amp; memory R stores its data in memory - this makes it relatively quickly accessible but can cause size limitations in certain fields. In this class we will mainly work with small to moderate data sets, which means we should not run into any space limitations. R does provide tooling that allows you to work with big data via distributed data (i.e. sparklyr) and relational databrases (i.e. SQL). R memory is session-specific, so quitting R (i.e. shutting down RStudio) removes the data from memory. A general way to conceptualize data import into and use within R: Data sits in on the computer/server - this is frequently called “disk” R code can be used to copy a data file from disk to the R session’s memory R data then sits within R’s memory ready to be used by other R code Here is a visualization of this process: 10.3 Delimited files Text files are a popular way to hold and exchange tabular data as almost any data application supports exporting data to the CSV (or other text file) format. Text file formats use delimiters to separate the different elements in a line, and each line of data is in its own line in the text file. Therefore, importing different kinds of text files can follow a fairly consistent process once you’ve identified the delimiter. There are three main groups of functions that we can use to read in text files: Base R functions *readr** package functions *vroom** package functions Here, we’ll focus on the middle one. All three functions will import a tabular delimited file (.csv, .tsv, .txt, etc.) and convert it to a data frame in R, but each has subtle differences. You can read why we favor readr and vroom over the base R importing functions (i.e. read.csv()) here. We will not cover the vroom package but it is good to know about as it can be extremely fast for very large data sets. Read more about vroom here. The following will import a data set describing the sale of individual residential property in Ames, Iowa from 2006 to 2010 (source). library(readr) ames &lt;- read_csv(&#39;data/ames_raw.csv&#39;) ## Rows: 2930 Columns: 82 ## ── Column specification ─────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (45): PID, MS SubClass, MS Zoning, Street, Alley, Lot Shape, Land Contour, Utilities, L... ## dbl (37): Order, Lot Frontage, Lot Area, Overall Qual, Overall Cond, Year Built, Year Remod... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. We can look at it in our notebook or console and we see that it is displayed in a well-organized, concise manner. More on this in a second. ames ## # A tibble: 2,930 × 82 ## Order PID `MS SubClass` `MS Zoning` `Lot Frontage` `Lot Area` Street Alley `Lot Shape` ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 05263011… 020 RL 141 31770 Pave &lt;NA&gt; IR1 ## 2 2 05263500… 020 RH 80 11622 Pave &lt;NA&gt; Reg ## 3 3 05263510… 020 RL 81 14267 Pave &lt;NA&gt; IR1 ## 4 4 05263530… 020 RL 93 11160 Pave &lt;NA&gt; Reg ## 5 5 05271050… 060 RL 74 13830 Pave &lt;NA&gt; IR1 ## 6 6 05271050… 060 RL 78 9978 Pave &lt;NA&gt; IR1 ## 7 7 05271271… 120 RL 41 4920 Pave &lt;NA&gt; Reg ## 8 8 05271450… 120 RL 43 5005 Pave &lt;NA&gt; IR1 ## 9 9 05271460… 120 RL 39 5389 Pave &lt;NA&gt; IR1 ## 10 10 05271621… 060 RL 60 7500 Pave &lt;NA&gt; Reg ## # … with 2,920 more rows, and 73 more variables: `Land Contour` &lt;chr&gt;, Utilities &lt;chr&gt;, ## # `Lot Config` &lt;chr&gt;, `Land Slope` &lt;chr&gt;, Neighborhood &lt;chr&gt;, `Condition 1` &lt;chr&gt;, ## # `Condition 2` &lt;chr&gt;, `Bldg Type` &lt;chr&gt;, `House Style` &lt;chr&gt;, `Overall Qual` &lt;dbl&gt;, ## # `Overall Cond` &lt;dbl&gt;, `Year Built` &lt;dbl&gt;, `Year Remod/Add` &lt;dbl&gt;, `Roof Style` &lt;chr&gt;, ## # `Roof Matl` &lt;chr&gt;, `Exterior 1st` &lt;chr&gt;, `Exterior 2nd` &lt;chr&gt;, `Mas Vnr Type` &lt;chr&gt;, ## # `Mas Vnr Area` &lt;dbl&gt;, `Exter Qual` &lt;chr&gt;, `Exter Cond` &lt;chr&gt;, Foundation &lt;chr&gt;, ## # `Bsmt Qual` &lt;chr&gt;, `Bsmt Cond` &lt;chr&gt;, `Bsmt Exposure` &lt;chr&gt;, `BsmtFin Type 1` &lt;chr&gt;, … If we check the class of our object we do see that it is a data.frame; however, we also see some additional information. That’s because this is a special kind of data frame known as a tibble. class(ames) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; 10.3.1 Tibbles Tibbles are data frames, but they tweak some older behaviors of data frames to make life a little easier. There are two main differences in the usage of a tibble vs. a classic data frame: printing and subsetting. Tibbles have a refined print method that shows only the first 10 rows, and all the columns that fit on your screen. This makes it much easier to work with large data. We can see this difference with the following: # printed output of a tibble ames ## # A tibble: 2,930 × 82 ## Order PID `MS SubClass` `MS Zoning` `Lot Frontage` `Lot Area` Street Alley `Lot Shape` ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 05263011… 020 RL 141 31770 Pave &lt;NA&gt; IR1 ## 2 2 05263500… 020 RH 80 11622 Pave &lt;NA&gt; Reg ## 3 3 05263510… 020 RL 81 14267 Pave &lt;NA&gt; IR1 ## 4 4 05263530… 020 RL 93 11160 Pave &lt;NA&gt; Reg ## 5 5 05271050… 060 RL 74 13830 Pave &lt;NA&gt; IR1 ## 6 6 05271050… 060 RL 78 9978 Pave &lt;NA&gt; IR1 ## 7 7 05271271… 120 RL 41 4920 Pave &lt;NA&gt; Reg ## 8 8 05271450… 120 RL 43 5005 Pave &lt;NA&gt; IR1 ## 9 9 05271460… 120 RL 39 5389 Pave &lt;NA&gt; IR1 ## 10 10 05271621… 060 RL 60 7500 Pave &lt;NA&gt; Reg ## # … with 2,920 more rows, and 73 more variables: `Land Contour` &lt;chr&gt;, Utilities &lt;chr&gt;, ## # `Lot Config` &lt;chr&gt;, `Land Slope` &lt;chr&gt;, Neighborhood &lt;chr&gt;, `Condition 1` &lt;chr&gt;, ## # `Condition 2` &lt;chr&gt;, `Bldg Type` &lt;chr&gt;, `House Style` &lt;chr&gt;, `Overall Qual` &lt;dbl&gt;, ## # `Overall Cond` &lt;dbl&gt;, `Year Built` &lt;dbl&gt;, `Year Remod/Add` &lt;dbl&gt;, `Roof Style` &lt;chr&gt;, ## # `Roof Matl` &lt;chr&gt;, `Exterior 1st` &lt;chr&gt;, `Exterior 2nd` &lt;chr&gt;, `Mas Vnr Type` &lt;chr&gt;, ## # `Mas Vnr Area` &lt;dbl&gt;, `Exter Qual` &lt;chr&gt;, `Exter Cond` &lt;chr&gt;, Foundation &lt;chr&gt;, ## # `Bsmt Qual` &lt;chr&gt;, `Bsmt Cond` &lt;chr&gt;, `Bsmt Exposure` &lt;chr&gt;, `BsmtFin Type 1` &lt;chr&gt;, … # printed output of a regular data frame as.data.frame(ames) ## Order PID MS SubClass MS Zoning Lot Frontage Lot Area Street Alley Lot Shape ## 1 1 0526301100 020 RL 141 31770 Pave &lt;NA&gt; IR1 ## 2 2 0526350040 020 RH 80 11622 Pave &lt;NA&gt; Reg ## 3 3 0526351010 020 RL 81 14267 Pave &lt;NA&gt; IR1 ## 4 4 0526353030 020 RL 93 11160 Pave &lt;NA&gt; Reg ## 5 5 0527105010 060 RL 74 13830 Pave &lt;NA&gt; IR1 ## 6 6 0527105030 060 RL 78 9978 Pave &lt;NA&gt; IR1 ## 7 7 0527127150 120 RL 41 4920 Pave &lt;NA&gt; Reg ## 8 8 0527145080 120 RL 43 5005 Pave &lt;NA&gt; IR1 ## 9 9 0527146030 120 RL 39 5389 Pave &lt;NA&gt; IR1 ## 10 10 0527162130 060 RL 60 7500 Pave &lt;NA&gt; Reg ## 11 11 0527163010 060 RL 75 10000 Pave &lt;NA&gt; IR1 ## 12 12 0527165230 020 RL NA 7980 Pave &lt;NA&gt; IR1 ## Land Contour Utilities Lot Config Land Slope Neighborhood Condition 1 Condition 2 Bldg Type ## 1 Lvl AllPub Corner Gtl NAmes Norm Norm 1Fam ## 2 Lvl AllPub Inside Gtl NAmes Feedr Norm 1Fam ## 3 Lvl AllPub Corner Gtl NAmes Norm Norm 1Fam ## 4 Lvl AllPub Corner Gtl NAmes Norm Norm 1Fam ## 5 Lvl AllPub Inside Gtl Gilbert Norm Norm 1Fam ## 6 Lvl AllPub Inside Gtl Gilbert Norm Norm 1Fam ## 7 Lvl AllPub Inside Gtl StoneBr Norm Norm TwnhsE ## 8 HLS AllPub Inside Gtl StoneBr Norm Norm TwnhsE ## 9 Lvl AllPub Inside Gtl StoneBr Norm Norm TwnhsE ## 10 Lvl AllPub Inside Gtl Gilbert Norm Norm 1Fam ## 11 Lvl AllPub Corner Gtl Gilbert Norm Norm 1Fam ## 12 Lvl AllPub Inside Gtl Gilbert Norm Norm 1Fam ## House Style Overall Qual Overall Cond Year Built Year Remod/Add Roof Style Roof Matl ## 1 1Story 6 5 1960 1960 Hip CompShg ## 2 1Story 5 6 1961 1961 Gable CompShg ## 3 1Story 6 6 1958 1958 Hip CompShg ## 4 1Story 7 5 1968 1968 Hip CompShg ## 5 2Story 5 5 1997 1998 Gable CompShg ## 6 2Story 6 6 1998 1998 Gable CompShg ## 7 1Story 8 5 2001 2001 Gable CompShg ## 8 1Story 8 5 1992 1992 Gable CompShg ## 9 1Story 8 5 1995 1996 Gable CompShg ## 10 2Story 7 5 1999 1999 Gable CompShg ## 11 2Story 6 5 1993 1994 Gable CompShg ## 12 1Story 6 7 1992 2007 Gable CompShg ## Exterior 1st Exterior 2nd Mas Vnr Type Mas Vnr Area Exter Qual Exter Cond Foundation ## 1 BrkFace Plywood Stone 112 TA TA CBlock ## 2 VinylSd VinylSd None 0 TA TA CBlock ## 3 Wd Sdng Wd Sdng BrkFace 108 TA TA CBlock ## 4 BrkFace BrkFace None 0 Gd TA CBlock ## 5 VinylSd VinylSd None 0 TA TA PConc ## 6 VinylSd VinylSd BrkFace 20 TA TA PConc ## 7 CemntBd CmentBd None 0 Gd TA PConc ## 8 HdBoard HdBoard None 0 Gd TA PConc ## 9 CemntBd CmentBd None 0 Gd TA PConc ## 10 VinylSd VinylSd None 0 TA TA PConc ## 11 HdBoard HdBoard None 0 TA TA PConc ## 12 HdBoard HdBoard None 0 TA Gd PConc ## Bsmt Qual Bsmt Cond Bsmt Exposure BsmtFin Type 1 BsmtFin SF 1 BsmtFin Type 2 BsmtFin SF 2 ## 1 TA Gd Gd BLQ 639 Unf 0 ## 2 TA TA No Rec 468 LwQ 144 ## 3 TA TA No ALQ 923 Unf 0 ## 4 TA TA No ALQ 1065 Unf 0 ## 5 Gd TA No GLQ 791 Unf 0 ## 6 TA TA No GLQ 602 Unf 0 ## 7 Gd TA Mn GLQ 616 Unf 0 ## 8 Gd TA No ALQ 263 Unf 0 ## 9 Gd TA No GLQ 1180 Unf 0 ## 10 TA TA No Unf 0 Unf 0 ## 11 Gd TA No Unf 0 Unf 0 ## 12 Gd TA No ALQ 935 Unf 0 ## Bsmt Unf SF Total Bsmt SF Heating Heating QC Central Air Electrical 1st Flr SF 2nd Flr SF ## 1 441 1080 GasA Fa Y SBrkr 1656 0 ## 2 270 882 GasA TA Y SBrkr 896 0 ## 3 406 1329 GasA TA Y SBrkr 1329 0 ## 4 1045 2110 GasA Ex Y SBrkr 2110 0 ## 5 137 928 GasA Gd Y SBrkr 928 701 ## 6 324 926 GasA Ex Y SBrkr 926 678 ## 7 722 1338 GasA Ex Y SBrkr 1338 0 ## 8 1017 1280 GasA Ex Y SBrkr 1280 0 ## 9 415 1595 GasA Ex Y SBrkr 1616 0 ## 10 994 994 GasA Gd Y SBrkr 1028 776 ## 11 763 763 GasA Gd Y SBrkr 763 892 ## 12 233 1168 GasA Ex Y SBrkr 1187 0 ## Low Qual Fin SF Gr Liv Area Bsmt Full Bath Bsmt Half Bath Full Bath Half Bath Bedroom AbvGr ## 1 0 1656 1 0 1 0 3 ## 2 0 896 0 0 1 0 2 ## 3 0 1329 0 0 1 1 3 ## 4 0 2110 1 0 2 1 3 ## 5 0 1629 0 0 2 1 3 ## 6 0 1604 0 0 2 1 3 ## 7 0 1338 1 0 2 0 2 ## 8 0 1280 0 0 2 0 2 ## 9 0 1616 1 0 2 0 2 ## 10 0 1804 0 0 2 1 3 ## 11 0 1655 0 0 2 1 3 ## 12 0 1187 1 0 2 0 3 ## Kitchen AbvGr Kitchen Qual TotRms AbvGrd Functional Fireplaces Fireplace Qu Garage Type ## 1 1 TA 7 Typ 2 Gd Attchd ## 2 1 TA 5 Typ 0 &lt;NA&gt; Attchd ## 3 1 Gd 6 Typ 0 &lt;NA&gt; Attchd ## 4 1 Ex 8 Typ 2 TA Attchd ## 5 1 TA 6 Typ 1 TA Attchd ## 6 1 Gd 7 Typ 1 Gd Attchd ## 7 1 Gd 6 Typ 0 &lt;NA&gt; Attchd ## 8 1 Gd 5 Typ 0 &lt;NA&gt; Attchd ## 9 1 Gd 5 Typ 1 TA Attchd ## 10 1 Gd 7 Typ 1 TA Attchd ## 11 1 TA 7 Typ 1 TA Attchd ## 12 1 TA 6 Typ 0 &lt;NA&gt; Attchd ## Garage Yr Blt Garage Finish Garage Cars Garage Area Garage Qual Garage Cond Paved Drive ## 1 1960 Fin 2 528 TA TA P ## 2 1961 Unf 1 730 TA TA Y ## 3 1958 Unf 1 312 TA TA Y ## 4 1968 Fin 2 522 TA TA Y ## 5 1997 Fin 2 482 TA TA Y ## 6 1998 Fin 2 470 TA TA Y ## 7 2001 Fin 2 582 TA TA Y ## 8 1992 RFn 2 506 TA TA Y ## 9 1995 RFn 2 608 TA TA Y ## 10 1999 Fin 2 442 TA TA Y ## 11 1993 Fin 2 440 TA TA Y ## 12 1992 Fin 2 420 TA TA Y ## Wood Deck SF Open Porch SF Enclosed Porch 3Ssn Porch Screen Porch Pool Area Pool QC Fence ## 1 210 62 0 0 0 0 &lt;NA&gt; &lt;NA&gt; ## 2 140 0 0 0 120 0 &lt;NA&gt; MnPrv ## 3 393 36 0 0 0 0 &lt;NA&gt; &lt;NA&gt; ## 4 0 0 0 0 0 0 &lt;NA&gt; &lt;NA&gt; ## 5 212 34 0 0 0 0 &lt;NA&gt; MnPrv ## 6 360 36 0 0 0 0 &lt;NA&gt; &lt;NA&gt; ## 7 0 0 170 0 0 0 &lt;NA&gt; &lt;NA&gt; ## 8 0 82 0 0 144 0 &lt;NA&gt; &lt;NA&gt; ## 9 237 152 0 0 0 0 &lt;NA&gt; &lt;NA&gt; ## 10 140 60 0 0 0 0 &lt;NA&gt; &lt;NA&gt; ## 11 157 84 0 0 0 0 &lt;NA&gt; &lt;NA&gt; ## 12 483 21 0 0 0 0 &lt;NA&gt; GdPrv ## Misc Feature Misc Val Mo Sold Yr Sold Sale Type Sale Condition SalePrice ## 1 &lt;NA&gt; 0 5 2010 WD Normal 215000 ## 2 &lt;NA&gt; 0 6 2010 WD Normal 105000 ## 3 Gar2 12500 6 2010 WD Normal 172000 ## 4 &lt;NA&gt; 0 4 2010 WD Normal 244000 ## 5 &lt;NA&gt; 0 3 2010 WD Normal 189900 ## 6 &lt;NA&gt; 0 6 2010 WD Normal 195500 ## 7 &lt;NA&gt; 0 4 2010 WD Normal 213500 ## 8 &lt;NA&gt; 0 1 2010 WD Normal 191500 ## 9 &lt;NA&gt; 0 3 2010 WD Normal 236500 ## 10 &lt;NA&gt; 0 6 2010 WD Normal 189000 ## 11 &lt;NA&gt; 0 4 2010 WD Normal 175900 ## 12 Shed 500 3 2010 WD Normal 185000 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 2918 rows ] The differences in the subsetting are not important at this time but the main takeaway is that tibbles are more strict and will behave more consistently then data frames for certain subsetting tasks. Read more about tibbles here. 10.3.2 File paths This is a good time to have a discussion on file paths. It’s important to understand where files exist on your computer and how to reference those paths. There are two main approaches: Absolute paths Relative paths An absolute path always contains the root elements and the complete list of directories to locate the specific file or folder. For the ames_raw.csv file, the absolute path on my computer is: library(here) absolute_path &lt;- here(&#39;data/ames_raw.csv&#39;) absolute_path ## [1] &quot;/Users/b294776/Desktop/workspace/training/UC/uc-bana-7025/data/ames_raw.csv&quot; I can always use the absolute path in read_csv(): ames &lt;- read_csv(absolute_path) ## Rows: 2930 Columns: 82 ## ── Column specification ─────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (45): PID, MS SubClass, MS Zoning, Street, Alley, Lot Shape, Land Contour, Utilities, L... ## dbl (37): Order, Lot Frontage, Lot Area, Overall Qual, Overall Cond, Year Built, Year Remod... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. In contrast, a relative path is a path built starting from the current location. For example, say that I am operating in a directory called “Project A”. If I’m working in “my_notebook.Rmd” and I have a “my_data.csv” file in that same directory: # illustration of the directory layout Project A ├── my_notebook.Rmd └── my_data.csv Then I can use this relative path to import this file: read_csv('my_data.csv'). This just means to look for the ‘my_data.csv’ file relative to the current directory that I am in. Often, people store data in a “data” directory. If this directory is a subdirectory within my Project A directory: # illustration of the directory layout Project A ├── my_notebook.Rmd └── data └── my_data.csv Then I can use this relative path to import this file: read_csv('data/my_data.csv'). This just means to look for the ‘data’ subdirectory relative to the current directory that I am in and then look for the ‘my_data.csv’ file. Sometimes, the data directory may not be in the current directory. Sometimes a project directory will look like the following where there is a subdirectory containing multiple notebooks and then another subdirectory containing data assets. If you are working in “notebook1.Rmd” within the notebooks subdirectory, you will need to tell R to go up one directory relative to the notebook you are working in to the main Project A directory and then go down into the data directory. # illustration of the directory layout Project A ├── notebooks │ ├── notebook1.Rmd │ ├── notebook2.Rmd │ └── notebook3.Rmd └── data └── my_data.csv I can do this by using dot-notation in my relative path specification - here I use ‘..’ to imply “go up one directory relative to my current location”: read_csv('../data/my_data.csv'). Note that the path specified in pd.read_csv() does not need to be a local path. For example, the ames_raw.csv data is located online at https://raw.githubusercontent.com/bradleyboehmke/uc-bana-7025/main/data/ames_raw.csv. We can use read_csv() to import directly from this location: url &lt;- &#39;https://raw.githubusercontent.com/bradleyboehmke/uc-bana-7025/main/data/ames_raw.csv&#39; ames &lt;- read_csv(url) ## Rows: 2930 Columns: 82 ## ── Column specification ─────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (45): PID, MS SubClass, MS Zoning, Street, Alley, Lot Shape, Land Contour, Utilities, L... ## dbl (37): Order, Lot Frontage, Lot Area, Overall Qual, Overall Cond, Year Built, Year Remod... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 10.3.3 Metadata Once we’ve imported the data we can get some descriptive metadata about our data frame. For example, we can get the dimensions of our data frame. Here, we see that we have 2,930 rows and 82 columns. dim(ames) ## [1] 2930 82 We can also see all the names of our columns: names(ames) ## [1] &quot;Order&quot; &quot;PID&quot; &quot;MS SubClass&quot; &quot;MS Zoning&quot; &quot;Lot Frontage&quot; ## [6] &quot;Lot Area&quot; &quot;Street&quot; &quot;Alley&quot; &quot;Lot Shape&quot; &quot;Land Contour&quot; ## [11] &quot;Utilities&quot; &quot;Lot Config&quot; &quot;Land Slope&quot; &quot;Neighborhood&quot; &quot;Condition 1&quot; ## [16] &quot;Condition 2&quot; &quot;Bldg Type&quot; &quot;House Style&quot; &quot;Overall Qual&quot; &quot;Overall Cond&quot; ## [21] &quot;Year Built&quot; &quot;Year Remod/Add&quot; &quot;Roof Style&quot; &quot;Roof Matl&quot; &quot;Exterior 1st&quot; ## [26] &quot;Exterior 2nd&quot; &quot;Mas Vnr Type&quot; &quot;Mas Vnr Area&quot; &quot;Exter Qual&quot; &quot;Exter Cond&quot; ## [31] &quot;Foundation&quot; &quot;Bsmt Qual&quot; &quot;Bsmt Cond&quot; &quot;Bsmt Exposure&quot; &quot;BsmtFin Type 1&quot; ## [36] &quot;BsmtFin SF 1&quot; &quot;BsmtFin Type 2&quot; &quot;BsmtFin SF 2&quot; &quot;Bsmt Unf SF&quot; &quot;Total Bsmt SF&quot; ## [41] &quot;Heating&quot; &quot;Heating QC&quot; &quot;Central Air&quot; &quot;Electrical&quot; &quot;1st Flr SF&quot; ## [46] &quot;2nd Flr SF&quot; &quot;Low Qual Fin SF&quot; &quot;Gr Liv Area&quot; &quot;Bsmt Full Bath&quot; &quot;Bsmt Half Bath&quot; ## [51] &quot;Full Bath&quot; &quot;Half Bath&quot; &quot;Bedroom AbvGr&quot; &quot;Kitchen AbvGr&quot; &quot;Kitchen Qual&quot; ## [56] &quot;TotRms AbvGrd&quot; &quot;Functional&quot; &quot;Fireplaces&quot; &quot;Fireplace Qu&quot; &quot;Garage Type&quot; ## [61] &quot;Garage Yr Blt&quot; &quot;Garage Finish&quot; &quot;Garage Cars&quot; &quot;Garage Area&quot; &quot;Garage Qual&quot; ## [66] &quot;Garage Cond&quot; &quot;Paved Drive&quot; &quot;Wood Deck SF&quot; &quot;Open Porch SF&quot; &quot;Enclosed Porch&quot; ## [71] &quot;3Ssn Porch&quot; &quot;Screen Porch&quot; &quot;Pool Area&quot; &quot;Pool QC&quot; &quot;Fence&quot; ## [76] &quot;Misc Feature&quot; &quot;Misc Val&quot; &quot;Mo Sold&quot; &quot;Yr Sold&quot; &quot;Sale Type&quot; ## [81] &quot;Sale Condition&quot; &quot;SalePrice&quot; You may have also noticed the message each time we read in the data set that identified the delimiter and it also showed the following, which states that when we read in the data, 45 variables were read in as character strings and 37 were read in as double floating points. chr (45): PID, MS SubClass, MS Zoning, Street, Alley, Lot Shape, Land Contour, Utilities,... dbl (37): Order, Lot Frontage, Lot Area, Overall Qual, Overall Cond, Year Built, Year Rem... There was also a message that stated “use spec() to retrieve the full column specification for this data.” When we do so we see that it lists all the columns and the data type that were read in as. spec(ames) ## cols( ## Order = col_double(), ## PID = col_character(), ## `MS SubClass` = col_character(), ## `MS Zoning` = col_character(), ## `Lot Frontage` = col_double(), ## `Lot Area` = col_double(), ## Street = col_character(), ## Alley = col_character(), ## `Lot Shape` = col_character(), ## `Land Contour` = col_character(), ## Utilities = col_character(), ## `Lot Config` = col_character(), ## `Land Slope` = col_character(), ## Neighborhood = col_character(), ## `Condition 1` = col_character(), ## `Condition 2` = col_character(), ## `Bldg Type` = col_character(), ## `House Style` = col_character(), ## `Overall Qual` = col_double(), ## `Overall Cond` = col_double(), ## `Year Built` = col_double(), ## `Year Remod/Add` = col_double(), ## `Roof Style` = col_character(), ## `Roof Matl` = col_character(), ## `Exterior 1st` = col_character(), ## `Exterior 2nd` = col_character(), ## `Mas Vnr Type` = col_character(), ## `Mas Vnr Area` = col_double(), ## `Exter Qual` = col_character(), ## `Exter Cond` = col_character(), ## Foundation = col_character(), ## `Bsmt Qual` = col_character(), ## `Bsmt Cond` = col_character(), ## `Bsmt Exposure` = col_character(), ## `BsmtFin Type 1` = col_character(), ## `BsmtFin SF 1` = col_double(), ## `BsmtFin Type 2` = col_character(), ## `BsmtFin SF 2` = col_double(), ## `Bsmt Unf SF` = col_double(), ## `Total Bsmt SF` = col_double(), ## Heating = col_character(), ## `Heating QC` = col_character(), ## `Central Air` = col_character(), ## Electrical = col_character(), ## `1st Flr SF` = col_double(), ## `2nd Flr SF` = col_double(), ## `Low Qual Fin SF` = col_double(), ## `Gr Liv Area` = col_double(), ## `Bsmt Full Bath` = col_double(), ## `Bsmt Half Bath` = col_double(), ## `Full Bath` = col_double(), ## `Half Bath` = col_double(), ## `Bedroom AbvGr` = col_double(), ## `Kitchen AbvGr` = col_double(), ## `Kitchen Qual` = col_character(), ## `TotRms AbvGrd` = col_double(), ## Functional = col_character(), ## Fireplaces = col_double(), ## `Fireplace Qu` = col_character(), ## `Garage Type` = col_character(), ## `Garage Yr Blt` = col_double(), ## `Garage Finish` = col_character(), ## `Garage Cars` = col_double(), ## `Garage Area` = col_double(), ## `Garage Qual` = col_character(), ## `Garage Cond` = col_character(), ## `Paved Drive` = col_character(), ## `Wood Deck SF` = col_double(), ## `Open Porch SF` = col_double(), ## `Enclosed Porch` = col_double(), ## `3Ssn Porch` = col_double(), ## `Screen Porch` = col_double(), ## `Pool Area` = col_double(), ## `Pool QC` = col_character(), ## Fence = col_character(), ## `Misc Feature` = col_character(), ## `Misc Val` = col_double(), ## `Mo Sold` = col_double(), ## `Yr Sold` = col_double(), ## `Sale Type` = col_character(), ## `Sale Condition` = col_character(), ## SalePrice = col_double() ## ) Lastly, its always good to understand if, and how many, missing values are in the data set we can do this easily by running the following, which shows 13,997 elements are missing. That’s a lot! sum(is.na(ames)) ## [1] 13997 We can even apply some operators and indexing procedures we learned in previous lessons to quickly view all columns with missing values and get a total sum of the missing values within those columns. In future modules we’ll learn different ways we can handle these missing values. missing_values &lt;- colSums(is.na(ames)) sort(missing_values[missing_values &gt; 0], decreasing = TRUE) ## Pool QC Misc Feature Alley Fence Fireplace Qu Lot Frontage ## 2917 2824 2732 2358 1422 490 ## Garage Yr Blt Garage Finish Garage Qual Garage Cond Garage Type Bsmt Exposure ## 159 159 159 159 157 83 ## BsmtFin Type 2 Bsmt Qual Bsmt Cond BsmtFin Type 1 Mas Vnr Type Mas Vnr Area ## 81 80 80 80 23 23 ## Bsmt Full Bath Bsmt Half Bath BsmtFin SF 1 BsmtFin SF 2 Bsmt Unf SF Total Bsmt SF ## 2 2 1 1 1 1 ## Electrical Garage Cars Garage Area ## 1 1 1 10.3.4 Knowledge check Check out the help documentation for read_csv() by executing ?read_csv. What parameter in read_csv() allows us to specify values that represent missing values? Read in this energy_consumption.csv file. What are the dimensions of this data? What data type is each column? Apply dplyr::glimpse(ames). What information does this provide? 10.4 Excel files With Excel still being the spreadsheet software of choice its important to be able to efficiently import and export data from these files. Often, many users will simply resort to exporting the Excel file as a CSV file and then import into R using readxl::read_excel; however, this is far from efficient. This section will teach you how to eliminate the CSV step and to import data directly from Excel using the readxl package. To illustrate, we’ll import so mock grocery store products data located in a products.xlsx file. To read in Excel data you will use the excel_sheets() functions. This allows you to read the names of the different worksheets in the Excel workbook and identify the specific worksheet of interest and then specify that in read_excel. library(readxl) excel_sheets(&#39;data/products.xlsx&#39;) ## [1] &quot;metadata&quot; &quot;products data&quot; &quot;grocery list&quot; If you don’t explicitly specify a sheet then the first worksheet will be imported. products &lt;- read_excel(&#39;data/products.xlsx&#39;, sheet = &#39;products data&#39;) products ## # A tibble: 151,141 × 5 ## product_num department commodity brand_ty x5 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 92993 NON-FOOD PET PRIVATE N ## 2 93924 NON-FOOD PET PRIVATE N ## 3 94272 NON-FOOD PET PRIVATE N ## 4 94299 NON-FOOD PET PRIVATE N ## 5 94594 NON-FOOD PET PRIVATE N ## 6 94606 NON-FOOD PET PRIVATE N ## 7 94613 NON-FOOD PET PRIVATE N ## 8 95625 NON-FOOD PET PRIVATE N ## 9 96152 NON-FOOD PET PRIVATE N ## 10 96153 NON-FOOD PET PRIVATE N ## # … with 151,131 more rows 10.5 SQL databases Many organizations continue to use relational databases along with SQL to interact with these data assets. If you are unfamiliar with relational databases and SQL then this is a quick read that explains the benefits of these tools. R can connect to almost any existing database type. Most common database types have R packages that allow you to connect to them (e.g., RSQLite, RMySQL, etc). Furthermore, the DBI and dbplyr packages support connecting to the widely-used open source databases sqlite, mysql and postgresql, as well as Google’s bigquery, and it can also be extended to other database types . RStudio has created a website that provides documentation and best practices to work on database interfaces. In this example I will illustrate connecting to a local sqlite database. First, let’s load the libraries we’ll need: library(dbplyr) library(dplyr) library(RSQLite) The following illustrates with the example Chinook Database, which I’ve downloaded to my data directory. The following uses 2 functions that talk to the SQLite database. dbconnect() comes from the DBI package and is not something that you’ll use directly as a user. It simply allows R to send commands to databases irrespective of the database management system used. The SQLite() function from the RSQLite package allows R to interface with SQLite databases. chinook &lt;- dbConnect(SQLite(), &quot;data/chinook.db&quot;) This command does not load the data into the R session (as the read_csv() function did). Instead, it merely instructs R to connect to the SQLite database contained in the chinook.db file. Using a similar approach, you could connect to many other database management systems that are supported by R including MySQL, PostgreSQL, BigQuery, etc. Let’s take a closer look at the chinook database we just connected to: src_dbi(chinook) ## src: sqlite 3.38.5 [/Users/b294776/Desktop/workspace/training/UC/uc-bana-7025/data/chinook.db] ## tbls: albums, artists, customers, employees, genres, invoice_items, invoices, media_types, ## playlist_track, playlists, sqlite_sequence, sqlite_stat1, tracks Just like a spreadsheet with multiple worksheets, a SQLite database can contain multiple tables. In this case there are several tables listed in the tbls row in the output above: albums artists customers etc. Once you’ve made the connection, you can use tbl() to read in the “tracks” table directly as a data frame. tracks &lt;- tbl(chinook, &#39;tracks&#39;) If you are familiar with SQL then you can even pass a SQL query directly in the tbl() call using dbplyr’s sql(). For example, the following SQL query: SELECTS the name, composer, and milliseconds columns, FROM the tracks table, WHERE observations in the milliseconds column are greater than 200,000 and WHERE observations in the composer column are not missing (NULL) sql_query &lt;- &quot;SELECT name, composer, milliseconds FROM tracks WHERE milliseconds &gt; 200000 and composer is not null&quot; long_tracks &lt;- tbl(chinook, sql(sql_query)) long_tracks ## # Source: SQL [?? x 3] ## # Database: sqlite 3.38.5 ## # [/Users/b294776/Desktop/workspace/training/UC/uc-bana-7025/data/chinook.db] ## name composer milliseconds ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 For Those About To Rock (We Salute You) Angus Young, Malcolm Young, Brian John… 343719 ## 2 Fast As a Shark F. Baltes, S. Kaufman, U. Dirkscneider… 230619 ## 3 Restless and Wild F. Baltes, R.A. Smith-Diesel, S. Kaufm… 252051 ## 4 Princess of the Dawn Deaffy &amp; R.A. Smith-Diesel 375418 ## 5 Put The Finger On You Angus Young, Malcolm Young, Brian John… 205662 ## 6 Let&#39;s Get It Up Angus Young, Malcolm Young, Brian John… 233926 ## 7 Inject The Venom Angus Young, Malcolm Young, Brian John… 210834 ## 8 Snowballed Angus Young, Malcolm Young, Brian John… 203102 ## 9 Evil Walks Angus Young, Malcolm Young, Brian John… 263497 ## 10 Breaking The Rules Angus Young, Malcolm Young, Brian John… 263288 ## # … with more rows 10.6 Many other file types There are many other file types that you may encounter in your career. Most of which we can import into R one way or another. For example the haven package reads SPSS, Stata, and SAS files, xml2 allows you to read in XML, and rvest helps to scrape data from HTML web pages. Two non-tabular file types you may experience in practice are JSON files and R object files. 10.6.1 JSON files JSON files are non-tabular data files that are popular in data engineering due to their space efficiency and flexibility. Here is an example JSON file: { &quot;planeId&quot;: &quot;1xc2345g&quot;, &quot;manufacturerDetails&quot;: { &quot;manufacturer&quot;: &quot;Airbus&quot;, &quot;model&quot;: &quot;A330&quot;, &quot;year&quot;: 1999 }, &quot;airlineDetails&quot;: { &quot;currentAirline&quot;: &quot;Southwest&quot;, &quot;previousAirlines&quot;: { &quot;1st&quot;: &quot;Delta&quot; }, &quot;lastPurchased&quot;: 2013 }, &quot;numberOfFlights&quot;: 4654 } JSON Files can be imported using the jsonlite library. library(jsonlite) imported_json &lt;- fromJSON(&#39;data/json_example.json&#39;) Since JSON files can have multiple dimensions, jsonlite will import JSON files as a list since that is the most flexible data structure in R. class(imported_json) ## [1] &quot;list&quot; And we can view the data: imported_json ## $planeId ## [1] &quot;1xc2345g&quot; ## ## $manufacturerDetails ## $manufacturerDetails$manufacturer ## [1] &quot;Airbus&quot; ## ## $manufacturerDetails$model ## [1] &quot;A330&quot; ## ## $manufacturerDetails$year ## [1] 1999 ## ## ## $airlineDetails ## $airlineDetails$currentAirline ## [1] &quot;Southwest&quot; ## ## $airlineDetails$previousAirlines ## $airlineDetails$previousAirlines$`1st` ## [1] &quot;Delta&quot; ## ## ## $airlineDetails$lastPurchased ## [1] 2013 ## ## ## $numberOfFlights ## [1] 4654 10.6.2 R object files Sometimes you may need to save data or other R objects outside of your workspace. You may want to share R data/objects with co-workers, transfer between projects or computers, or simply archive them. There are three primary ways that people tend to save R data/objects: as .RData, .rda, or as .rds files. You can read about the differences of these R objects here. If you have an .RData or .rda file you need to load you can do so with the following. You will not seen any output in your console or script because this simply loads the data objects within xy.RData into your global environment. load(&#39;data/xy.RData&#39;) For .rds files you can use readr’s read_rds() function: read_rds(&#39;data/x.rds&#39;) ## [1] 0.758310 0.999693 0.314913 0.978646 0.168522 0.597689 0.209016 0.848492 0.073632 0.106206 ## [11] 0.116106 0.696880 0.725726 0.011556 0.052708 0.472079 0.072446 0.786448 0.924837 0.228298 10.7 Exercises R stores its data in _______ . What happens to R’s data when the R or RStudio session is terminated? Load the hearts.csv data file into R using the readr library. What are the dimensions of this data? What data types are the variables in this data set? Use the head() and tail() functions to assess the first and last 15 rows of this data set. Now import the hearts_data_dictionary.csv file, which provides some information on each variable. Do the data types of the hearts.csv variables align with the description of each variable? "],["lab-1.html", "11 Lab", " 11 Lab TBD "],["overview-2.html", "12 Overview 12.1 Learning objectives 12.2 Tasks 12.3 Course readings", " 12 Overview In the last module we discussed general guidelines for first interacting with a new data set. In module 3 we want to build on those activities by learning how to clean and tidy our data, and then beginning our journey to creating insights with data through data manipulation. 12.1 Learning objectives By the end of this module you should be able to: Explain the basic concepts of “tidy” data. Perform data tidying tasks with R such as reshaping, splitting, and combining data along with handling missing values. Manipulate your data by applying filters, selecting and renaming columns, creating new variables, and more. Compute descriptive statistics across all observations and within different grouped levels of observations. 12.2 Tasks TBD 12.3 Course readings TBD "],["lesson-3a-pipe-operator.html", "13 Lesson 3a: Pipe operator 13.1 Learning objectives 13.2 Pipe (%&gt;%) operator 13.3 Additional pipe operators (optional) 13.4 Additional resources 13.5 Exercises", " 13 Lesson 3a: Pipe operator Removing duplication is an important principle to keep in mind with your code; however, equally important is to keep your code efficient and readable. Efficiency is often accomplished by leveraging functions and iteration in your code (which we cover later in this class). However, efficiency also includes eliminating the creation and saving of unnecessary objects that often result when you are trying to make your code more readable, clear, and explicit. Consequently, writing code that is simple, readable, and efficient is often considered contradictory. For this reason, the magrittr package is a powerful tool to have in your data wrangling toolkit. The magrittr package was created by Stefan Milton Bache and, in Stefan’s words, has two primary aims: “to decrease development time and to improve readability and maintainability of code.” Hence, it aims to increase efficiency and improve readability; and in the process it greatly simplifies your code. This lesson covers the basics of the magrittr toolkit. The primary function in the magrittr package is the pipe operator (%&gt;%), this operator has been incorporated in many packages and you will see it used often throughout this class. 13.1 Learning objectives Upon completing this module you will be able to: Explain the different approaches commonly used to chain multiple expressions together. Understand how the pipe (%&gt;%) operator works along with some alternative pipe operators. 13.2 Pipe (%&gt;%) operator The principal function provided by the magrittr package is %&gt;%, or what’s called the “pipe” operator. This operator will forward a value, or the result of an expression, into the next function call/expression. For instance a function to filter data can be written as: filter(data, variable == numeric_value) or data %&gt;% filter(variable == numeric_value) Both functions complete the same task and the benefit of using %&gt;% may not be immediately evident; however, when you desire to perform multiple functions its advantage becomes obvious. For instance, if we want to filter some data, group it by categories, summarize it, and then order the summarized results we could write it out three different ways. Don’t worry about the details of this code, this is mainly for illustration purposes. You will learn all about these functions in later lessons! Nested Option: # provides the various arrange, summarize, etc functions library(dplyr) # perform nested functions arrange( summarize( group_by( filter(df, quantity &gt; 1), store_id ), avg_sales = mean(sales_value) ), desc(avg_sales) ) ## # A tibble: 188 × 2 ## store_id avg_sales ## &lt;chr&gt; &lt;dbl&gt; ## 1 736 86.0 ## 2 197 43.5 ## 3 901 43.5 ## 4 2903 43 ## 5 476 32.8 ## 6 634 25.7 ## 7 3149 24.9 ## 8 3479 19.6 ## 9 656 19.1 ## 10 3131 15 ## # … with 178 more rows This first option is considered a “nested” option such the functions are nested within one another. Historically, this has been the traditional way of integrating code; however, it becomes extremely difficult to read what exactly the code is doing and it also becomes easier to make mistakes when making updates to your code. Although not in violation of the DRY principle6, it definitely violates the basic principle of readability and clarity, which makes communication of your analysis more difficult. To make things more readable, people often move to the following approach… Multiple Object Option: a &lt;- filter(df, quantity &gt; 1) b &lt;- group_by(a, store_id) c &lt;- summarise(b, avg_sales = mean(sales_value)) d &lt;- arrange(c, desc(avg_sales)) print(d) ## # A tibble: 188 × 2 ## store_id avg_sales ## &lt;chr&gt; &lt;dbl&gt; ## 1 736 86.0 ## 2 197 43.5 ## 3 901 43.5 ## 4 2903 43 ## 5 476 32.8 ## 6 634 25.7 ## 7 3149 24.9 ## 8 3479 19.6 ## 9 656 19.1 ## 10 3131 15 ## # … with 178 more rows This second option helps in making the data wrangling steps more explicit and obvious but definitely violates the DRY principle. By sequencing multiple functions in this way you are likely saving multiple outputs that are not very informative to you or others; rather, the only reason you save them is to insert them into the next function to eventually get the final output you desire. This inevitably creates unnecessary copies and wrecks havoc on properly managing your objects…basically it results in a global environment charlie foxtrot! To provide the same readability (or even better), we can use %&gt;% to string these arguments together without unnecessary object creation… %&gt;% Option: df %&gt;% filter(quantity &gt; 1) %&gt;% group_by(store_id) %&gt;% summarise(avg_sales = mean(sales_value)) %&gt;% arrange(desc(avg_sales)) ## # A tibble: 188 × 2 ## store_id avg_sales ## &lt;chr&gt; &lt;dbl&gt; ## 1 736 86.0 ## 2 197 43.5 ## 3 901 43.5 ## 4 2903 43 ## 5 476 32.8 ## 6 634 25.7 ## 7 3149 24.9 ## 8 3479 19.6 ## 9 656 19.1 ## 10 3131 15 ## # … with 178 more rows This final option which integrates %&gt;% operators makes for more efficient and legible code. Its efficient in that it doesn’t save unnecessary objects (as in option 2) and performs as effectively (as both option 1 &amp; 2) but makes your code more readable in the process. Its legible in that you can read this as you would read normal prose (we read the %&gt;% as “and then”): “take df and then filter and then group by and then summarize and then arrange.” Notice how above we didn’t have to load the magrittr package to use the pipe operator (%&gt;%)? This is because the pipe operator has been incorporated into the dplyr package and since we loaded that package we have direct access to %&gt;%. In fact, all tidyverse packages have incorporated the pipe operator. And since R is a functional programming language, meaning that everything you do is basically built on functions, you can use the pipe operator to feed into just about any argument call. For example, we can pipe into a linear regression function and then get the summary of the regression parameters. Note in this case I insert “data = .” into the lm() function. When using the %&gt;% operator the default is the argument that you are forwarding will go in as the first argument of the function that follows the %&gt;%. However, in some functions the argument you are forwarding does not go into the default first position. In these cases, you place “.” to signal which argument you want the forwarded expression to go to. df %&gt;% filter(store_id == &quot;367&quot;) %&gt;% lm(sales_value ~ week + retail_disc, data = .) %&gt;% summary() ## ## Call: ## lm(formula = sales_value ~ week + retail_disc, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.62 -2.19 -1.16 0.34 47.42 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.10845 0.20549 15.13 &lt;2e-16 *** ## week 0.00185 0.00641 0.29 0.77 ## retail_disc 1.09089 0.10396 10.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.5 on 2126 degrees of freedom ## Multiple R-squared: 0.0493, Adjusted R-squared: 0.0484 ## F-statistic: 55.1 on 2 and 2126 DF, p-value: &lt;2e-16 You can also use %&gt;% to feed into plots: You will learn more about plotting techniques in week 5. # visualization package library(ggplot2) df %&gt;% filter(store_id == &quot;367&quot;, week &lt;= 10) %&gt;% ggplot(aes(x = factor(week), y = sales_value)) + geom_jitter(width = .05, alpha = .4) + geom_boxplot(alpha = .1) 13.3 Additional pipe operators (optional) You should only review this section after you have a firm grasp on how to perform basic data transformation and tidying procedures. Consequently, this is a good section to come back to later in the week. Don’t worry, you will not be quizzed on any of the content that follows! magrittr also offers some alternative pipe operators. Some functions, such as plotting functions, will cause the string of piped arguments to terminate. The tee (%T&gt;%) operator allows you to continue piping functions that normally cause termination. # normal piping terminates with the plot() function resulting in # NULL results for the summary() function df %&gt;% filter(store_id == &quot;367&quot;) %&gt;% select(quantity, sales_value) %&gt;% plot() %&gt;% summary() ## Length Class Mode ## 0 NULL NULL # load magrittr to use additional pipe operators library(magrittr) # inserting %T&gt;% allows you to plot and perform the functions that # follow the plotting function df %&gt;% filter(store_id == &quot;367&quot;) %&gt;% select(quantity, sales_value) %T&gt;% plot() %&gt;% summary() ## quantity sales_value ## Min. : 0 Min. : 0.00 ## 1st Qu.: 1 1st Qu.: 1.49 ## Median : 1 Median : 2.50 ## Mean : 166 Mean : 3.62 ## 3rd Qu.: 1 3rd Qu.: 3.99 ## Max. :23735 Max. :53.14 The compound assignment %&lt;&gt;% operator is used to update a value by first piping it into one or more expressions, and then assigning the result. For instance, let’s say you want to transform the sales_value variable to a logarithmic measurement. Using %&lt;&gt;% will perform the functions to the right of %&lt;&gt;% and save the changes these functions perform to the variable or data frame called to the left of %&lt;&gt;%. # note that sales_value is in its typical measurement head(df$sales_value) ## [1] 3.86 1.59 1.00 11.87 1.29 2.50 # we can log transform sales_value and save this change using %&lt;&gt;% df$sales_value %&lt;&gt;% log head(df$sales_value) ## [1] 1.35067 0.46373 0.00000 2.47401 0.25464 0.91629 You should be cautious in your use of %&lt;&gt;% since it does not change the name of the variable and you are overwriting the original variable’s values. Some functions (e.g. lm, aggregate, cor) have a data argument, which allows the direct use of names inside the data as part of the call. The exposition (%$%) operator is useful when you want to pipe a data frame, which may contain many columns, into a function that is only applied to some of the columns. For example, the correlation (cor) function only requires an x and y argument so if you pipe the customer transaction data into the cor function using %&gt;% you will get an error because cor doesn’t know how to handle df. However, using %$% allows you to say “take this data frame and then perform cor() on these specified columns within df.” # regular piping results in an error df %&gt;% filter(store_id == &quot;367&quot;) %&gt;% cor(retail_disc, quantity) ## Error in pmatch(use, c(&quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;pairwise.complete.obs&quot;, : object &#39;quantity&#39; not found # using %$% allows you to specify variables of interest df %&gt;% filter(store_id == &quot;367&quot;) %$% cor(retail_disc, quantity) ## [1] 0.10263 13.4 Additional resources The magrittr package and its pipe operators are a great tool for making your code simple, efficient, and readable. There are limitations, or at least suggestions, on when and how you should use the operators. Garrett Grolemund and Hadley Wickham offer some advice on the proper use of pipe operators in their R for Data Science book. However, the %&gt;% has greatly transformed our ability to write “simplified” code in R. As the pipe gains in popularity you will likely find it in more future packages and being familiar will likely result in better communication of your code. Some additional resources regarding magrittr and the pipe operators you may find useful: The magrittr vignette (vignette(\"magrittr\")) in your console) provides additional examples of using pipe operators and functions provided by magrittr. A blog post by Stefan Milton Bache regarding the past, present and future of magrittr magrittr questions on Stack Overflow The ensurer package, also written by Stefan Milton Bache, provides a useful way of verifying and validating data outputs in a sequence of pipe operators. 13.5 Exercises Look at the code the follows. How would you rewrite this code using the pipe operator? foo_foo_1 &lt;- hop(foo_foo, through = forest) foo_foo_2 &lt;- scoop(foo_foo_1, up = field_mice) foo_foo_3 &lt;- bop(foo_foo_2, on = head) Don’t repeat yourself (DRY) is a software development principle aimed at reducing repetition. Formulated by Andy Hunt and Dave Thomas in their book The Pragmatic Programmer, the DRY principle states that “every piece of knowledge must have a single, unambiguous, authoritative representation within a system.” This principle has been widely adopted to imply that you should not duplicate code. Although the principle was meant to be far grander than that, there’s plenty of merit behind this slight misinterpretation.↩︎ "],["lesson-3b-data-transformation.html", "14 Lesson 3b: Data transformation 14.1 Learning objectives 14.2 Prerequisites 14.3 Filtering observations 14.4 Selecting variables 14.5 Computing summary statistics 14.6 Sorting observations 14.7 Creating new variables 14.8 Putting it altogether 14.9 Exercises 14.10 Additional resources", " 14 Lesson 3b: Data transformation When wrangling data you often need to create some new variables and filter for certain observations of interest, or maybe you just want to rename the variables or reorder the observations in order to make the data a little easier to work with. You’ll learn how to do all that (and more!) in this lesson, which will teach you how to transform your data using the dplyr package 14.1 Learning objectives Upon completing this module you will be able to: Filter a data frame for observations of interest. Select and/or rename specific variables. Compute summary statistics. Sort observations. Create new variables. 14.2 Prerequisites Load the dplyr package to provide you access to the functions we’ll cover in this lesson. library(dplyr) Alternatively, you could load the tidyverse package, which automatically loads the dplyr package. To illustrate various transformation tasks we will use the following customer transaction data from the completejourney package: library(completejourney) (df &lt;- transactions_sample) ## # A tibble: 75,000 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2261 309 31625220889 940996 1 3.86 0.43 0 ## 2 2131 368 32053127496 873902 1 1.59 0.9 0 ## 3 511 316 32445856036 847901 1 1 0.69 0 ## 4 400 388 31932241118 13094913 2 11.9 2.9 0 ## 5 918 340 32074655895 1085604 1 1.29 0 0 ## 6 718 324 32614612029 883203 1 2.5 0.49 0 ## 7 868 323 32074722463 9884484 1 3.49 0 0 ## 8 1688 450 34850403304 1028715 1 2 1.79 0 ## 9 467 31782 31280745102 896613 2 6.55 4.44 0 ## 10 1947 32004 32744181707 978497 1 3.99 0 0 ## # … with 74,990 more rows, and 3 more variables: coupon_match_disc &lt;dbl&gt;, week &lt;int&gt;, ## # transaction_timestamp &lt;dttm&gt; 14.3 Filtering observations Filtering data is a common task to identify and select observations in which a particular variable matches a specific value or condition. The filter() function provides this capability. The first argument is the name of the data frame. The second and subsequent arguments are the expressions that filter the data frame. For example, if we want to filter for only transactions at the store with ID 309: filter(df, store_id == &quot;309&quot;) ## # A tibble: 426 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2261 309 31625220889 940996 1 3.86 0.43 0 ## 2 519 309 32931585175 1029743 1 2.69 0 0 ## 3 1073 309 40532931248 1058997 1 1.89 0 0 ## 4 517 309 32989806742 5590158 1 1 0.9 0 ## 5 1023 309 35713891195 15596520 1 4.59 0.4 0 ## 6 1770 309 32505226867 12263600 1 1.5 0.29 0 ## 7 1257 309 33983222000 999090 1 3.49 0 0 ## 8 1509 309 32090471323 1016800 2 6.67 2.51 0 ## 9 1167 309 34103947117 1037417 1 3.65 0 0 ## 10 1836 309 40532932017 879008 1 6.99 1 0 ## # … with 416 more rows, and 3 more variables: coupon_match_disc &lt;dbl&gt;, week &lt;int&gt;, ## # transaction_timestamp &lt;dttm&gt; You can pipe data into the filter function using the following syntax: df %&gt;% filter(store_id == “309”) When you run that line of code, dplyr executes the filtering operation and returns a new data frame. dplyr functions never modify their inputs, so if you want to save the result, you’ll need to use the assignment operator, &lt;-: store_309 &lt;- filter(df, store_id == &quot;309&quot;) store_309 ## # A tibble: 426 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2261 309 31625220889 940996 1 3.86 0.43 0 ## 2 519 309 32931585175 1029743 1 2.69 0 0 ## 3 1073 309 40532931248 1058997 1 1.89 0 0 ## 4 517 309 32989806742 5590158 1 1 0.9 0 ## 5 1023 309 35713891195 15596520 1 4.59 0.4 0 ## 6 1770 309 32505226867 12263600 1 1.5 0.29 0 ## 7 1257 309 33983222000 999090 1 3.49 0 0 ## 8 1509 309 32090471323 1016800 2 6.67 2.51 0 ## 9 1167 309 34103947117 1037417 1 3.65 0 0 ## 10 1836 309 40532932017 879008 1 6.99 1 0 ## # … with 416 more rows, and 3 more variables: coupon_match_disc &lt;dbl&gt;, week &lt;int&gt;, ## # transaction_timestamp &lt;dttm&gt; To use filtering effectively, you have to know how to select the observations that you want using comparison operators. R provides the standard suite of comparison operators to include: Operator Description &lt; less than &gt; greater than == equal to != not equal to &lt;= less than or equal to &gt;= greater than or equal to %in% group membership is.na is NA !is.na is not NA We can apply several of these comparison operators in the filter function using logical operators. The primary logical operators you will use are: Operator Example Description &amp; x &amp; y intersection of x and y | x \\| y union of x or y ! x &amp; !y x but exclude y and intersect of y xor xor(x, y) only values in x and y that are disjointed with one another The following visual representation helps to differentiate how these operators work with fictional x and y data: Figure 14.1: Complete set of boolean operations. x is the left-hand circle, y is the right-hand circle, and the shaded region show which parts each operator selects. R4DS For example, we can filter for transactions for a particular store ID or household ID: filter(df, store_id == &quot;309&quot; | household_id == &quot;1762&quot;) Or if we wanted to filter for transactions for a particular store ID and household ID: filter(df, store_id == &quot;309&quot; &amp; household_id == &quot;1762&quot;) A comma between comparison operators acts just like and &amp; operator. So filter(df, store_id == “309”, household_id == “1762”) is the same as filter(df, store_id == “309” &amp; household_id == “1762”) We can continue to add additional operations. In this example we filter for transactions made: at a particular store ID, by a particular household, who purchased more than 4 of a certain product or the product cost more than $10. df %&gt;% filter( store_id == &quot;309&quot;, household_id == &quot;1167&quot;, quantity &gt; 4 | sales_value &gt; 10 ) ## # A tibble: 3 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1167 309 34642351799 8249262 1 15.0 0 0 ## 2 1167 309 31993100768 941036 1 14.9 0 0 ## 3 1167 309 40341135791 1051093 6 3 1.74 0 ## # … with 3 more variables: coupon_match_disc &lt;dbl&gt;, week &lt;int&gt;, transaction_timestamp &lt;dttm&gt; A useful shortcut for writing multiple or statements is to use %in%. For example, the following code finds all transactions made at store 309 or 400. filter(df, store_id == &quot;309&quot; | store_id == &quot;400&quot;) ## # A tibble: 1,203 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2261 309 31625220889 940996 1 3.86 0.43 0 ## 2 1113 400 34204801923 998119 1 3.19 0.11 0 ## 3 519 309 32931585175 1029743 1 2.69 0 0 ## 4 1946 400 41352204380 1120213 3 5 0 0 ## 5 709 400 40911786843 923559 1 1 0.39 0 ## 6 725 400 33106940650 1000664 1 0.34 0.15 0 ## 7 1662 400 34258865155 1114465 2 2.08 0.7 0 ## 8 1662 400 32065200596 1085939 1 2.19 0 0 ## 9 1957 400 34338650191 933835 1 0.6 0 0 ## 10 1925 400 34133653463 13072776 1 1.99 0 0 ## # … with 1,193 more rows, and 3 more variables: coupon_match_disc &lt;dbl&gt;, week &lt;int&gt;, ## # transaction_timestamp &lt;dttm&gt; The %in% operator allows us to select every row where x is one of the values in y. We could use it to rewrite the code above as: filter(df, store_id %in% c(&quot;309&quot;, &quot;400&quot;)) ## # A tibble: 1,203 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2261 309 31625220889 940996 1 3.86 0.43 0 ## 2 1113 400 34204801923 998119 1 3.19 0.11 0 ## 3 519 309 32931585175 1029743 1 2.69 0 0 ## 4 1946 400 41352204380 1120213 3 5 0 0 ## 5 709 400 40911786843 923559 1 1 0.39 0 ## 6 725 400 33106940650 1000664 1 0.34 0.15 0 ## 7 1662 400 34258865155 1114465 2 2.08 0.7 0 ## 8 1662 400 32065200596 1085939 1 2.19 0 0 ## 9 1957 400 34338650191 933835 1 0.6 0 0 ## 10 1925 400 34133653463 13072776 1 1.99 0 0 ## # … with 1,193 more rows, and 3 more variables: coupon_match_disc &lt;dbl&gt;, week &lt;int&gt;, ## # transaction_timestamp &lt;dttm&gt; There are additional filtering and subsetting functions that are quite useful: # remove duplicate rows df %&gt;% distinct() # random sample, 50% sample size without replacement df %&gt;% sample_frac(size = 0.5, replace = FALSE) # random sample of 10 rows with replacement df %&gt;% sample_n(size = 10, replace = TRUE) # select rows 3-5 df %&gt;% slice(3:5) # select top n entries - in this case ranks variable net_spend_amt and selects # the rows with the top 5 values df %&gt;% top_n(n = 5, wt = sales_value) 14.3.1 Knowledge check Using the completejourney sample transactions data as we did above… Filter for transactions with greater than 2 units. Filter for transactions with greater than 2 units during week 25 that occurred at store 441. Filter for transactions with greater than 2 units during week 25 that occurred at store 343 or 441. Filter for transactions with greater than 2 units during week 25 that occurred at store 343 or 441 but excludes household 253. 14.4 Selecting variables It’s not uncommon for us to use data sets with hundreds of variables. In this case, the first challenge is often narrowing in on the variables you’re actually interested in. select allows you to rapidly zoom in on a useful subset using operations based on the names of the variables. select is not terribly useful with our transaction data because we only have 8 variables, but you can still get the general idea: # Select columns by name select(df, household_id, store_id, sales_value) ## # A tibble: 75,000 × 3 ## household_id store_id sales_value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2261 309 3.86 ## 2 2131 368 1.59 ## 3 511 316 1 ## 4 400 388 11.9 ## 5 918 340 1.29 ## 6 718 324 2.5 ## 7 868 323 3.49 ## 8 1688 450 2 ## 9 467 31782 6.55 ## 10 1947 32004 3.99 ## # … with 74,990 more rows # Select all columns between household_id and sales_value (inclusive) select(df, household_id:sales_value) ## # A tibble: 75,000 × 6 ## household_id store_id basket_id product_id quantity sales_value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2261 309 31625220889 940996 1 3.86 ## 2 2131 368 32053127496 873902 1 1.59 ## 3 511 316 32445856036 847901 1 1 ## 4 400 388 31932241118 13094913 2 11.9 ## 5 918 340 32074655895 1085604 1 1.29 ## 6 718 324 32614612029 883203 1 2.5 ## 7 868 323 32074722463 9884484 1 3.49 ## 8 1688 450 34850403304 1028715 1 2 ## 9 467 31782 31280745102 896613 2 6.55 ## 10 1947 32004 32744181707 978497 1 3.99 ## # … with 74,990 more rows # Select all columns except those between household_id and sales_value select(df, -c(household_id:sales_value)) ## # A tibble: 75,000 × 5 ## retail_disc coupon_disc coupon_match_disc week transaction_timestamp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dttm&gt; ## 1 0.43 0 0 5 2017-01-28 14:06:53 ## 2 0.9 0 0 10 2017-02-28 22:31:57 ## 3 0.69 0 0 13 2017-03-26 13:22:21 ## 4 2.9 0 0 8 2017-02-18 13:13:10 ## 5 0 0 0 10 2017-03-02 15:05:57 ## 6 0.49 0 0 15 2017-04-05 18:14:17 ## 7 0 0 0 10 2017-03-02 17:45:37 ## 8 1.79 0 0 33 2017-08-11 22:41:02 ## 9 4.44 0 0 2 2017-01-06 07:47:01 ## 10 0 0 0 16 2017-04-13 17:30:04 ## # … with 74,990 more rows There are a number of helper functions you can use within select: starts_with(\"abc\"): matches names that begin with “abc”. ends_with(\"xyz\"): matches names that end with “xyz”. contains(\"ijk\"): matches names that contain “ijk”. matches(\"(.)\\\\1\"): selects variables that match a regular expression. This one matches any variables that contain repeated characters. You’ll learn more about regular expressions in the character strings chapter. num_range(\"x\", 1:3): matches x1, x2 and x3. See ?select for more details. For example, we can select all variables that contain “id” in their name: select(df, contains(&quot;id&quot;)) ## # A tibble: 75,000 × 4 ## household_id store_id basket_id product_id ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2261 309 31625220889 940996 ## 2 2131 368 32053127496 873902 ## 3 511 316 32445856036 847901 ## 4 400 388 31932241118 13094913 ## 5 918 340 32074655895 1085604 ## 6 718 324 32614612029 883203 ## 7 868 323 32074722463 9884484 ## 8 1688 450 34850403304 1028715 ## 9 467 31782 31280745102 896613 ## 10 1947 32004 32744181707 978497 ## # … with 74,990 more rows select can be used to rename variables, but it’s rarely useful because it drops all of the variables not explicitly mentioned. Instead, use rename, which is a variant of select that keeps all the variables that aren’t explicitly mentioned: rename(df, store = store_id, product = product_id) ## # A tibble: 75,000 × 11 ## household_id store basket_id product quantity sales_value retail_disc coupon_disc ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2261 309 31625220889 940996 1 3.86 0.43 0 ## 2 2131 368 32053127496 873902 1 1.59 0.9 0 ## 3 511 316 32445856036 847901 1 1 0.69 0 ## 4 400 388 31932241118 13094913 2 11.9 2.9 0 ## 5 918 340 32074655895 1085604 1 1.29 0 0 ## 6 718 324 32614612029 883203 1 2.5 0.49 0 ## 7 868 323 32074722463 9884484 1 3.49 0 0 ## 8 1688 450 34850403304 1028715 1 2 1.79 0 ## 9 467 31782 31280745102 896613 2 6.55 4.44 0 ## 10 1947 32004 32744181707 978497 1 3.99 0 0 ## # … with 74,990 more rows, and 3 more variables: coupon_match_disc &lt;dbl&gt;, week &lt;int&gt;, ## # transaction_timestamp &lt;dttm&gt; Another option is to use select in conjunction with the everything helper. This is useful if you have a handful of variables you’d like to move to the start of the data frame. select(df, household_id, quantity, sales_value, everything()) ## # A tibble: 75,000 × 11 ## household_id quantity sales_value store_id basket_id product_id retail_disc coupon_disc ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2261 1 3.86 309 31625220889 940996 0.43 0 ## 2 2131 1 1.59 368 32053127496 873902 0.9 0 ## 3 511 1 1 316 32445856036 847901 0.69 0 ## 4 400 2 11.9 388 31932241118 13094913 2.9 0 ## 5 918 1 1.29 340 32074655895 1085604 0 0 ## 6 718 1 2.5 324 32614612029 883203 0.49 0 ## 7 868 1 3.49 323 32074722463 9884484 0 0 ## 8 1688 1 2 450 34850403304 1028715 1.79 0 ## 9 467 2 6.55 31782 31280745102 896613 4.44 0 ## 10 1947 1 3.99 32004 32744181707 978497 0 0 ## # … with 74,990 more rows, and 3 more variables: coupon_match_disc &lt;dbl&gt;, week &lt;int&gt;, ## # transaction_timestamp &lt;dttm&gt; 14.4.1 Knowledge check Check out the completejourney::demographics data set. Select all columns that start with “household_”. Select all columns that contain “_” and filter for observations where the household has one or more kids. 14.5 Computing summary statistics Obviously the goal of all this data wrangling is to be able to perform statistical analysis on our data. The summarize function allows us to perform the majority of the initial summary statistics when performing exploratory data analysis. For example, we can compute the mean sales_value across all observations: summarise(df, avg_sales_value = mean(sales_value)) ## # A tibble: 1 × 1 ## avg_sales_value ## &lt;dbl&gt; ## 1 3.12 These data have no missing values. However, if there are missing values you will need to use na.rm = TRUE in the function to remove missing values prior to computing the summary statistic: summarize(df, avg_sales_value = mean(sales_value, na.rm = TRUE)) There are a wide variety of functions you can use within summarize(). For example, the following lists just a few examples: Function Description min(), max() min, max values in vector mean() mean value median() median value sum() sum of all vector values var(), sd() variance/std of vector first(), last() first/last value in vector nth() nth value in vector n() number of values in vector n_distinct() number of distinct values in vector As long as the function reduces a vector of values down to a single summarized value, you can use it in summarize(). Figure 14.2: Summarize functions need to condense a vector input down to a single summarized output value. summarize is not terribly useful unless we pair it with another function called group_by. This changes the unit of analysis from the complete data set to individual groups. Then, when you use the dplyr verbs on a grouped data frame they’ll be automatically applied “by group”. For example, if we applied exactly the same code to a data frame grouped by store_id, we get the average sales value for each store_id level: by_store &lt;- group_by(df, store_id) summarize(by_store, avg_sales_value = mean(sales_value)) ## # A tibble: 293 × 2 ## store_id avg_sales_value ## &lt;chr&gt; &lt;dbl&gt; ## 1 108 14.0 ## 2 1089 1 ## 3 1098 2.28 ## 4 1102 1.17 ## 5 112 1.39 ## 6 1132 0.71 ## 7 1240 1 ## 8 1247 0.39 ## 9 1252 0.99 ## 10 134 1.89 ## # … with 283 more rows A more efficient way to write this same code is to use the pipe operator: df %&gt;% group_by(store_id) %&gt;% summarize(avg_sales_value = mean(sales_value)) ## # A tibble: 293 × 2 ## store_id avg_sales_value ## &lt;chr&gt; &lt;dbl&gt; ## 1 108 14.0 ## 2 1089 1 ## 3 1098 2.28 ## 4 1102 1.17 ## 5 112 1.39 ## 6 1132 0.71 ## 7 1240 1 ## 8 1247 0.39 ## 9 1252 0.99 ## 10 134 1.89 ## # … with 283 more rows We can compute multiple summary statistics: df %&gt;% group_by(store_id) %&gt;% summarize( `10%` = quantile(sales_value, .1), `50%` = quantile(sales_value, .5), avg_sales_value = mean(sales_value), `90%` = quantile(sales_value, .9), sd = sd(sales_value), n() ) ## # A tibble: 293 × 7 ## store_id `10%` `50%` avg_sales_value `90%` sd `n()` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 108 14.0 14.0 14.0 14.0 0 2 ## 2 1089 1 1 1 1 NA 1 ## 3 1098 2.28 2.28 2.28 2.28 NA 1 ## 4 1102 0.55 0.825 1.17 2.07 0.916 4 ## 5 112 0.94 1.09 1.39 2.13 0.652 6 ## 6 1132 0.39 0.39 0.71 1.35 0.716 5 ## 7 1240 1 1 1 1 NA 1 ## 8 1247 0.39 0.39 0.39 0.39 NA 1 ## 9 1252 0.99 0.99 0.99 0.99 NA 1 ## 10 134 1.89 1.89 1.89 1.89 NA 1 ## # … with 283 more rows There are additional summarize alternative functions that are quite useful. Test these out and see how they work: # compute the average for multiple specified variables df %&gt;% summarize_at(c(&quot;sales_value&quot;, &quot;quantity&quot;), mean) # compute the average for all numeric variables df %&gt;% summarize_if(is.numeric, mean) summarize is a very universal function that can be used on continuous and categorical variables; however, count is a great function to use to compute the number of observations for each level of a categorical variable (or a combination of categorical variables): # number of observations in each level of store_id count(df, store_id) ## # A tibble: 293 × 2 ## store_id n ## &lt;chr&gt; &lt;int&gt; ## 1 108 2 ## 2 1089 1 ## 3 1098 1 ## 4 1102 4 ## 5 112 6 ## 6 1132 5 ## 7 1240 1 ## 8 1247 1 ## 9 1252 1 ## 10 134 1 ## # … with 283 more rows # number of observations in each level of store_id and sort output count(df, store_id, sort = TRUE) ## # A tibble: 293 × 2 ## store_id n ## &lt;chr&gt; &lt;int&gt; ## 1 367 2129 ## 2 406 1634 ## 3 356 1386 ## 4 292 1333 ## 5 31782 1238 ## 6 343 1220 ## 7 381 1211 ## 8 361 1149 ## 9 32004 1128 ## 10 321 1106 ## # … with 283 more rows # number of observations in each combination of sort_id &amp; product_id count(df, store_id, product_id, sort = TRUE) ## # A tibble: 63,652 × 3 ## store_id product_id n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 367 1082185 34 ## 2 375 6534178 33 ## 3 422 6534178 33 ## 4 429 6534178 32 ## 5 343 6534178 28 ## 6 406 6534178 28 ## 7 367 6534178 24 ## 8 361 6534178 23 ## 9 406 1082185 23 ## 10 31862 1082185 21 ## # … with 63,642 more rows 14.5.1 Knowledge check Using the completejourney sample transactions data as we did above… Compute the total quantity of items purchased across all transactions. Compute the total quantity of items purchased by household. Compute the total quantity of items purchased by household for only transactions at store 309 where the quantity purchased was greater than one. 14.6 Sorting observations Often, we desire to view observations in rank order for a particular variable(s). The arrange function allows us to order data by variables in ascending or descending order. For example, we can sort our observations based on sales_value: # default is ascending order arrange(df, sales_value) ## # A tibble: 75,000 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1038 340 31389956808 983659 0 0 0 0 ## 2 1166 408 31969185576 5978656 0 0 0 1.4 ## 3 1397 381 40800715180 8020001 0 0 0 0 ## 4 1889 369 33433310971 903325 0 0 0 0 ## 5 1241 368 31834162699 7441102 0 0 0 0 ## 6 867 369 40436331223 5978656 0 0 0 1.25 ## 7 2204 367 33397465730 887782 0 0 0 0 ## 8 40 406 40085429046 5978648 0 0 0 0 ## 9 1100 358 41125111338 976199 0 0 0 0 ## 10 1829 445 33655105183 1014948 1 0 3.99 0 ## # … with 74,990 more rows, and 3 more variables: coupon_match_disc &lt;dbl&gt;, week &lt;int&gt;, ## # transaction_timestamp &lt;dttm&gt; # use desc() for descending order arrange(df, desc(sales_value)) ## # A tibble: 75,000 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1246 334 32005986123 12484608 1 100 0 0 ## 2 2318 381 31993236318 13040176 1 100. 0 0 ## 3 1172 396 34338411595 15630122 1 88.9 0 0 ## 4 1959 736 33510101062 5716076 30080 86.0 0 0 ## 5 1959 323 34811925575 6544236 26325 75 3.95 0 ## 6 2133 433 35727256342 6534178 32623 75 3.26 0 ## 7 1764 327 33971056246 6534178 24583 68.8 2.46 0 ## 8 1959 384 34103546818 6544236 23519 67.0 3.52 0 ## 9 2312 442 41351830986 916561 1 66.1 18.9 0 ## 10 2360 323 35463937998 12812261 8 65.6 63.1 0 ## # … with 74,990 more rows, and 3 more variables: coupon_match_disc &lt;dbl&gt;, week &lt;int&gt;, ## # transaction_timestamp &lt;dttm&gt; This function becomes particularly useful when combining with summary statistics. For example, we can quickly find the product with the largest average sales value amount by adding arrange to the end of this sequence of functions: df %&gt;% group_by(product_id) %&gt;% summarize(avg_sales_value = mean(sales_value)) %&gt;% arrange(desc(avg_sales_value)) ## # A tibble: 20,902 × 2 ## product_id avg_sales_value ## &lt;chr&gt; &lt;dbl&gt; ## 1 13040176 100. ## 2 15630122 88.9 ## 3 5716076 86.0 ## 4 1100869 63.7 ## 5 904021 63.7 ## 6 1775642 62.9 ## 7 5668996 62.0 ## 8 6544236 60.0 ## 9 5571881 60.0 ## 10 839075 56.0 ## # … with 20,892 more rows Missing values (NAs) will always be moved to the end of the list regardless if you perform ascending or descending sorting. 14.6.1 Knowledge check Compute the average sales value by household and arrange in descending order to find the household with the largest average spend. Find the products with the largest median spend. Compute the total quantity of items purchased by household for only transactions at store 309 where the quantity purchased was greater than one. Which household purchased the largest quantity of items? 14.7 Creating new variables Often, we want to create a new variable that is a function of the current variables in our data frame. The mutate function allows us to add new variables while preserving the existing variables. For example, we can compute the net spend per item by dividing sales_value by quantity. mutate(df, spend_per_item = sales_value / quantity) ## # A tibble: 75,000 × 12 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2261 309 31625220889 940996 1 3.86 0.43 0 ## 2 2131 368 32053127496 873902 1 1.59 0.9 0 ## 3 511 316 32445856036 847901 1 1 0.69 0 ## 4 400 388 31932241118 13094913 2 11.9 2.9 0 ## 5 918 340 32074655895 1085604 1 1.29 0 0 ## 6 718 324 32614612029 883203 1 2.5 0.49 0 ## 7 868 323 32074722463 9884484 1 3.49 0 0 ## 8 1688 450 34850403304 1028715 1 2 1.79 0 ## 9 467 31782 31280745102 896613 2 6.55 4.44 0 ## 10 1947 32004 32744181707 978497 1 3.99 0 0 ## # … with 74,990 more rows, and 4 more variables: coupon_match_disc &lt;dbl&gt;, week &lt;int&gt;, ## # transaction_timestamp &lt;dttm&gt;, spend_per_item &lt;dbl&gt; mutate always adds new columns at the end of your dataset. There are many functions for creating new variables that you can use with mutate. The key property is that the function must be vectorized: it must take a vector of values as input, return a vector with the same number of values as output. There’s no way to list every possible function that you might use, but here’s a selection of functions that are frequently useful: Function Description +,-,*,/,^ arithmetic x / sum(x) arithmetic w/aggregation %/%, %% modular arithmetic log, exp, sqrt transformations lag, lead offsets cumsum, cumprod, cum... cum/rolling aggregates &gt;, &gt;=, &lt;, &lt;=, !=, == logical comparisons min_rank, dense_rank ranking between are values between a and b? ntile bin values into n buckets The following provides a few examples of integrating these functions with mutate. # reduce the number of variables so you can see the transformations df2 &lt;- select(df, household_id, quantity, sales_value, transaction_timestamp) # compute total net spend amount per item df2 %&gt;% mutate(spend_per_item = sales_value / quantity) ## # A tibble: 75,000 × 5 ## household_id quantity sales_value transaction_timestamp spend_per_item ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 2261 1 3.86 2017-01-28 14:06:53 3.86 ## 2 2131 1 1.59 2017-02-28 22:31:57 1.59 ## 3 511 1 1 2017-03-26 13:22:21 1 ## 4 400 2 11.9 2017-02-18 13:13:10 5.94 ## 5 918 1 1.29 2017-03-02 15:05:57 1.29 ## 6 718 1 2.5 2017-04-05 18:14:17 2.5 ## 7 868 1 3.49 2017-03-02 17:45:37 3.49 ## 8 1688 1 2 2017-08-11 22:41:02 2 ## 9 467 2 6.55 2017-01-06 07:47:01 3.28 ## 10 1947 1 3.99 2017-04-13 17:30:04 3.99 ## # … with 74,990 more rows # log transform sales_value df2 %&gt;% mutate(log_sales_value = log(sales_value)) ## # A tibble: 75,000 × 5 ## household_id quantity sales_value transaction_timestamp log_sales_value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 2261 1 3.86 2017-01-28 14:06:53 1.35 ## 2 2131 1 1.59 2017-02-28 22:31:57 0.464 ## 3 511 1 1 2017-03-26 13:22:21 0 ## 4 400 2 11.9 2017-02-18 13:13:10 2.47 ## 5 918 1 1.29 2017-03-02 15:05:57 0.255 ## 6 718 1 2.5 2017-04-05 18:14:17 0.916 ## 7 868 1 3.49 2017-03-02 17:45:37 1.25 ## 8 1688 1 2 2017-08-11 22:41:02 0.693 ## 9 467 2 6.55 2017-01-06 07:47:01 1.88 ## 10 1947 1 3.99 2017-04-13 17:30:04 1.38 ## # … with 74,990 more rows # order by date and compute the cumulative sum df2 %&gt;% arrange(transaction_timestamp) %&gt;% mutate(cumsum_sales_value = cumsum(sales_value)) ## # A tibble: 75,000 × 5 ## household_id quantity sales_value transaction_timestamp cumsum_sales_value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 906 1 1.5 2017-01-01 07:30:27 1.5 ## 2 1873 1 1.88 2017-01-01 08:47:20 3.38 ## 3 993 1 3.49 2017-01-01 09:16:13 6.87 ## 4 1465 1 1.38 2017-01-01 09:46:04 8.25 ## 5 239 1 1.59 2017-01-01 10:05:51 9.84 ## 6 58 1 2.49 2017-01-01 10:14:16 12.3 ## 7 1519 2 3.55 2017-01-01 10:32:09 15.9 ## 8 1130 1 3.77 2017-01-01 10:38:18 19.6 ## 9 2329 1 3.99 2017-01-01 10:46:03 23.6 ## 10 2329 1 3.59 2017-01-01 10:46:03 27.2 ## # … with 74,990 more rows # compute sum of sales_value for each product and # rank order totals across 25 bins df %&gt;% group_by(product_id) %&gt;% summarize(total = sum(sales_value)) %&gt;% mutate(bins = ntile(total, 25)) ## # A tibble: 20,902 × 3 ## product_id total bins ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1000002 18.4 23 ## 2 1000050 33.9 24 ## 3 1000106 6.47 15 ## 4 1000140 3.29 9 ## 5 1000148 6 15 ## 6 1000165 3.34 9 ## 7 1000204 6.17 15 ## 8 1000205 3.98 10 ## 9 1000228 2.34 6 ## 10 1000236 5.39 14 ## # … with 20,892 more rows 14.7.1 Knowledge check Using the completejourney sample transactions data as we did above… Create a new column (total_disc) that is the sum of all discounts applied to each transaction (total_disc = coupon_disc + retail_disc + coupon_match_disc). Create a new column (disc_to_sales) that computes the ratio of total discount to total sales value (total_disc / sales_value). Using the results from #2, create a new column bins that bins the disc_to_sales column into 10 bins. Filter for those transactions with the highest discount to sales ratio (bin 10). 14.8 Putting it altogether The beauty of dplyr is how it makes exploratory data analysis very simple and efficient. For example, we can combine all the above functions to: group the data by store_id and product_id, compute the total sales_value, create a rank order variable, filter for the top 5 products within each store with the highest total sales value, sort total by descending order. df %&gt;% group_by(store_id, product_id) %&gt;% summarize(total = sum(sales_value)) %&gt;% mutate(rank = min_rank(desc(total))) %&gt;% filter(rank &lt;= 5) %&gt;% arrange(desc(total)) ## `summarise()` has grouped output by &#39;store_id&#39;. You can override using the `.groups` argument. ## # A tibble: 1,064 × 4 ## # Groups: store_id [293] ## store_id product_id total rank ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 375 6534178 1057. 1 ## 2 429 6534178 980. 1 ## 3 406 6534178 922 1 ## 4 343 6534178 711. 1 ## 5 422 6534178 658. 1 ## 6 367 6534178 631. 1 ## 7 329 6534178 582. 1 ## 8 361 6534178 553. 1 ## 9 429 6533889 494. 2 ## 10 33923 6534178 478. 1 ## # … with 1,054 more rows 14.9 Exercises Using what you’ve learned thus far, can you find the store and week that experienced the greatest week over week growth in the number of units sold? The steps to follow include: Group by store and week Compute sum of the quantity of items sold Create week over week percent growth in total units sold. You may want to check out the lag() function for this step. Arrange in descending order See the code chunk hint below. # hint df %&gt;% group_by(______, ______) %&gt;% summarize(______) %&gt;% mutate(______) %&gt;% arrange(______) 14.10 Additional resources dplyr is an extremely powerful package with many data transformation capabilities. This chapter discusses the more commonly applied functions but there are many more capabilities provided by dplyr not discussed. Here are some resources to help you learn more about dplyr: R for Data Science book, chapter 5 RStudio’s Data wrangling with R webinar dplyr vignette DataCamp’s Data Manipulation in R with dplyr course RStudio’s Data wrangling cheat sheet "],["lesson-3c-tidy-data.html", "15 Lesson 3c: Tidy data 15.1 Learning objectives 15.2 Prerequisites 15.3 Making wide data longer 15.4 Making long data wider 15.5 Separate one variable into multiple 15.6 Combine multiple variables into one 15.7 Additional tidying functions 15.8 Putting it altogether 15.9 Exercises 15.10 Additional resources", " 15 Lesson 3c: Tidy data “Cannot emphasize enough how much time you save by putting analysis efforts into tidying data first.” - Hilary Parker Jenny Bryan stated that “classroom data are like teddy bears and real data are like a grizzley bear with salmon blood dripping out its mouth.” In essence, she was getting to the point that often when we learn how to perform a modeling approach in the classroom, the data used is provided in a format that appropriately feeds into the modeling tool of choice. In reality, datasets are messy and “every messy dataset is messy in its own way.”7 The concept of “tidy data” was established by Hadley Wickham and represents a “standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).”8 The objective should always to be to get a dataset into a tidy form which consists of: Each variable forms a column Each observation forms a row Each type of observational unit forms a table Figure 15.1: Tidy data. To create tidy data you need to be able to reshape your data; preferably via efficient and simple code. To help with this process Hadley created the tidyr package. This lesson covers the basics of tidyr to help you reshape your data as necessary. If you’d like to learn more about the underlying theory, you might enjoy the Tidy Data paper published in the Journal of Statistical Software, http://www.jstatsoft.org/v59/i10/paper. 15.1 Learning objectives Upon completing this module you will be able to: Make wide data long and long data wide. Separate and combine parts of columns. Impute missing values 15.2 Prerequisites Load the tidyr package to provide you access to the functions we’ll cover in this lesson. We’ll also use dplyr for a few examples. library(tidyr) library(dplyr) To illustrate various tidying tasks we will use several untidy datasets provided in the data directory (or via Canvas material download). These are artificial customer transaction datasets that are designed to mimick actual data. 15.3 Making wide data longer There are times when our data is considered “wide” or “unstacked” and a common attribute/variable of concern is spread out across columns. To reformat the data such that these common attributes are gathered together as a single variable, the pivot_longer() function will take multiple columns and collapse them into key-value pairs, duplicating all other columns as needed. For example, let’s say we have the given data frame. (untidy1 &lt;- readr::read_csv(&quot;data/untidy1.csv&quot;)) ## # A tibble: 3,144 × 5 ## hshd_id prod_desc Mar Apr May ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3376002291 HNTS MEAT FLV PASTA SAUCE 1 0 0 ## 2 3376163223 RAGU SPICY ITL STYL SAUCE 1.89 0 0 ## 3 3376542041 HMK CARD MDAY MOTHER 0 0 5.29 ## 4 3377032853 HMK CARD BRTHDAY CROWN 3.99 0 0 ## 5 3377032853 MZTA MARINARA PASTA SAUCE 8.78 0 0 ## 6 3377285583 HMK CARD ADMIN ANYONE 0 2 0 ## 7 3377285583 HMK CARD MDAY 0 0 4.99 ## 8 3377285583 HMK CARD MDAY ANYONE 0 3.99 0 ## 9 3377285583 HMK CARD MDAY MOTHER 0 2.99 7.99 ## 10 3377285583 SLV PLT TOMTO MARNARA SCE 8.97 0 0 ## # … with 3,134 more rows This data is considered untidy in the wide sense since the month variable is structured such that each month represents a variable. If we wanted to compute the total amount each household spent by month, this data set does not provide a convenient shape to work with. To re-structure the month component as an individual variable, we can pivot this data to be longer such that each month is within one column variable and the values associated with each month in a second column variable. untidy1 %&gt;% pivot_longer(cols = Mar:May, names_to = &quot;month&quot;, values_to = &quot;net_spend_amt&quot;) ## # A tibble: 9,432 × 4 ## hshd_id prod_desc month net_spend_amt ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 3376002291 HNTS MEAT FLV PASTA SAUCE Mar 1 ## 2 3376002291 HNTS MEAT FLV PASTA SAUCE Apr 0 ## 3 3376002291 HNTS MEAT FLV PASTA SAUCE May 0 ## 4 3376163223 RAGU SPICY ITL STYL SAUCE Mar 1.89 ## 5 3376163223 RAGU SPICY ITL STYL SAUCE Apr 0 ## 6 3376163223 RAGU SPICY ITL STYL SAUCE May 0 ## 7 3376542041 HMK CARD MDAY MOTHER Mar 0 ## 8 3376542041 HMK CARD MDAY MOTHER Apr 0 ## 9 3376542041 HMK CARD MDAY MOTHER May 5.29 ## 10 3377032853 HMK CARD BRTHDAY CROWN Mar 3.99 ## # … with 9,422 more rows This new structure allows us to perform follow-on analysis in a much easier fashion. For example, if we wanted to compute the total amount each household spent by month, we could simply do the following sequence: untidy1 %&gt;% pivot_longer(cols = Mar:May, names_to = &quot;month&quot;, values_to = &quot;net_spend_amt&quot;) %&gt;% group_by(hshd_id, month) %&gt;% summarize(monthly_spend = sum(net_spend_amt)) ## # A tibble: 3,708 × 3 ## # Groups: hshd_id [1,236] ## hshd_id month monthly_spend ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 3376002291 Apr 0 ## 2 3376002291 Mar 1 ## 3 3376002291 May 0 ## 4 3376163223 Apr 0 ## 5 3376163223 Mar 1.89 ## 6 3376163223 May 0 ## 7 3376542041 Apr 0 ## 8 3376542041 Mar 0 ## 9 3376542041 May 5.29 ## 10 3377032853 Apr 0 ## # … with 3,698 more rows It’s important to note that there is flexibility in how you specify the columns you would like to gather. In our example we used cols = Mar:May to imply we want to use all the columns including and between Mar and May. We could also have used the following to produce the same results: untidy1 %&gt;% pivot_longer(cols = Mar:May, names_to = &quot;month&quot;, values_to = &quot;net_spend_amt&quot;) untidy1 %&gt;% pivot_longer(cols = c(Mar, Apr, May), names_to = &quot;month&quot;, values_to = &quot;net_spend_amt&quot;) untidy1 %&gt;% pivot_longer(cols = 3:5, names_to = &quot;month&quot;, values_to = &quot;net_spend_amt&quot;) untidy1 %&gt;% pivot_longer(cols = -c(hshd_id, prod_desc), names_to = &quot;month&quot;, values_to = &quot;net_spend_amt&quot;) 15.3.1 Knowledge check Using the data provided by tidyr::table4b: Is this data untidy? If so, why? Compute the sum of the values across the years for each country. Now pivot this data set so that it is tidy with the year values represented in their own column called ‘year’ and the values listed in their own ‘population’ column. Using the pipe operator to chain together a sequence of functions, perform #3 and then compute the mean population for each country. 15.4 Making long data wider There are also times when we are required to turn long formatted data into wide formatted data – in other words, we want to pivot data to be wider. As a complement to pivot_longer, the pivot_wider function spreads a key-value pair across multiple columns. For example, the given data frame captures total household purchases for each product along with the household’s shopping habit (Premium Loyal, Valuable, etc.). (untidy2 &lt;- readr::read_csv(&quot;data/untidy2.csv&quot;)) ## # A tibble: 1,536 × 4 ## hshd_id shabit prod_merch_l20_desc net_spend_amt ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 3376002291 Valuable PASTA &amp; PIZZA SAUCE 1 ## 2 3376163223 Premium Loyal PASTA &amp; PIZZA SAUCE 1.89 ## 3 3376542041 Potential GREETING CARDS 5.29 ## 4 3377032853 Uncommitted GREETING CARDS 3.99 ## 5 3377032853 Uncommitted PASTA &amp; PIZZA SAUCE 8.78 ## 6 3377285583 Premium Loyal GREETING CARDS 22.0 ## 7 3377285583 Premium Loyal PASTA &amp; PIZZA SAUCE 11.4 ## 8 3377528247 Valuable GREETING CARDS 11.0 ## 9 3378020265 Valuable GREETING CARDS 6.99 ## 10 3379767653 Uncommitted GREETING CARDS 3.99 ## # … with 1,526 more rows Currently, this data set is in a tidy format. But say we wanted to perform a classification model where we try to use the amount spent on each product to predict the shopping habit of each household, we would need to reorganize the data to make it compatible with future algorithms. To address this, we would need to pivot this data such that each product as its own variable with the total dollar amount spent on each product as the values. In essence, we are transposing the values in prod_merch_l20_desc to be the new variable names (aka key) and then adding the values in net_spend_amt to be the value under each variable name. untidy2 %&gt;% pivot_wider(names_from = prod_merch_l20_desc, values_from = net_spend_amt) ## # A tibble: 1,236 × 5 ## hshd_id shabit `PASTA &amp; PIZZA SAUCE` `GREETING CARDS` `NF SHORTENING AND OIL` ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3376002291 Valuable 1 NA NA ## 2 3376163223 Premium Loyal 1.89 NA NA ## 3 3376542041 Potential NA 5.29 NA ## 4 3377032853 Uncommitted 8.78 3.99 NA ## 5 3377285583 Premium Loyal 11.4 22.0 NA ## 6 3377528247 Valuable NA 11.0 NA ## 7 3378020265 Valuable NA 6.99 NA ## 8 3379767653 Uncommitted 2.19 3.99 NA ## 9 3379815863 Premium Loyal 1.49 NA NA ## 10 3380253171 Uncommitted NA 7.99 NA ## # … with 1,226 more rows This results in each household (hshd_id) having its own observation. You probably notice that there are now a lot of missing values which causes NA to be populated. If you were to apply this data to one of the many machine learning algorithms, you would likely run into errors due to the NAs. In this example, we could replace those NAs with zeros using values_fill = 0, since the household did not purchase any of those products. untidy2 %&gt;% pivot_wider(names_from = prod_merch_l20_desc, values_from = net_spend_amt, values_fill = 0) ## # A tibble: 1,236 × 5 ## hshd_id shabit `PASTA &amp; PIZZA SAUCE` `GREETING CARDS` `NF SHORTENING AND OIL` ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3376002291 Valuable 1 0 0 ## 2 3376163223 Premium Loyal 1.89 0 0 ## 3 3376542041 Potential 0 5.29 0 ## 4 3377032853 Uncommitted 8.78 3.99 0 ## 5 3377285583 Premium Loyal 11.4 22.0 0 ## 6 3377528247 Valuable 0 11.0 0 ## 7 3378020265 Valuable 0 6.99 0 ## 8 3379767653 Uncommitted 2.19 3.99 0 ## 9 3379815863 Premium Loyal 1.49 0 0 ## 10 3380253171 Uncommitted 0 7.99 0 ## # … with 1,226 more rows 15.4.1 Knowledge check Using the data provided by tidyr::table2: Is this data untidy? If so, why? Compute the cases-to-population ratio for each country by year. Now pivot this data set so that it is tidy with 4 columns: country, year, cases, and population. Using the pipe operator to chain together a sequence of functions, perform #3 and then compute the cases-to-population ratio for each country by year. 15.5 Separate one variable into multiple Many times a single column variable will capture multiple variables, or even parts of a variable you just don’t care about. This is exemplified in the following untidy3 data frame. Here, the product variable combines two variables, the product category (i.e. greeting cards, Pasta &amp; pizza sauce) along with the product description (i.e. graduation cards, birthday cards). For many reasons (summary statistics, visualization, modeling) we would likely want to separate these parts into their own variables. (untidy3 &lt;- readr::read_csv(&quot;data/untidy3.csv&quot;)) ## # A tibble: 3,884 × 4 ## hshd_id shabit net_spend_amt product ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 8614328969 Premium Loyal 0.99 GREETING CARDS: CREPE ## 2 8614328969 Premium Loyal 0.99 GREETING CARDS: CREPE ## 3 3479082131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 4 3479082131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 5 3479082131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 6 3479082131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 7 5003651017 Uncommitted 1.89 GREETING CARDS: CREPE ## 8 5003651017 Uncommitted 1.89 GREETING CARDS: CREPE ## 9 5003651017 Uncommitted 1.89 GREETING CARDS: CREPE ## 10 8614328969 Premium Loyal 1.49 GREETING CARDS: BALLOONS ## # … with 3,874 more rows This can be accomplished using the separate function which separates a single column into multiple columns based on a separator. Additional arguments provide some flexibility with separating columns. # separate product column into two variables named &quot;prod_category&quot; &amp; &quot;prod_desc&quot; untidy3 %&gt;% separate(col = product, into = c(&quot;prod_category&quot;, &quot;prod_desc&quot;), sep = &quot;: &quot;) ## # A tibble: 3,884 × 5 ## hshd_id shabit net_spend_amt prod_category prod_desc ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 8614328969 Premium Loyal 0.99 GREETING CARDS CREPE ## 2 8614328969 Premium Loyal 0.99 GREETING CARDS CREPE ## 3 3479082131 Premium Loyal 1.89 GREETING CARDS CREPE ## 4 3479082131 Premium Loyal 1.89 GREETING CARDS CREPE ## 5 3479082131 Premium Loyal 1.89 GREETING CARDS CREPE ## 6 3479082131 Premium Loyal 1.89 GREETING CARDS CREPE ## 7 5003651017 Uncommitted 1.89 GREETING CARDS CREPE ## 8 5003651017 Uncommitted 1.89 GREETING CARDS CREPE ## 9 5003651017 Uncommitted 1.89 GREETING CARDS CREPE ## 10 8614328969 Premium Loyal 1.49 GREETING CARDS BALLOONS ## # … with 3,874 more rows The default separator is any non alpha-numeric character. In this example there are two: white space ” “ and colon : so we need to specify. You can also keep the original column that you are separating by including remove = FALSE. You can also pass a vector of integers to sep and separate() will interpret the integers as positions to split at. For example, say our household ID (hshd_id) value actually represents the the household and user. So, let’s say the first 7 digits is the household identifier and the last 3 digits is the user identifier. We can split this into two new variables with the following: untidy3 %&gt;% separate(hshd_id, into = c(&quot;hshd_id&quot;, &quot;member_id&quot;), sep = 7) ## # A tibble: 3,884 × 5 ## hshd_id member_id shabit net_spend_amt product ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 8614328 969 Premium Loyal 0.99 GREETING CARDS: CREPE ## 2 8614328 969 Premium Loyal 0.99 GREETING CARDS: CREPE ## 3 3479082 131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 4 3479082 131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 5 3479082 131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 6 3479082 131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 7 5003651 017 Uncommitted 1.89 GREETING CARDS: CREPE ## 8 5003651 017 Uncommitted 1.89 GREETING CARDS: CREPE ## 9 5003651 017 Uncommitted 1.89 GREETING CARDS: CREPE ## 10 8614328 969 Premium Loyal 1.49 GREETING CARDS: BALLOONS ## # … with 3,874 more rows You can use positive and negative values to split the column. Positive values start at 1 on the far-left of the strings; negative value start at -1 on the far-right of the strings. 15.5.1 Knowledge check Using the data provided by tidyr::table3: Is this data untidy? If so, why? The rate variable is actually combining the number of cases and the population. Split this column such that you have four columns: country, year, cases, and population. Using the pipe operator to chain together a sequence of functions, perform #2 and then compute the cases-to-population ratio for each country by year. 15.6 Combine multiple variables into one Similarly, there are times when we would like to combine the values of two variables. As a compliment to separate, the unite function is a convenient function to paste together multiple variable values into one. Consider the following data frame that has separate date variables. To perform time series analysis or for visualizations we may desire to have a single date column. (untidy4 &lt;- readr::read_csv(&quot;data/untidy4.csv&quot;)) ## # A tibble: 3,884 × 7 ## hshd_id shabit net_spend_amt prod_desc year month day ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8614328969 Premium Loyal 0.99 CREPE 2017 5 30 ## 2 8614328969 Premium Loyal 0.99 CREPE 2017 5 30 ## 3 3479082131 Premium Loyal 1.89 CREPE 2017 3 6 ## 4 3479082131 Premium Loyal 1.89 CREPE 2017 3 6 ## 5 3479082131 Premium Loyal 1.89 CREPE 2017 3 6 ## 6 3479082131 Premium Loyal 1.89 CREPE 2017 3 6 ## 7 5003651017 Uncommitted 1.89 CREPE 2017 3 28 ## 8 5003651017 Uncommitted 1.89 CREPE 2017 3 28 ## 9 5003651017 Uncommitted 1.89 CREPE 2017 3 28 ## 10 8614328969 Premium Loyal 1.49 BALLOONS 2017 5 30 ## # … with 3,874 more rows We can accomplish this by uniting these columns into one variable with unite. untidy4 %&gt;% unite(col = &quot;date&quot;, year:day, sep = &quot;-&quot;) ## # A tibble: 3,884 × 5 ## hshd_id shabit net_spend_amt prod_desc date ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 8614328969 Premium Loyal 0.99 CREPE 2017-5-30 ## 2 8614328969 Premium Loyal 0.99 CREPE 2017-5-30 ## 3 3479082131 Premium Loyal 1.89 CREPE 2017-3-6 ## 4 3479082131 Premium Loyal 1.89 CREPE 2017-3-6 ## 5 3479082131 Premium Loyal 1.89 CREPE 2017-3-6 ## 6 3479082131 Premium Loyal 1.89 CREPE 2017-3-6 ## 7 5003651017 Uncommitted 1.89 CREPE 2017-3-28 ## 8 5003651017 Uncommitted 1.89 CREPE 2017-3-28 ## 9 5003651017 Uncommitted 1.89 CREPE 2017-3-28 ## 10 8614328969 Premium Loyal 1.49 BALLOONS 2017-5-30 ## # … with 3,874 more rows Don’t worry, we’ll learn more appropriate ways to deal with dates in a later lesson. 15.6.1 Knowledge check Using the data provided by tidyr::table5: Is this data untidy? If so, why? Unite and separate the necessary columns such that you have four columns: country, year, cases, and population. Using the pipe operator to chain together a sequence of functions, perform #2 and then compute the cases-to-population ratio for each country by year. 15.7 Additional tidying functions The previous four functions (pivot_longer, pivot_wider, separate and unite) are the primary functions you will find yourself using on a continuous basis; however, there are some handy functions that are lesser known with the tidyr package. Consider this untidy data frame. expenses &lt;- tibble::as_tibble(read.table(header = TRUE, text = &quot; Dept Year Month Day Cost A 2015 01 01 $500.00 NA NA 02 05 $90.00 NA NA 02 22 $1,250.45 NA NA 03 NA $325.10 B NA 01 02 $260.00 NA NA 02 05 $90.00 &quot;, stringsAsFactors = FALSE)) Often Excel reports will not repeat certain variables. When we read these reports in, the empty cells are typically filled in with NA such as in the Dept and Year columns of our expense data frame. We can fill these values in with the previous entry using fill(): expenses %&gt;% fill(Dept, Year) ## # A tibble: 6 × 5 ## Dept Year Month Day Cost ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 A 2015 1 1 $500.00 ## 2 A 2015 2 5 $90.00 ## 3 A 2015 2 22 $1,250.45 ## 4 A 2015 3 NA $325.10 ## 5 B 2015 1 2 $260.00 ## 6 B 2015 2 5 $90.00 Also, sometimes accounting values in Excel spreadsheets get read in as a character value, which is the case for the Cost variable. We may wish to extract only the numeric part of this regular expression, which can be done with readr::parse_number. Note that parse_number works on a single variable so when you pipe the expense data frame into the function you need to use %$% operator as discussed in the Pipe Operator lesson. library(magrittr) expenses %$% readr::parse_number(Cost) ## [1] 500.0 90.0 1250.5 325.1 260.0 90.0 # you can use this to convert and save the Cost column to a numeric variable expenses %&gt;% dplyr::mutate(Cost = readr::parse_number(Cost)) ## # A tibble: 6 × 5 ## Dept Year Month Day Cost ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 A 2015 1 1 500 ## 2 &lt;NA&gt; NA 2 5 90 ## 3 &lt;NA&gt; NA 2 22 1250. ## 4 &lt;NA&gt; NA 3 NA 325. ## 5 B NA 1 2 260 ## 6 &lt;NA&gt; NA 2 5 90 You can also easily replace missing (or NA) values with a specified value: # replace the missing Day value expenses %&gt;% replace_na(replace = list(Day = 31)) ## # A tibble: 6 × 5 ## Dept Year Month Day Cost ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 A 2015 1 1 $500.00 ## 2 &lt;NA&gt; NA 2 5 $90.00 ## 3 &lt;NA&gt; NA 2 22 $1,250.45 ## 4 &lt;NA&gt; NA 3 31 $325.10 ## 5 B NA 1 2 $260.00 ## 6 &lt;NA&gt; NA 2 5 $90.00 # replace both the missing Day and Year values expenses %&gt;% replace_na(replace = list(Year = 2015, Day = 31)) ## # A tibble: 6 × 5 ## Dept Year Month Day Cost ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 A 2015 1 1 $500.00 ## 2 &lt;NA&gt; 2015 2 5 $90.00 ## 3 &lt;NA&gt; 2015 2 22 $1,250.45 ## 4 &lt;NA&gt; 2015 3 31 $325.10 ## 5 B 2015 1 2 $260.00 ## 6 &lt;NA&gt; 2015 2 5 $90.00 15.8 Putting it altogether Since the %&gt;% operator is embedded in tidyr, we can string multiple operations together to efficiently tidy data and make the process easy to read and follow. To illustrate, let’s use the following data, which has multiple messy attributes. a_mess &lt;- tibble::as_tibble(read.table(header = TRUE, text = &quot; Dep_Unt Year Q1 Q2 Q3 Q4 A.1 2006 15 NA 19 17 B.1 NA 12 13 27 23 A.2 NA 22 22 24 20 B.2 NA 12 13 25 18 A.1 2007 16 14 21 19 B.2 NA 13 11 16 15 A.2 NA 23 20 26 20 B.2 NA 11 12 22 16 &quot;)) In this case, a tidy data set should result in columns of Dept, Unit, Year, Quarter, and Cost. Furthermore, we want to fill in the year column where NAs currently exist. And we’ll assume that we know the missing value that exists in the Q2 column, and we’d like to update it. a_mess %&gt;% fill(Year) %&gt;% pivot_longer(cols = Q1:Q4, names_to = &quot;Quarter&quot;, values_to = &quot;Cost&quot;) %&gt;% separate(Dep_Unt, into = c(&quot;Dept&quot;, &quot;Unit&quot;)) %&gt;% replace_na(replace = list(Cost = 17)) ## # A tibble: 32 × 5 ## Dept Unit Year Quarter Cost ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 A 1 2006 Q1 15 ## 2 A 1 2006 Q2 17 ## 3 A 1 2006 Q3 19 ## 4 A 1 2006 Q4 17 ## 5 B 1 2006 Q1 12 ## 6 B 1 2006 Q2 13 ## 7 B 1 2006 Q3 27 ## 8 B 1 2006 Q4 23 ## 9 A 2 2006 Q1 22 ## 10 A 2 2006 Q2 22 ## # … with 22 more rows 15.9 Exercises Is the following data set “tidy”? Why or why not? people &lt;- tribble( ~name, ~names, ~values, #-----------------|--------|------ &quot;Phillip Woods&quot;, &quot;age&quot;, 45, &quot;Phillip Woods&quot;, &quot;height&quot;, 186, &quot;Phillip Woods&quot;, &quot;age&quot;, 50, &quot;Jessica Cordero&quot;, &quot;age&quot;, 37, &quot;Jessica Cordero&quot;, &quot;height&quot;, 156 ) Using the data set above, convert the “name” column into “first_name” and “last_name” columns. Now pivot this data set such that the “names” and “values” columns are converted into “age” and “height” columns. 15.10 Additional resources This chapter covers most, but not all, of what tidyr provides. There are several other resources you can check out to learn more. R for Data Science book, chapter 12 R Studio’s Data wrangling with R webinar tidyr vignette RStudio’s Data wrangling cheat sheet DataCamp’s Cleaning Data in R course Wickham, H. (2014). “Tidy data.” Journal of Statistical Software, 59(10). [document]↩︎ Ibid↩︎ "],["lab-2.html", "16 Lab", " 16 Lab TBD "],["overview-3.html", "17 Overview 17.1 Learning objectives 17.2 Tasks 17.3 Course readings", " 17 Overview It’s rare that a data analysis involves only a single table of data. Typically you have many tables of data, and you must combine them to answer the questions that you’re interested in. Collectively, multiple tables of data are called relational data because it is the relations, not just the individual datasets, that are important. Moreover, as data scientists work across multiple datasets they often find themselves working with different data types (i.e. numeric, categorical, date-times) and this can present many challenges. In this module we’ll explore how to use different join operations when working with relational data along with introduce additional Tidyverse packages that will simplify working with different data types. 17.1 Learning objectives By the end of this module you should be able to: Describe and apply the different join operations. Identify and manipulate a variety of different data types. 17.2 Tasks TBD 17.3 Course readings TBD "],["lesson-1.html", "18 Lesson 1 18.1 Learning objectives 18.2 Content A 18.3 Content B 18.4 Exercises", " 18 Lesson 1 TBD 18.1 Learning objectives 18.2 Content A 18.3 Content B 18.4 Exercises "],["lesson-2.html", "19 Lesson 2 19.1 Learning objectives 19.2 Content A 19.3 Content B 19.4 Exercises", " 19 Lesson 2 TBD 19.1 Learning objectives 19.2 Content A 19.3 Content B 19.4 Exercises "],["lab-3.html", "20 Lab", " 20 Lab TBD "],["overview-4.html", "21 Overview 21.1 Learning objectives 21.2 Tasks 21.3 Course readings", " 21 Overview Being able to create visualizations (graphical representations) of data is a key step in data analysis. In this module you will learn to use the ggplot2 library to create the meaningful, elegant, and finely tuned data visualizations. You will also learn to combine data transformation, manipulation, and visualization to explore your data and start extracting key insights. 21.1 Learning objectives By the end of this module you should be able to: Build meaning visualizations with ggplot2. Identify which plots to use to explore different types of variables. Combine descriptive statistics and visualization to identify summaries, relationships, differences, and abnormalities in the data. 21.2 Tasks TBD 21.3 Course readings TBD "],["lesson-1-1.html", "22 Lesson 1 22.1 Learning objectives 22.2 Content A 22.3 Content B 22.4 Exercises", " 22 Lesson 1 TBD 22.1 Learning objectives 22.2 Content A 22.3 Content B 22.4 Exercises "],["lesson-2-1.html", "23 Lesson 2 23.1 Learning objectives 23.2 Content A 23.3 Content B 23.4 Exercises", " 23 Lesson 2 TBD 23.1 Learning objectives 23.2 Content A 23.3 Content B 23.4 Exercises "],["lab-4.html", "24 Lab", " 24 Lab TBD "],["overview-5.html", "25 Overview 25.1 Learning objectives 25.2 Tasks 25.3 Course readings", " 25 Overview Don’t repeat yourself (DRY) is a software development principle aimed at reducing repetition. Formulated by Andy Hunt and Dave Thomas in their book The Pragmatic Programmer, the DRY principle states that “every piece of knowledge must have a single, unambiguous, authoritative representation within a system.” This principle has been widely adopted to imply that you should not duplicate code. Although the principle was meant to be far grander than that, there’s plenty of merit behind this slight misinterpretation. Removing duplication is an important part of writing efficient code and reducing potential errors. First, reduced duplication of code can improve computing time and reduces the amount of code writing required. Second, less duplication results in less creating and saving of unnecessary objects. Inefficient code invariably creates copies of objects you have little interest in other than to feed into some future line of code; this wrecks havoc on properly managing your objects as it basically results in a global environment charlie foxtrot! Less duplication also results in less editing. When changes to code are required, duplicated code becomes tedious to edit and invariably mistakes or fat-fingering occur in the cut-and-paste editing process which just lengthens the editing that much more. Thus, minimizing duplication by writing efficient code is important to becoming a data analyst and in this module you learn to improve your code efficiency with control flow, iteration, and functions. 25.1 Learning objectives By the end of this module you should be able to: Use a variety of control statements to control the flow of code evaluation. Simplify repetitive code with R’s looping statements. Understand when and how to write a function. 25.2 Tasks TBD 25.3 Course readings TBD "],["lesson-1-2.html", "26 Lesson 1 26.1 Learning objectives 26.2 Content A 26.3 Content B 26.4 Exercises", " 26 Lesson 1 TBD 26.1 Learning objectives 26.2 Content A 26.3 Content B 26.4 Exercises "],["lesson-2-2.html", "27 Lesson 2 27.1 Learning objectives 27.2 Content A 27.3 Content B 27.4 Exercises", " 27 Lesson 2 TBD 27.1 Learning objectives 27.2 Content A 27.3 Content B 27.4 Exercises "],["lab-5.html", "28 Lab", " 28 Lab TBD "],["overview-6.html", "29 Overview 29.1 Learning objectives 29.2 Tasks 29.3 Course readings", " 29 Overview Now that you are equipped with powerful programming tools we can finally move into the world of applied modeling. In this module we’ll build on your new tools of data wrangling and programming to fit, analyze, and understand different applied modeling techniques. First, you will gain a basic understanding of discovering relationships across variables and using models for data exploration. Then you will learn about the machine learning process and how to start applying it with R. This will give you the foundation to start building your own machine learning toolbox. 29.1 Learning objectives By the end of this module you should be able to: Identify covariate relationships within your data. Explain the modeling process and apply it within your R workflow. Apply and explain the results of a few foundational machine learning algorithms. 29.2 Tasks TBD 29.3 Course readings TBD "],["lesson-1-3.html", "30 Lesson 1 30.1 Learning objectives 30.2 Content A 30.3 Content B 30.4 Exercises", " 30 Lesson 1 TBD 30.1 Learning objectives 30.2 Content A 30.3 Content B 30.4 Exercises "],["lesson-2-3.html", "31 Lesson 2 31.1 Learning objectives 31.2 Content A 31.3 Content B 31.4 Exercises", " 31 Lesson 2 TBD 31.1 Learning objectives 31.2 Content A 31.3 Content B 31.4 Exercises "],["lab-6.html", "32 Lab", " 32 Lab TBD "],["computing-environment-1.html", "Computing Environment", " Computing Environment This book was built with the following computing environment and packages: sessioninfo::session_info(pkgs = &#39;attached&#39;) ## ─ Session info ────────────────────────────────────────────────────────────────────────────── ## setting value ## version R version 4.2.0 (2022-04-22) ## os macOS Monterey 12.2 ## system x86_64, darwin17.0 ## ui RStudio ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/New_York ## date 2022-05-26 ## rstudio 2022.02.2+485 Prairie Trillium (desktop) ## pandoc 2.18 @ /usr/local/bin/ (via rmarkdown) ## ## ─ Packages ────────────────────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## completejourney * 1.1.0 2019-09-28 [1] CRAN (R 4.2.0) ## dbplyr * 2.1.1 2021-04-06 [1] CRAN (R 4.2.0) ## dplyr * 1.0.9 2022-04-28 [1] CRAN (R 4.2.0) ## forcats * 0.5.1 2021-01-27 [1] CRAN (R 4.2.0) ## ggplot2 * 3.3.6 2022-05-03 [1] CRAN (R 4.2.0) ## here * 1.0.1 2020-12-13 [1] CRAN (R 4.2.0) ## jsonlite * 1.8.0 2022-02-22 [1] CRAN (R 4.2.0) ## magrittr * 2.0.3 2022-03-30 [1] CRAN (R 4.2.0) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.2.0) ## readr * 2.1.2 2022-01-30 [1] CRAN (R 4.2.0) ## readxl * 1.4.0 2022-03-28 [1] CRAN (R 4.2.0) ## RSQLite * 2.2.14 2022-05-07 [1] CRAN (R 4.2.0) ## stringr * 1.4.0 2019-02-10 [1] CRAN (R 4.2.0) ## tibble * 3.1.7 2022-05-03 [1] CRAN (R 4.2.0) ## tidyr * 1.2.0 2022-02-01 [1] CRAN (R 4.2.0) ## tidyverse * 1.3.1 2021-04-15 [1] CRAN (R 4.2.0) ## ## [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library ## ## ───────────────────────────────────────────────────────────────────────────────────────────── "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
