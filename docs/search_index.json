[["index.html", "Data Wrangling with R Syllabus Learning Objectives Material Class Structure Schedule Conventions used in this book Feedback Acknowledgements", " Data Wrangling with R Bradley Boehmke Syllabus This repository contains additional resources for the UC BANA 7025 Data Wrangling course. The following is a truncated syllabus; for the full syllabus along with complete course content please visit the online course content in Canvas. Welcome to Data Wrangling with R! This course provides an intensive, hands-on introduction to Data Wrangling with the R programming language. You will learn the fundamental skills required to acquire, munge, transform, manipulate, and visualize data in a computing environment that fosters reproducibility. Data wrangling, which is also commonly referred to as data munging, transformation, manipulation, janitor work, etc. can be a painstakingly laborious process. In fact, it has been stated that up to 80% of data analysis is spent on the process of cleaning and preparing data (Wickham 2014; Dasu and Johnson 2003). However, being a prerequisite to the rest of the data analysis workflow (visualization, modeling, reporting), it’s essential that you become fluent and efficient in data wrangling techniques. Learning Objectives This course will guide you through the data wrangling process along with give you a solid foundation of the basics of working with data in R. My goal is to teach you how to easily wrangle your data so you can spend more time focused on understanding the content of your data via visualization, modeling, and reporting your results. Upon successfully completing this course, you will be able to: Perform your data analysis in a literate programming environment Manage different types of data Manage different data structures Import and export data Index, subset, reshape and transform your data Compute descriptive statistics Visualize data Make your code efficient by using control statements &amp; iteration Write your own functions …all with R! This course assumes no prior knowledge of R. Experience with programming concepts or another programming language will help, but is not required to understand the material. Material The bulk of the classroom material will be provided via this book, the recorded lectures, and class notes. In some cases there are additional recommended readings, all of which are readily available online. Class Structure Modules: For this class each module is covered over the course of week. In the “Overview” section for each module you will find overall learning objectives, a short description of the learning content covered in that module, along with all tasks that are required of you for that module (i.e. quizzes, lab). Each module will have two or more primary lessons and associated quizzes along with a lab. Lessons: For each lesson you will read and work through the tutorial. Short videos will be sprinkled throughout the lesson to further discuss and reinforce lesson concepts. Each lesson will have various “Your Turn” exercises throughout, along with end-of-lesson exercises. I highly recommend you work through these exercises as they will prepare you for the quizzes, labs, and project work. Quizzes: There will be a short quiz associated with each lesson. These quizzes will be hosted in the course website on Canvas. Please check Canvas for due dates for these quizzes. Labs: There will be a lab associated with each module. For these labs students will be guided through a case study step-by-step. The aim is to provide a detailed view on how to manage a variety of complex real-world data; how to convert real problems into data wrangling and analysis problems; and to apply R to address these problems and extract insights from the data. Submission of these labs will be done through the course website on Canvas. Please check Canvas for due dates for these labs. Project: TBD Schedule See the Canvas course webpage for a detailed schedule with due dates for quizzes, labs, etc. Module Description 1 Introduction Intro to data wrangling, R, and this course R fundamentals &amp; the Rstudio IDE 2 Reproducible Documents and Importing Data Managing your workflow and reproducibility Importing data and understanding the basics of it 3 Tidy Data and Data Manipulation Tidying &amp; preparing data for analysis Data manipulation 4 Relational Data and More Tidyverse Packages Relational data Leveraging the Tidyverse to simplify data wrangling 5 Data Visualization &amp; Exploration Data visualization Exploratory data analysis 6 Creating Efficient Code in R Control statements &amp; iteration Writing functions 7 Introduction to Applied Modeling Correlation &amp; pattern discovery Introduction to machine learning Conventions used in this book The following typographical conventions are used in this book: strong italic: indicates new terms, bold: indicates package &amp; file names, inline code: monospaced highlighted text indicates functions or other commands that could be typed literally by the user, code chunk: indicates commands or other text that could be typed literally by the user 1 + 2 ## [1] 3 In addition to the general text used throughout, you will notice the following cells that provide additional context for improved learning: A tip or suggestion that will likely produce better results. A general note that could improve your understanding but is not required for the course requirements. Warning or caution to look out for. Knowledge check exercises to gauge your learning progress. Feedback To report errors or bugs that you find in this course material please post an issue at https://github.com/bradleyboehmke/uc-bana-7025/issues. For all other communication be sure to use Canvas or the university email. When communicating with me via email, please always include BANA7025 in the subject line. Acknowledgements This course and its materials have been influenced by the following resources: Jenny Bryan, STAT 545: Data wrangling, exploration, and analysis with R Garrett Grolemund &amp; Hadley Wickham, R for Data Science Stephanie Hicks, Statistical Computing Chester Ismay &amp; Albert Kim, ModernDive Alex Douglas et al., An Introduction to R References "],["overview.html", "1 Overview 1.1 Learning objectives 1.2 Tasks 1.3 Course readings", " 1 Overview Welcome to module 1! The focus of this module is to get you acquainted with the concept of data wrangling, the R programming language, and to get you performing some foundational programming tasks with R. 1.1 Learning objectives By the end of this module you should be able to: Explain the key components of data wrangling. Discuss the background and benefits of the R programming language. Understand how to install, open, and start using R with the RStudio IDE. Perform fundamental tasks in R such as assignment, evaluation, using R as a calculator, and installing packages. 1.2 Tasks TBD 1.3 Course readings TBD "],["lesson-1a-course-overview.html", "2 Lesson 1a: Course overview 2.1 Learning objectives 2.2 Purpose of this course 2.3 Assumptions &amp; Pre-requisites 2.4 Course Staff 2.5 Course Logistics 2.6 Grading 2.7 Getting Help 2.8 Fine Print 2.9 Exercises", " 2 Lesson 1a: Course overview TBD – complete this lesson towards the end of course dev 2.1 Learning objectives By the end of this lesson you will: Be able to explain the purpose and objectives of this course. Understand the assumptions and prerequisites that are expected for this course. Understand how this course flows and how to complete the required tasks (i.e. quizzes, labs). Know the grading philosophy and relative weights across the various tasks. Know how, when, and where to get help for course specific concerns. 2.2 Purpose of this course 2.3 Assumptions &amp; Pre-requisites 2.4 Course Staff 2.5 Course Logistics 2.6 Grading 2.7 Getting Help 2.8 Fine Print 2.9 Exercises "],["lesson-1b-r-rstudio.html", "3 Lesson 1b: R &amp; RStudio 3.1 Learning objectives 3.2 R vs. RStudio 3.3 Installing R and RStudio 3.4 Understanding the RStudio IDE 3.5 Getting help 3.6 Errors, warnings, and messages 3.7 Exercises", " 3 Lesson 1b: R &amp; RStudio This lesson is designed to help you get up and running with R and RStudio. It will also provide you with some tips for getting help as you start writing R code and differentiating errors, warnings, and messages (because we will experience these daily!). 3.1 Learning objectives Upon completing this module you will: Understand the difference between R and RStudio. Have R and RStudio installed on your computer. Be able to navigate the RStudio IDE. Know how to get help as you first start learning R. Differentiate errors, warnings, and messages. Know what additional resources are available to go deeper. 3.2 R vs. RStudio When students first learn about R and RStudio, they are often confused on how the two differentiate. R is a programming language that runs computations, while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. An analogy used by Ismay and Kim (2019) helps put this into perspective. “R is like a car’s engine while RStudio is like a car’s dashboard…just as the way of having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well.” Figure 3.1: Analogy of difference between R and RStudio (Ismay and Kim 2019) It is important to note that one can write R and do nearly everything you are going to do in this course without using RStudio. However, RStudio has lots of features that make writing, executing, and organizing your R code so much easier. Moreover, realize that there are other IDEs and platforms that you can use (i.e. VS code, Radiant, jupyter) to write and execute R code; however, RStudio has become the defacto IDE for R and is heavily used throughout the R community. Since RStudio has lots of features, it takes time to learn them. A good resource to learn more about RStudio are the R Studio Essentials collection of videos. 3.3 Installing R and RStudio It is important that you install R first and then install RStudio. You can download and install R from CRAN, the Comprehensive R Archive Network. It is highly recommended to install a precompiled binary distribution for your operating system (OS); follow these instructions: Go to https://cloud.r-project.org/ Select the installer for your operating system: If you are a Windows user: Click on “Download R for Windows”, then click on “base”, then click on the Download link. If you are macOS user: Click on “Download R for (Mac) OS X”, then under “Latest release:” click on R-X.X.X.pkg, where R-X.X.X is the version number. For example, the latest version of R as of April 25, 2022 was R-4.2.0. If you are a Linux user: Click on “Download R for Linux” and choose your distribution for more information on installing R for your setup. Follow the instructions of the installer. Next, you can download RStudio’s IDE by following these instructions: Go to RStudio for desktop https://www.rstudio.com/products/rstudio/download/ Scroll down to “Installers for Supported Platforms” near the bottom of the page. Select the install file for your OS. Follow the instructions of the installer. Installing R and RStudio should be fairly straightforward. However, a great set of detailed step-by-step instructions is in Rafael Irizarry’s Introduction to Data Science book (Irizarry 2019). After you install R and RStudio on your computer, you’ll have two new programs (also called applications) you can open. The R application icon looks like the left image below while the RStudio icon looks like the right image. We’ll always work in RStudio and not in the R application so be sure to select the correct icon. Figure 3.2: Use the RStudio icon (right) and not the R icon (left) to launch your R project. (Ismay and Kim 2019) If your OS allows, pin the RStudio icon to your desktop or Dock and keep the R icon in another location (i.e. Applications for Mac users). This makes it natural and efficient to always go to the RStudio icon on your desktop for launching the program. After you open RStudio, you should see something similar to the following image. Note that slight differences might exist depending on the version of your RStudio interface. Figure 3.3: RStudio interface to R. You are now ready to start programming! 3.4 Understanding the RStudio IDE The RStudio IDE is where all the action happens. There are four fundamental windows in the IDE, each with their own purpose. I discuss each briefly below but I highly suggest Oscar Torres-Reyna’s Introduction to RStudio for a thorough understanding of the console1. Figure 3.4: The four fundamental windows within the RStudio IDE 3.4.1 Script Editor The top left window is where your script files will display. There are multiple forms of script files but the basic one to start with is the .R file. To create a new file you use the File » New File menu. To open an existing file you use either the File » Open File… menu or the Recent Files menu to select from recently opened files. RStudio’s source editor includes a variety of productivity enhancing features including syntax highlighting, code completion, multiple-file editing, and find/replace. A good introduction to the script editor was written by RStudio’s Josh Paulson2. The script editor is a great place to put code you care about. Keep experimenting in the console, but once you have written code that works and does what you want, put it in the script editor. RStudio will automatically save the contents of the editor when you quit RStudio, and will automatically load it when you re-open. Nevertheless, it’s a good idea to save your scripts regularly and to back them up. To execute the code in the script editor you have two options: Place the cursor on the line that you would like to execute and execute Cmd/Ctrl + Enter. Alternatively, you could hit the Run button in the toolbar. If you want to run all lines of code in the script then you can highlight the lines you want to run and then execute one of the options in #1. Figure 3.5: Execute lines of code in your script with Cmd/Ctrl + Enter or using the Run button. 3.4.2 Workspace Environment The top right window is the workspace environment which captures much of your your current R working environment and includes any user-defined objects (vectors, matrices, data frames, lists, functions). When saving your R working session, these are the components along with the script files that will be saved in your working directory, which is the default location for all file inputs and outputs. To get or set your working directory so you can direct where your files are saved: # returns path for the current working directory getwd() # set the working directory to a specified directory setwd(&quot;path/of/directory&quot;) For example, if I call getwd() the file path “/Users/b294776/Desktop” is returned. If I want to set the working directory to the “Workspace” folder within the “Desktop” directory I would use setwd(\"/Users/b294776/Desktop/Workspace\"). Now if I call getwd() again it returns “/Users/b294776/Desktop/Workspace”. An alternative solution is to go to the following location in your toolbar Session » Set Working Directory » Choose Directory and select the directory of choice (much easier!). The workspace environment will also list your user defined objects such as vectors, matrices, data frames, lists, and functions. For example, if you type the following in your console: x &lt;- 2 y &lt;- 3 You will now see x and y listed in your workspace environment. To identify or remove the objects (i.e. vectors, data frames, user defined functions, etc.) in your current R environment: # list all objects ls() # identify if an R object with a given name is present exists(&quot;x&quot;) # remove defined object from the environment rm(x) # you can remove multiple objects rm(x, y) # basically removes everything in the working environment -- use with caution! rm(list = ls()) If you have a lot of objects in your workspace environment you can use the 🧹 icon in the workspace environment tab to clear out everything. You can also view previous commands in the workspace environment by clicking the History tab, by simply pressing the up arrow on your keyboard, or by typing into the console: # default shows 25 most recent commands history() # show 100 most recent commands history(100) # show entire saved history history(Inf) 3.4.3 Console The bottom left window contains the console. You can code directly in this window but it will not save your code. It is best to use this window when you are simply wanting to perform calculator type functions. This is also where your outputs will be presented when you run code in your script. Go ahead and type the following in your console: 2 * 3 + 8 / 2 3.4.4 Misc. Displays The bottom right window contains multiple tabs. The Files tab allows you to see which files are available in your working directory. The Plots tab will display any plots/graphics that are produced by your code. The Packages tab will list all packages downloaded to your computer and also the ones that are loaded (more on this later). And the Help tab allows you to search for topics you need help on and will also display any help responses (more on this later as well). 3.4.5 Workspace Options &amp; Shortcuts There are multiple options available for you to set and customize both R and your RStudio console. For R, you can read about, and set, available options for the current R session with the following code. For now you don’t need to worry about making any adjustments, just know that many options do exist. # learn about available options help(options) # view current option settings options() # change a specific option (i.e. number of digits to print on output) options(digits=3) For a thorough tutorial regarding the RStudio console and how to customize different components check out this tutorial. You can also find the RStudio console cheatsheet shown below by going to Help menu » Cheatsheets. As with most computer programs, there are numerous keyboard shortcuts for working with the console. To access a menu displaying all the shortcuts in RStudio you can use option + shift + k. Within RStudio you can also access them in the Help menu » Keyboard Shortcuts Help. Figure 3.6: RStudio IDE cheat sheet. 3.4.6 Knowledge check Identify which working directory you are working out of. Create a folder on your computer titled Learning R. Within RStudio, set your working directory to this folder. Type pi in the console. Set the option to show 8 digits. Re-type pi in the console. Type ?pi in the console. Note that documentation on this object pops up in the Help tab in the Misc. Display. Now check out your code History tab. Create a new .R file and save this as my-first-script (note how this now appears in your Learning R folder). Type pi in line 1 of this script, option(digits = 8) in line 2, and pi again in line three. Execute this code one line at a time and then re-execute all lines at once. 3.5 Getting help The help documentation and support in R is comprehensive and easily accessible from the the console. 3.5.1 General Help To leverage general help resources you can use: # provides general help links help.start() # searches the help system for documentation matching a given character string help.search(&quot;linear regression&quot;) Note that the help.search(\"some text here\") function requires a character string enclosed in quotation marks. So if you are in search of time series functions in R, using help.search(\"time series\") will pull up a healthy list of vignettes and code demonstrations that illustrate packages and functions that work with time series data. For more direct help on functions that are installed on your computer you can use the following. Test these out in your console: help(mean) # provides details for specific function ?mean # provides same information as help(functionname) example(mean) # provides examples for said function Note that the help() and ? function calls only work for functions within loaded packages. You’ll understand what this means shortly. 3.5.2 Getting Help From the Web Typically, a problem you may be encountering is not new and others have faced, solved, and documented the same issue online. The following resources can be used to search for online help. Although, I typically just Google the problem and find answers relatively quickly. RSiteSearch(\"key phrase\"): searches for the key phrase in help manuals and archived mailing lists on the R Project website. Stack Overflow: a searchable Q&amp;A site oriented toward programming issues. 75% of my answers typically come from Stack Overflow. Cross Validated: a searchable Q&amp;A site oriented toward statistical analysis. RStudio Community: a community for all things R and RStudio where you can get direct answers to your problems and also give back by helping to solve and answer other’s questions. R-seek: a Google custom search that is focused on R-specific websites R-bloggers: a central hub of content collected from over 500 bloggers who provide news and tutorials about R. Twitter has a thriving R community (#rstats) and is definitely worth following. However, it is not the place to ask for help on specific code functionality. 3.5.3 Knowledge check Does help.start() provide a link to an introduction to R manual? Does R’s mode function compute the statistical mode? Review at least 5 Stack Overflow questions about R. 3.6 Errors, warnings, and messages One thing that intimidates new R users (and new programmers in general) is when they run into errors, warnings, and messages. R reports errors, warnings, and messages in the following way. Don’t worry about what the code is doing, just get familiar with the differences in how errors, warnings, and messages are reported. Errors: When the red text is a legitimate error, it will be prefaced with “Error in…” and will try to explain what went wrong. Generally when there’s an error, the code will not run. Think of errors as a red traffic light: something is wrong! For references on common errors, check out the following links by Noam Ross, David Smith, and Chester Ismay. # Example of an error 1 + &#39;a&#39; ## Error in 1 + &quot;a&quot;: non-numeric argument to binary operator Warnings: Warning messages will be prefaced with “Warning:” and R will try to explain why there’s a warning. Generally your code will still work, but with some caveats. Think of warnings as a yellow traffic light: everything is working, but watch out/pay attention. Typically when you get a warning you want to figure out what is throwing the warning because the end result may not be what you had intended! x &lt;- 1:2 y &lt;- 1:3 # Example of a warning x + y ## Warning in x + y: longer object length is not a multiple of shorter object length ## [1] 2 4 4 Messages: When the text doesn’t start with either “Error” or “Warning”, it’s just a friendly message. You’ll often see these messages when you load R packages or make certain function calls. Think of messages as a green traffic light: everything is working fine and keep on going! # Example of a message library(completejourney) ## Welcome to the completejourney package! Learn more about these data ## sets at http://bit.ly/completejourney. 3.7 Exercises What differentiates R and RStudio? What are the four main panes in RStudio? What function tells you what directory your are operating in? How could you change the directory to a new folder? How could you access R documentation to learn more about the lm() function? References "],["lesson-1c-r-fundamentals.html", "4 Lesson 1c: R fundamentals 4.1 Learning objectives 4.2 Assignment &amp; evaluation 4.3 R as a calculator 4.4 Working with packages 4.5 Style guide 4.6 Exercises", " 4 Lesson 1c: R fundamentals This lesson serves to introduce you to many of the basics of R to get you comfortable. This includes understanding how to assign and evaluate expressions, performing basic calculator-like activities, the idea of vectorization and packages. Finally, I offer some basic styling guidelines to help you write code that is easier to digest by others. 4.1 Learning objectives Upon completing this module you will be able to: Assign values to variables and evaluate them. Perform basic mathematical operations. Explain what packages and be able to install and load them. Understand and apply basic styling guidelines to your code. 4.2 Assignment &amp; evaluation 4.2.1 Assignment The first operator you’ll run into is the assignment operator. The assignment operator is used to assign a value. For instance we can assign the value 3 to the variable x using the &lt;- assignment operator. # idiomatic assignment x &lt;- 3 R is a dynamically typed programming language which means it will perform the process of verifying and enforcing the constraints of types at run-time. If you are unfamiliar with dynamically versus statically-typed languages then do not worry about this detail. Just realize that dynamically typed languages allow for the simplicity of running the above command and R automatically inferring that 3 should be a numeric type rather than a character string. Interestingly, R actually allows for five assignment operators3: # leftward assignment x &lt;- value x = value x &lt;&lt;- value # rightward assignment value -&gt; x value -&gt;&gt; x The original assignment operator in R was &lt;- and has continued to be the preferred among R users. The = assignment operator was added in 2001 primarily because it is the accepted assignment operator in many other languages and beginners to R coming from other languages were so prone to use it. Using = is not wrong, just realize that most R programmers prefer to keep = reserved for argument association and use &lt;- for assignment. The operators &lt;&lt;- is normally only used in functions or looping constructs which we will not get into the details. And the rightward assignment operators perform the same as their leftward counterparts, they just assign the value in an opposite direction. Overwhelmed yet? Don’t be. This is just meant to show you that there are options and you will likely come across them sooner or later. My suggestion is to stick with the tried, true, and idiomatic &lt;- operator. This is the most conventional assignment operator used and is what you will find in all the base R source code…which means it should be good enough for you. 4.2.2 Evaluation We can then evaluate the variable by simply typing x at the command line which will return the value of x. Note that prior to the value returned you’ll see ## [1] in the console. This simply implies that the output returned is the first output. Note that you can type any comments in your code by preceding the comment with the hash tag (#) symbol. Any values, symbols, and texts following # will not be evaluated. # evaluation x ## [1] 3 4.2.3 Case Sensitivity Lastly, note that R is a case sensitive programming language. Meaning all variables, functions, and objects must be called by their exact spelling: x &lt;- 1 y &lt;- 3 z &lt;- 4 x * y * z ## [1] 12 x * Y * z ## Error in eval(expr, envir, enclos): object &#39;Y&#39; not found 4.2.4 Knowledge check Assign the value 5 to variable x (note how this shows up in your Global Environment). Assign the character “abc” to variable y. Evaluate the value of x and y at in the console. Now use the rm() function to remove these objects from you working environment. 4.3 R as a calculator 4.3.1 Basic Arithmetic At its most basic function R can be used as a calculator. When applying basic arithmetic, the PEMDAS order of operations applies: parentheses first followed by exponentiation, multiplication and division, and finally addition and subtraction. 8 + 9 / 5 ^ 2 ## [1] 8.36 8 + 9 / (5 ^ 2) ## [1] 8.36 8 + (9 / 5) ^ 2 ## [1] 11.24 (8 + 9) / 5 ^ 2 ## [1] 0.68 By default R will display seven digits but this can be changed using options() as previously outlined. 1 / 7 ## [1] 0.14286 options(digits = 3) 1 / 7 ## [1] 0.143 Also, large numbers will be expressed in scientific notation which can also be adjusted using options(). 888888 * 888888 ## [1] 7.9e+11 options(digits = 10) 888888 * 888888 ## [1] 790121876544 Note that the largest number of digits that can be displayed is 22. Requesting any larger number of digits will result in an error message. pi ## [1] 3.141592654 options(digits = 22) pi ## [1] 3.141592653589793115998 options(digits = 23) ## Error in options(digits = 23): invalid &#39;digits&#39; parameter, allowed 1...22 pi ## [1] 3.141592653589793115998 We can also perform integer divide (%/%) and modulo (%%) functions. The integer divide function will give the integer part of a fraction while the modulo will provide the remainder. # regular division 42 / 4 ## [1] 10.5 # integer division 42 %/% 4 ## [1] 10 # modulo (remainder) 42 %% 4 ## [1] 2 4.3.2 Miscellaneous Mathematical Functions There are many built-in functions to be aware of. These include but are not limited to the following. Go ahead and run this code in your console. x &lt;- 10 abs(x) # absolute value sqrt(x) # square root exp(x) # exponential transformation log(x) # logarithmic transformation cos(x) # cosine and other trigonometric functions 4.3.3 Infinite, and NaN Numbers When performing undefined calculations, R will produce Inf (infinity) and NaN (not a number) outputs. These can easily pop up in regular data wrangling tasks and later modules will discuss how to work with these types of outputs along with missing values. # infinity 1 / 0 ## [1] Inf # infinity minus infinity Inf - Inf ## [1] NaN # negative infinity -1 / 0 ## [1] -Inf # not a number 0 / 0 ## [1] NaN # square root of -9 sqrt(-9) ## Warning in sqrt(-9): NaNs produced ## [1] NaN 4.3.4 Knowledge check Assign the values 1000, 5, and 0.05 to variables D, K, and h respectively. Compute \\(2 \\times D \\times K\\). Compute \\(\\frac{2 \\times D \\times K}{h}\\). Now put this together to compute the Economic Order Quantity, which is \\(\\sqrt{\\frac{2 \\times D \\times K}{h}}\\). Save the output as Q. 4.4 Working with packages In R, the fundamental unit of shareable code is the package. A package bundles together code, data, documentation, and tests and provides an easy method to share with others (Wickham 2015). As of April 2022 there were nearly 19,000 packages available on CRAN, 2,000 on Bioconductor, and countless more available through GitHub. This huge variety of packages is one of the reasons that R is so successful: chances are that someone has already solved a problem that you’re working on, and you can benefit from their work by downloading and using their package. 4.4.1 Installing Packages The most common place to get packages is from CRAN. To install packages from CRAN you use install.packages(\"packagename\"). For instance, if you want to install the ggplot2 package, which is a very popular visualization package you would type the following in the console: # install package from CRAN install.packages(&quot;ggplot2&quot;) As previously stated, packages are also available through Bioconductor and GitHub. Bioconductor provides R packages primarily for genomic data analyses and packages on GitHub are usually under development but have not gone through all the checks and balances to be loaded onto CRAN (aka download and use these packages at your discretion). You can learn how to install Bioconductor packages here and GitHub packages here. 4.4.2 Loading Packages Once the package is downloaded to your computer you can access the functions and resources provided by the package in two different ways: # load the package to use in the current R session library(packagename) # use a particular function within a package without loading the package packagename::functionname For instance, if you want to have full access to the dplyr package you would use library(dplyr); however, if you just wanted to use the filter() function which is provided by the dplyr package without fully loading dplyr you can use dplyr::filter(...)4. 4.4.3 Getting Help on Packages For more direct help on packages that are installed on your computer you can use the help and vignette functions. Here we can get help on the ggplot2 package with the following: help(package = &quot;ggplot2&quot;) # provides details regarding contents of a package vignette(package = &quot;ggplot2&quot;) # list vignettes available for a specific package vignette(&quot;ggplot2-specs&quot;) # view specific vignette vignette() # view all vignettes on your computer Note that some packages will have multiple vignettes. For instance vignette(package = \"grid\") will list the 13 vignettes available for the grid package. To access one of the specific vignettes you simply use vignette(\"vignettename\"). 4.4.4 Useful Packages There are thousands of helpful R packages for you to use, but navigating them all can be a challenge. To help you out, RStudio compiled a guide to some of the best packages for loading, manipulating, visualizing, analyzing, and reporting data. In addition, their list captures packages that specialize in spatial data, time series and financial data, increasing spead and performance, and developing your own R packages. 4.4.5 Knowledge check Install the completejourney package. Load the completejourney package. Access the help documentation for the completejourney package. Check out the vignette(s) for completejourney. Call the get_transactions() function provided by completejourney and save the output to a transactions variable (note that this takes a little time as you are trying to download 1.5 million transactions!). 4.4.6 Tidyverse The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures; and this group of packages has become extremely popular and common across the data science community. You will learn about many of these packages throughout this course. Take a little time to familiarize yourself with the Tidyverse. Not only will you find high-level descriptions of the different tidyverse packages but you will also find a lot of educational content that you can and should take advantage of! Figure 4.1: Tidyverse is a collection of packages designed to simplify many tasks throughout the data analysis process. Let’s go ahead and install the tidyverse package, which will actually install a bunch of other packages for us. install.packages(&quot;tidyverse&quot;) The single line of code above 👆… is equivalent to running the 29 lines of code below 👇! install.packages(&quot;ggplot2&quot;) install.packages(&quot;tibble&quot;) install.packages(&quot;tidyr&quot;) install.packages(&quot;readr&quot;) install.packages(&quot;purrr&quot;) install.packages(&quot;dplyr&quot;) install.packages(&quot;stringr&quot;) install.packages(&quot;forcats&quot;) install.packages(&quot;cli&quot;) install.packages(&quot;crayon&quot;) install.packages(&quot;dbplyr&quot;) install.packages(&quot;dtplyr&quot;) install.packages(&quot;googledrive&quot;) install.packages(&quot;googlesheets4&quot;) install.packages(&quot;haven&quot;) install.packages(&quot;hms&quot;) install.packages(&quot;httr&quot;) install.packages(&quot;jsonlite&quot;) install.packages(&quot;lubridate&quot;) install.packages(&quot;magrittr&quot;) install.packages(&quot;modelr&quot;) install.packages(&quot;pillar&quot;) install.packages(&quot;readxl&quot;) install.packages(&quot;reprex&quot;) install.packages(&quot;rlang&quot;) install.packages(&quot;rstudioapi&quot;) install.packages(&quot;rvest&quot;) install.packages(&quot;xml2&quot;) If we load the tidyverse package we will see that it loads 8 packages for us: ggplot2_, tibble, tidyr, readr, purrr, dplyr, stringr, and forcats. These are packages that we will tend to use in almost any data analysis project. library(tidyverse) The single line of code above 👆… is equivalent to running the 8 lines of code below 👇! library(ggplot2) library(tibble) library(tidyr) library(readr) library(purrr) library(dplyr) library(stringr) library(forcats) 4.5 Style guide “Good coding style is like using correct punctuation. You can manage without it, but it sure makes things easier to read.” - Hadley Wickham As a medium of communication, its important to realize that the readability of code does in fact make a difference. Well styled code has many benefits to include making it easy to i) read, ii) extend, and iii) debug. Unfortunately, R does not come with official guidelines for code styling but such is an inconvenient truth of most open source software. However, this should not lead you to believe there is no style to be followed and over time implicit guidelines for proper code styling have been documented. What follows are a few of the basic guidelines from the tidyverse style guide. These suggestions will help you get started with good styling suggestions as you begin this book but as you progress you should leverage the far more detailed tidyverse style guide along with useful packages such as lintr and styler to help enforce good code syntax on yourself. 4.5.1 Notation and Naming File names should be meaningful and end with a .R extension. # Good weather-analysis.R emerson-text-analysis.R lesson-1b-homework.R # Bad basic-stuff.r detail.r If files need to be run in sequence, prefix them with numbers: 0-download.R 1-preprocessing.R 2-explore.R 3-fit-model.R In R, naming conventions for variables and functions are famously muddled. They include the following: namingconvention # all lower case; no separator naming.convention # period separator naming_convention # underscore separator (aka snake case) namingConvention # lower camel case NamingConvention # upper camel case Historically, there has been no clearly preferred approach with multiple naming styles sometimes used within a single package. Bottom line, your naming convention will be driven by your preference but the ultimate goal should be consistency. Vast majority of the R community uses lowercase with an underscore (“_“) to separate words within a variable/function name (‘snake_case’). Furthermore, variable names should be nouns and function names should be verbs to help distinguish their purpose. Also, refrain from using existing names of functions (i.e. mean, sum, true). 4.5.2 Organization Organization of your code is also important. There’s nothing like trying to decipher 500 lines of code that has no organization. The easiest way to achieve organization is to comment your code. When you have large sections within your script you should separate them to make it obvious of the distinct purpose of the code. # Download Data ------------------------------------------------------------------- lines of code here # Preprocess Data ----------------------------------------------------------------- lines of code here # Exploratory Analysis ------------------------------------------------------------ lines of code here You can easily add these section breaks within RStudio wth Cmd+Shift+R. Then comments for specific lines of code can be done as follows: code_1 # short comments can be placed to the right of code code_2 # blah code_3 # blah # or comments can be placed above a line of code code_4 # Or extremely long lines of commentary that go beyond the suggested 80 # characters per line can be broken up into multiple lines. Just don&#39;t forget # to use the hash on each. code_5 You can easily comment or uncomment lines by highlighting the line and then pressing Cmd+Shift+C. 4.5.3 Syntax The maximum number of characters on a single line of code should be 80 or less. If you are using RStudio you can have a margin displayed so you know when you need to break to a new line5. Also, when indenting your code use two spaces rather than using tabs. The only exception is if a line break occurs inside parentheses. In this case it is common to do either of the following: # option 1 super_long_name &lt;- seq(ymd_hm(&quot;2015-1-1 0:00&quot;), ymd_hm(&quot;2015-1-1 12:00&quot;), by = &quot;hour&quot;) # option 2 super_long_name &lt;- seq( ymd_hm(&quot;2015-1-1 0:00&quot;), ymd_hm(&quot;2015-1-1 12:00&quot;), by = &quot;hour&quot; ) Proper spacing within your code also helps with readability. Place spaces around all infix operators (=, +, -, &lt;-, etc.). The same rule applies when using = in function calls. Always put a space after a comma, and never before. # Good average &lt;- mean(feet / 12 + inches, na.rm = TRUE) # Bad average&lt;-mean(feet/12+inches,na.rm=TRUE) There’s a small exception to this rule: :, :: and ::: don’t need spaces around them. # Good x &lt;- 1:10 base::get # Bad x &lt;- 1 : 10 base :: get It is important to think about style when communicating any form of language. Writing code is no exception and is especially important if your code will be read by others. Following these basic style guides will get you on the right track for writing code that can be easily communicated to others. 4.5.4 Knowledge check Review chapters 1 &amp; 2 in the Tidyverse Style Guide. Go back through the script you’ve been writing to execute the exercises in this module and make sure your naming conventions are consistent, your code is nicely organized and annotated, your syntax includes proper spacing. 4.6 Exercises Say you have a 12” pizza. Compute the area of the pizza and assign that value to the variable area. Now say the cost of the pizza was $8. Compute the cost per square inch and assign that value to a variable ppsi. Based on the style guide section rename the ppsi variable in question 1 to be more meaningful. If you did not already do so, install the tidyverse package. How many vignettes does the dplyr package have? Where can you go to learn more about the tidyverse packages? When you load the tidyverse packages what other packages is it automatically loading for you? Using the resource in #5, explain at a high-level what the packages in #6 do. References "],["lesson-1d-vectors.html", "5 Lesson 1d: Vectors 5.1 Learning objectives 5.2 Creating vectors 5.3 Extracting elements 5.4 Replacing elements 5.5 Operations 5.6 Missing data 5.7 Vectorization 5.8 Exercises", " 5 Lesson 1d: Vectors In the last module we started to explore how to use R as a calculator. This is great; however, we were only working with individual values. Often, we want to work on several values at once so we need a structure that will hold multiple pieces of data. We will discuss data structures more in the next module but in this lesson we introduce the vector, which is the fundamental data structure in R. Once you have a good grasp of working with vectors, then working with other data structures because much easier. 5.1 Learning objectives By the end of this lesson you will be able to: Create vectors. Extract and replace elements within a vector. Perform basic operations on a vector (i.e. compute the mean). Work with missing data in a vector. Explain and take advantage of vectorization. 5.2 Creating vectors There are multiple ways to create vectors but the first one you’ll be introduced to is by using c(). The c() function is short for concatenate and we use it to join together a series of values and store them in a vector. my_vec &lt;- c(2, 3, 1, 6, 4, 3, 3, 7) To examine the value of our new object we can simply type out the name of the object as we did before: my_vec ## [1] 2 3 1 6 4 3 3 7 5.2.1 Creating sequences Sometimes it can be useful to create a vector that contains a regular sequence of values in steps of one. Here we can make use of a shortcut using the : symbol. my_seq &lt;- 1:10 # create regular sequence my_seq ## [1] 1 2 3 4 5 6 7 8 9 10 my_seq2 &lt;- 10:1 # in decending order my_seq2 ## [1] 10 9 8 7 6 5 4 3 2 1 Other useful functions for generating vectors of sequences include the seq() and rep() functions. For example, to generate a sequence from 1 to 5 in steps of 0.5 my_seq2 &lt;- seq(from = 1, to = 5, by = 0.5) my_seq2 ## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Here we’ve used the arguments from = and to = to define the limits of the sequence and the by = argument to specify the increment of the sequence. Play around with other values for these arguments to see their effect. The rep() function allows you to replicate (repeat) values a specified number of times. To repeat the value 2, 10 times # repeats 2, 10 times my_seq3 &lt;- rep(2, times = 10) my_seq3 ## [1] 2 2 2 2 2 2 2 2 2 2 You can also repeat non-numeric values # repeats ‘abc’ 3 times my_seq4 &lt;- rep(&quot;abc&quot;, times = 3) my_seq4 ## [1] &quot;abc&quot; &quot;abc&quot; &quot;abc&quot; or each element of a series # repeats the series 1 to 5, 3 times my_seq5 &lt;- rep(1:5, times = 3) my_seq5 ## [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 or elements of a series # repeats each element of the series 3 times my_seq6 &lt;- rep(1:5, each = 3) my_seq6 ## [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 We can also repeat a non-sequential series # repeats each element of the series 3 times my_seq7 &lt;- rep(c(3, 1, 10, 7), each = 3) my_seq7 ## [1] 3 3 3 1 1 1 10 10 10 7 7 7 Note in the code above how we’ve used the c() function inside the seq() function. Nesting functions allows us to build quite complex commands within a single line of code and is a very common practice when using R. However, care needs to be taken as too many nested functions can make your code quite difficult for others to understand (or yourself some time in the future!). We could rewrite the code above to explicitly separate the two different steps to generate our vector. Either approach will give the same result, you just need to use your own judgement as to which is more readable. in_vec &lt;- c(3, 1, 10, 7) my_seq7 &lt;- rep(in_vec, each = 3) my_seq7 ## [1] 3 3 3 1 1 1 10 10 10 7 7 7 5.2.2 Knowledge check Use c() to create a vector called weight containing the weight (in lbs) of 10 children: 75, 95, 92, 89, 101, 87, 79, 92, 70, 99. Use the seq() function to create a sequence of numbers ranging from 0 to 1 in steps of 0.1. Generate the following sequences. You will need to experiment with the arguments to the rep() function to generate these sequences: 1 2 3 1 2 3 1 2 3 “a” “a” “a” “c” “c” “c” “e” “e” “e” “g” “g” “g” “a” “c” “e” “g” “a” “c” “e” “g” “a” “c” “e” “g” 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 1 1 2 2 2 2 3 3 3 4 4 5 5.3 Extracting elements To extract (also known as indexing) one or more values (more generally known as elements) from a vector we use the square bracket [ ] notation. The general approach is to name the object you wish to extract from, then a set of square brackets with an index of the element you wish to extract contained within the square brackets. This index can be a position or the result of a logical test. 5.3.1 Positional indexing # extract the 3rd value my_vec[3] ## [1] 1 # if you want to assign this value in another object val_3 &lt;- my_vec[3] val_3 ## [1] 1 Note that the positional index starts at 1 rather than 0 like some other other programming languages (i.e. Python). We can also extract more than one value by using the c() function inside the square brackets. Here we extract the 1st, 5th, 6th and 8th element from the my_vec object my_vec[c(1, 5, 6, 8)] ## [1] 2 4 3 7 Or we can extract a range of values using the : notation. To extract the values from the 3rd to the 8th elements my_vec[3:8] ## [1] 1 6 4 3 3 7 5.3.2 Logical indexing Another really useful way to extract data from a vector is to use a logical expression as an index. For example, to extract all elements with a value greater than 4 in the vector my_vec my_vec[my_vec &gt; 4] ## [1] 6 7 Here, the logical expression is my_vec &gt; 4 and R will only extract those elements that satisfy this logical condition. So how does this actually work? If we look at the output of just the logical expression without the square brackets you can see that R returns a vector containing either TRUE or FALSE which correspond to whether the logical condition is satisfied for each element. In this case only the 4th and 8th elements return a TRUE as their value is greater than 4. my_vec &gt; 4 ## [1] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE So what R is actually doing under the hood is equivalent to my_vec[c(FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE)] ## [1] 6 7 and only those element that are TRUE will be extracted. In addition to the &lt; and &gt; operators you can also use composite operators to increase the complexity of your expressions. For example the expression for ‘greater or equal to’ is &gt;=. To test whether a value is equal to a value we need to use a double equals symbol == and for ‘not equal to’ we use != (the ! symbol means ‘not’). my_vec[my_vec &gt;= 4] # values greater or equal to 4 ## [1] 6 4 7 my_vec[my_vec &lt; 4] # values less than 4 ## [1] 2 3 1 3 3 my_vec[my_vec &lt;= 4] # values less than or equal to 4 ## [1] 2 3 1 4 3 3 my_vec[my_vec == 4] # values equal to 4 ## [1] 4 my_vec[my_vec != 4] # values not equal to 4 ## [1] 2 3 1 6 3 3 7 We can also combine multiple logical expressions using Boolean expressions. In R the &amp; symbol means AND and the | symbol means OR. For example, to extract values in my_vec which are less than 6 AND greater than 2 val26 &lt;- my_vec[my_vec &lt; 6 &amp; my_vec &gt; 2] val26 ## [1] 3 4 3 3 or extract values in my_vec that are greater than 6 OR less than 3 val63 &lt;- my_vec[my_vec &gt; 6 | my_vec &lt; 3] val63 ## [1] 2 1 7 5.3.3 Knowledge check Use c() to create a vector called weight containing the weight (in lbs) of 10 children: 75, 95, 92, 89, 101, 87, 79, 92, 70, 99. Extract the weights for the first and last child. Extract the weights for the first five children. Extract the weights for those children that weigh over 90lbs. 5.4 Replacing elements We can change the values of some elements in a vector using our [ ] notation in combination with the assignment operator &lt;-. For example, to replace the 4th value of our my_vec object from 6 to 500 my_vec[4] &lt;- 500 my_vec ## [1] 2 3 1 500 4 3 3 7 We can also replace more than one value or even replace values based on a logical expression # replace the 6th and 7th element with 100 my_vec[c(6, 7)] &lt;- 100 my_vec ## [1] 2 3 1 500 4 100 100 7 # replace element that are less than or equal to 4 with 1000 my_vec[my_vec &lt;= 4] &lt;- 1000 my_vec ## [1] 1000 1000 1000 500 1000 100 100 7 5.4.1 Knowledge check Use c() to create a vector called weight containing the weight (in lbs) of 10 children: 75, 95, 92, 89, 101, 87, 79, 92, 70, 99. Say we made a mistake and the third child actually weighs 96, change the value for that element. Say the minimum weight allowed to be recorded is 80lbs. Change the value for all elements that are less than 80 to be equal to 80. 5.5 Operations We can use other functions to perform useful operations on vectors. For example, we can calculate the mean, variance, standard deviation and number of elements in our vector by using the mean(), var(), sd() and length() functions mean(my_vec) # returns the mean of my_vec ## [1] 588.38 var(my_vec) # returns the variance of my_vec ## [1] 214367 sd(my_vec) # returns the standard deviation of my_vec ## [1] 463 length(my_vec) # returns the number of elements in my_vec ## [1] 8 If we wanted to use any of these values later on in our analysis we can just assign the resulting value to another object vec_mean &lt;- mean(my_vec) vec_mean ## [1] 588.38 We can also sort values in our element: # ascending order sort(my_vec) ## [1] 7 100 100 500 1000 1000 1000 1000 # decending order sort(my_vec, decreasing = TRUE) ## [1] 1000 1000 1000 1000 500 100 100 7 There are a lot operations that we can perform on vectors. As we progress through this course we’ll see many of these in action. 5.5.1 Knowledge check Use c() to create a vector called weight containing the weight (in lbs) of 10 children: 75, 95, 92, 89, 101, 87, 79, 92, 70, 99. Identify functions that will compute the minimum and maximum values in this vector. What is the mean, median, and standard deviation of child weights? Apply the summary() function on this vector. What does this return? 5.6 Missing data In R, missing data is usually represented by an NA symbol meaning ‘Not Available’. Data may be missing for a whole bunch of reasons, maybe your machine broke down, maybe the weather was too bad to collect data on a particular day, etc. Missing data can be a pain in the proverbial both from an R perspective and also a statistical perspective. From an R perspective missing data can be problematic as different functions deal with missing data in different ways. For example, let’s say we collected air temperature readings over 10 days, but our thermometer broke on day 2 and again on day 9 so we have no data for those days temp &lt;- c(7.2, NA, 7.1, 6.9, 6.5, 5.8, 5.8, 5.5, NA, 5.5) temp ## [1] 7.2 NA 7.1 6.9 6.5 5.8 5.8 5.5 NA 5.5 We now want to calculate the mean temperature over these days using the mean() function mean_temp &lt;- mean(temp) mean_temp ## [1] NA Flippin heck, what’s happened here? Why does the mean() function return an NA? Actually, R is doing something very sensible (at least in our opinion!). If a vector has a missing value then the only possible value to return when calculating a mean is NA. R doesn’t know that you perhaps want to ignore the NA values. Happily, if we look at the help file via help(\"mean\") we can see there is an argument na.rm = which is set to FALSE by default. Most statistical operators will have an na.rm parameter that takes a TRUE or FALSE argument indicating whether NA values should be stripped before the computation proceeds. If we change this argument to na.rm = TRUE when we use the mean() function this will allow us to ignore the NA values when calculating the mean mean_temp &lt;- mean(temp, na.rm = TRUE) mean_temp ## [1] 6.2875 It’s important to note that the NA values have not been removed from our temp object (that would be bad practice), rather the mean() function has just ignored them. The point of the above is to highlight how we can change the default behaviour of a function using an appropriate argument. 5.6.1 Knowledge check Use c() to create a vector called weight containing the weight (in lbs) of 10 children: 75, 95, 92, 89, 101, NA, 79, 92, 70, 99. Compute the min, max, mean, median, and standard deviation of child weights? Apply the summary() function on this vector. How does this differ from before? 5.7 Vectorization 5.7.1 Looping versus Vectorization A key difference between R and many other languages is a topic known as vectorization. What does this mean? It means that many functions that are to be applied individually to each element in a vector of numbers require a loop assessment to evaluate; however, in R many of these functions have been coded in C to perform much faster than a for loop would perform. For example, let’s say you want to add the elements of two separate vectors of numbers (x and y). x &lt;- c(1, 3, 4) y &lt;- c(1, 2, 4) x ## [1] 1 3 4 y ## [1] 1 2 4 In other languages you might have to run a loop to add two vectors together. In this for loop I print each iteration to show that the loop calculates the sum for the first elements in each vector, then performs the sum for the second elements, etc. Don’t worry if you don’t understand each piece of this code. This is just for illustration purposes! You will learn about looping procedures in a later module. # empty vector to store results z &lt;- as.vector(NULL) # `for` loop to add corresponding elements in each vector for (i in seq_along(x)) { z[i] &lt;- x[i] + y[i] print(z) } ## [1] 2 ## [1] 2 5 ## [1] 2 5 8 Instead, in R, + is a vectorized function which can operate on entire vectors at once. So rather than creating for loops for many functions, you can just use simple syntax to perform element-wise operations with both vectors: # add each element in x and y x + y ## [1] 2 5 8 # multiply each element in x and y x * y ## [1] 1 6 16 # compare each element in x to y x &gt; y ## [1] FALSE TRUE FALSE 5.7.2 Recycling When performing vector operations in R, it is important to know about recycling. When performing an operation on two or more vectors of unequal length, R will recycle elements of the shorter vector(s) to match the longest vector. For example: long &lt;- 1:10 short &lt;- 1:5 long ## [1] 1 2 3 4 5 6 7 8 9 10 short ## [1] 1 2 3 4 5 # R will recycle (reuse) the short vector until it reaches # the end of the long vector long + short ## [1] 2 4 6 8 10 7 9 11 13 15 The elements of long and short are added together starting from the first element of both vectors. When R reaches the end of the short vector, it starts again at the first element of short and continues until it reaches the last element of the long vector. This functionality is very useful when you want to perform the same operation on every element of a vector. For example, say we want to multiply every element of our vector long by 3: long &lt;- 1:10 c &lt;- 3 long * c ## [1] 3 6 9 12 15 18 21 24 27 30 There are no scalars in R, so c is actually a vector of length 1; in order to add its value to every element of long, it is recycled to match the length of long. Don’t get hung up with some of the verbiage used here (i.e. vectors vs. scalars), we will cover what this means in later a module. When the length of the longer object is a multiple of the shorter object length, the recycling occurs silently. When the longer object length is not a multiple of the shorter object length, a warning is given: even_length &lt;- 1:10 odd_length &lt;- 1:3 even_length + odd_length ## Warning in even_length + odd_length: longer object length is not a multiple of shorter object length ## [1] 2 4 6 5 7 9 8 10 12 11 5.7.3 Knowledge check Create this vector my_vec &lt;- 1:10. Add 1 to every element in my_vec. Divide every element in my_vec by 2. Create a second vector my_vec2 &lt;- 10:18 and add my_vec to my_vec2. 5.8 Exercises Create a vector called weight containing the weight (in kg) of 10 children: 69, 62, 57, 59, 59, 64, 56, 66, 67, 66. Create a vector called height containing the height (in cm) of the same 10 children: 112, 102, 83, 84, 99, 90, 77, 112, 133, 112. Use the summary() function to summarize these data. Extract the height of the 2nd, 3rd, 9th and 10th child and assign these heights to a variable called some_child. Also extract all the heights of children less than or equal to 99 cm and assign to a variable called shorter_child. Use the information in your weight and height variables to calculate the body mass index (BMI) for each child. The BMI is calculated as weight (in kg) divided by the square of the height (in meters). Store the results of this calculation in a variable called bmi. Note: you don’t need to do this calculation for each child individually, you can leverage vectorization to do this! "],["lab.html", "6 Lab", " 6 Lab TBD "],["overview-1.html", "7 Overview 7.1 Learning objectives 7.2 Tasks 7.3 Course readings", " 7 Overview Welcome to module 2! This module will focus on … 7.1 Learning objectives By the end of this module you should be able to: Use RStudio Projects to organize a data wrangling project. Understand how to use R scripts, R Markdown, and R Notebook documents to make your data wrangling projects and outputs reproducible. Import data from a variety of sources and perform basic tasks to understand simple attributes about your imported dataset. Describe key differences between different data structures. 7.2 Tasks TBD 7.3 Course readings TBD "],["lesson-2a-workflow.html", "8 Lesson 2a: Workflow 8.1 Learning objectives 8.2 R Projects 8.3 R Markdown 8.4 R Notebooks 8.5 Exercises 8.6 Computing environment", " 8 Lesson 2a: Workflow This lesson serves to introduce you to workflow options that will serve to organize your projects and make them reproducible. You’ll learn how to have a project-oriented environment along with how to create R Markdown and Notebook scripts for efficient and reproducible deliverables. 8.1 Learning objectives Upon completing this module you will be able to: Explain the benefits of an R project and new ones. Explain the similarities and differences between R Markdown files and R Notebooks. Create both R Markdown and R Notebook deliverables. 8.2 R Projects “Organization is what you do before you do something, so that when you do it, it is not all mixed up.” - A.A. Milne If you are not careful your data analyses can become an explosion of data files, R scripts, ggplot graphs, and final reports. Each project evolves and mutates in its own way and keeping all the files associated with a project organized together is a wise practice. In fact, it is such a wise practice that RStudio has built-in support to manage your projects. This built-in capability is called…wait for it…RStudio projects. RStudio projects make it straightforward to divide your work into multiple contexts, each with their own working directory, workspace, history, and source documents. 8.2.1 Creating Projects RStudio projects are associated with R working directories. You can create an RStudio project: In a new directory In an existing directory where you already have R code and data By cloning a version control (Git or Subversion) repository by selecting File » New Project and then completing the following set-up tasks: Figure 8.1: Creating a new R project. 8.2.2 So What’s Different? When a new project is created RStudio: Creates a project file (with an .Rproj extension) within the project directory. This file contains various project options (discussed below) and can also be used as a shortcut for opening the project directly from the filesystem. Creates a hidden directory (named .Rproj.user) where project-specific temporary files (e.g. auto-saved source documents, window-state, etc.) are stored. This directory is also automatically added to .Rbuildignore, .gitignore, etc. if required. Loads the project into RStudio and display its name in the Projects toolbar (which is located on the far right side of the main toolbar) Figure 8.2: Example of a new R project. When a project is opened (File » Open Project or by clicking on the .Rproj file directly for the project): A new R session is started The .Rprofile file in the project’s main directory is sourced by R The .RData file in the project’s main directory is loaded (if any) The history for the project is loaded into the History panel The working directory is set to the project’s directory. Previously edited source documents are restored into editor tabs Other RStudio settings are restored to where they were the last time the project was closed As you write and execute code in the project all updates and outputs created will be saved to the project directory. And when you close out of the project the .RData and .Rhistory files will be saved (if these options are selected in the global options) and the list of open source documents are saved so that they can be restored the next time you open the project. There are additional project options you can choose from to customize the project at Tools » Project Options. These project options are overrides for existing global options. To inherit the default global behavior for a project you can specify (Default) as the option value. Figure 8.3: Project options. 8.2.3 Knowledge check Go ahead and create an R Project for this class. Make sure the following RStudio preference settings are set: General: Set “Save workspace to .RData on exit: Never”. Code: In the display tab check the “Show margin” option and set “Margin Column: 80”. Code &gt;&gt; Diagnostics: Make sure the “Provide R style diagnostics” is checked. 8.3 R Markdown R Markdown provides an easy way to produce a rich, fully-documented reproducible analysis. It allows the user to share a single file that contains all of the prose, code, and metadata needed to reproduce the analysis from beginning to end. R Markdown allows for “chunks” of R code to be included along with Markdown text to produce a nicely formatted HTML, PDF, or Word file without having to know any HTML or LaTeX code or have to fuss with difficult formatting issues. One R Markdown file can generate a variety of different formats and all of this is done in a single text file with a few bits of formatting. Figure 8.4: R Markdown can generate many different types of reports. So how does it work? Creating documents with R Markdown starts with an .Rmd file that contains a combination of text and R code chunks. The .Rmd file is fed to knitr, which executes all of the R code chunks and creates a new markdown (.md) document with the output. Pandoc then processes the .md file to create a finished report in the form of a web page, PDF, Word document, slide show, etc. Figure 8.5: R Markdown generation process. Sounds confusing you say, don’t fret. Much of what takes place happens behind the scenes. You primarily need to worry only about the syntax required in the .Rmd file. You then press a button and out comes your report. Figure 8.6: R Markdown generation process that you care about. 8.3.1 Creating an R Markdown File To create an R Markdown file you can select File » New File » R Markdown or you can select the shortcut for creating a new document in the top left-hand corner of the RStudio window. You will be given an option to create an HTML, PDF, or Word document; however, R Markdown let’s you change seamlessly between these options after you’ve created your document so I tend to just select the default HTML option. Figure 8.7: Creating a new R Markdown file. There are additional options such as creating Presentations (HTML or PDF), Shiny documents, or other template documents but for now we will focus on the initial HTML, PDF, or Word document options. 8.3.2 Components of an R Markdown File There are three general components of an R Markdown file that you will eventually become accustomed to. This includes the YAML, the general markdown (or text) component, and code chunks. 8.3.2.1 YAML Header The first few lines you see in the R Markdown report are known as the YAML. --- title: &quot;R Markdown Demo&quot; author: &quot;Brad Boehmke&quot; date: &quot;2016-08-15&quot; output: html_document --- These lines will generate a generic heading at the top of the final report. There are several YAML options to enhance your reports such as the following: You can include hyperlinks around the title or author name: --- title: &quot;R Markdown Demo&quot; author: &quot;[Brad Boehmke](http://bradleyboehmke.github.io)&quot; date: &quot;2016-08-15&quot; output: html_document --- If you don’t want the date to be hard-coded you can include R code so that anytime you re-run the report the current date will print off at the top. You can also exclude the date (or author and title information) by including NULL or simply by deleting that line: --- title: &quot;R Markdown Demo&quot; author: &quot;[Brad Boehmke](http://bradleyboehmke.github.io)&quot; date: &quot;2022-06-14&quot; output: html_document --- By default, your report will not include a table of contents (TOC). However, you can easily generate one by including the toc: true argument. There are several TOC options such as the level of headers to include in the TOC, whether to have a fixed or floating TOC, to have a collapsable TOC, etc. You can find many of the TOC options here. --- title: &quot;R Markdown Demo&quot; author: &quot;[Brad Boehmke](http://bradleyboehmke.github.io)&quot; date: &quot;2022-06-14&quot; output: html_document: toc: true toc_float: true --- When knitr processes an R Markdown input file it creates a markdown (.md) file which is subsequently transformed into HTML by pandoc. If you want to keep a copy of the markdown file after rendering you can do so using the keep_md: true option. This will likely not be a concern at first but when (if) you start doing a lot of online writing you will find that keeping the .md file is extremely beneficial. --- title: &quot;R Markdown Demo&quot; author: &quot;[Brad Boehmke](http://bradleyboehmke.github.io)&quot; date: &quot;2022-06-14&quot; output: html_document: keep_md: true --- There are many YAML options which you can read more about at: HTML reports: http://rmarkdown.rstudio.com/html_document_format.html PDF (LaTex) reports: http://rmarkdown.rstudio.com/pdf_document_format.html Word reports: http://rmarkdown.rstudio.com/word_document_format.html 8.3.2.2 Text Formatting The beauty of R Markdown is the ability to easily combine prose (text) and code. For the text component, much of your writing is similar to when you type a Word document; however, to perform many of the basic text formatting you use basic markdown code such as: Figure 8.8: Markdown syntax. There are many additional formatting options which can be viewed here and here; however, this should get you well on your way. 8.3.2.3 Code Chunks R code chunks can be used as a means to render R output into documents or to simply display code for illustration. Code chunks start with the following line: {r chunk_name}&lt;/code&gt; and end with &lt;code&gt;. You can quickly insert chunks into your R Markdown file with the keyboard shortcut Cmd + Option + I (Windows Ctrl + Alt + I). Here is a simple R code chunk that will result in both the code and it’s output being included: ```{r} head(iris) ``` Figure 8.9: Code chunks in the .Rmd file versus the output produced. Chunk output can be customized with many knitr options which are arguments set in the {} of a chunk header. Examples include: 1. echo=FALSE hides the code but displays results: ```{r echo=FALSE} x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) cor(x, y) ``` 2. results='hide' hides the results but shows the code ```{r results=&#39;hide&#39;} x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) cor(x, y) ``` 3. eval=FALSE displays the code but does not evaluate it ```{r eval=FALSE} x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) cor(x, y) ``` 4. include=FALSE evaluates the code but does not display code or output ```{r include=FALSE} x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) cor(x, y) ``` 5. warning=FALSE and message=FALSE are useful for suppressing any messages produced when loading packages ```{r, warning=FALSE, message=FALSE} library(dplyr) ``` 6. collapse=TRUE will collapse your output to be contained within the code chunk ```{r, collapse=TRUE} head(iris) ``` 7. fig... options are available to align and size figure outputs ```{r, fig.align=&#39;center&#39;, fig.height=3, fig.width=4} library(ggplot2) ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() ``` Figure 8.10: Code chunks in the .Rmd file versus the output produced. 8.3.2.4 Inline code chunks A key motivation for reproducible research is to link any results reported directly to the data and functions used to create them. Consequently, you should never manual insert numbers such as “The average miles per gallon is 20.1.” Rather, code results can be inserted directly into the text of a .Rmd file by enclosing the code with `r ` such as: “The average miles per gallon is `r mean(mtcars$mpg)`.” Now if the underlying data changes you do not need to remember all the inline values you manually entered. You may not like the fact that the output is reporting all the decimals. You could include the round function in the inline code: `r round(mean(mtcars$mpg), 1)`. Figure 8.11: Inline code chunks in the .Rmd file versus the output produced. 8.3.2.5 Dealing with Tables By default, the table outputs produced in R Markdown will look like the output you would see in your console. However, if you prefer that data be displayed with additional formatting you can use the knitr::kable() function. For example: ```{r, results=&#39;asis&#39;} knitr::kable(iris) ``` To include captions: ```{r} knitr::kable(head(iris), caption = &#39;Example caption for the iris data frame&#39;) ``` The simplest approach to print nice looking tables is to use the printr package which can be installed from CRAN. ```{r} library(printr) head(iris) ``` Figure 8.12: Tables in the .Rmd file versus the output produced. There are several packages that can be used to make very nice tables: printr xtable stargazer tables pander 8.3.3 Knitting the R Markdown File When you are all done writing your .Rmd document you have two options to render the output. The first is to call the following function in your console: render(\"document_name.Rmd\", output_format = \"html_document\"). Alternatively you can click the drop down arrow next to the knit button on the RStudio toolbar, select the document format (HTML, PDF, Word) and your report will be developed. Figure 8.13: Generating (aka knitting) a report. The following output formats are available to use with R Markdown. Documents: html_notebook - Interactive R Notebooks html_document - HTML document w/ Bootstrap CSS pdf_document - PDF document (via LaTeX template) word_document - Microsoft Word document (docx) odt_document - OpenDocument Text document rtf_document - Rich Text Format document md_document - Markdown document (various flavors) Presentations (slides): ioslides_presentation - HTML presentation with ioslides revealjs::revealjs_presentation - HTML presentation with reveal.js slidy_presentation - HTML presentation with W3C Slidy beamer_presentation - PDF presentation with LaTeX Beamer More: flexdashboard::flex_dashboard - Interactive dashboards tufte::tufte_handout - PDF handouts in the style of Edward Tufte tufte::tufte_html - HTML handouts in the style of Edward Tufte tufte::tufte_book - PDF books in the style of Edward Tufte html_vignette - R package vignette (HTML) github_document - GitHub Flavored Markdown document bookdown - Write HTML, PDF, ePub, and Kindle books with R Markdown 8.3.4 Additional Resources R Markdown is an incredible tool for reproducible research and there are a lot of resources available. Here are just a few of the available resources to learn more about R Markdown. Rstudio tutorials R Markdown course by DataCamp Karl Browman’s tutorial Daring Fireball Reproducible Research course on Coursera Chester Ismay’s book Also, you can find the R Markdown cheatsheet within the RStudio console at Help menu » Cheatsheets. Figure 8.14: R Markdown cheat sheet. 8.3.5 Knowledge check Create a new R Markdown document with File &gt; New File &gt; R Markdown… Knit it by clicking the appropriate button. Knit it by using the appropriate keyboard short cut. Verify that you can modify the input and see the output update. Practice what you’ve learned by creating a brief CV. The title should be your name, and you should include headings for (at least) education or employment. Each of the sections should include a bulleted list of jobs/degrees. Highlight the year in bold. Using the R Markdown quick reference, figure out how to: Add a footnote. Add a horizontal rule. Add a block quote. 8.4 R Notebooks An R Notebook is an R Markdown document that allows for independent and interactive execution of the code chunks. This allows you to visually assess the output as you develop your R Markdown document without having to knit the entire document to see the output. Figure 8.15: Example R Notebook. R Notebooks can be thought of as a unique execution mode for R Markdown documents as any R Markdown document can be used as a notebook, and all R Notebooks can be rendered to other R Markdown document types. The interactive capabilities of the notebook mode makes it extremely useful for writing R Markdown documents and iterating on code. 8.4.1 Creating an R Notebook Creating an R Notebook is similar to creating an R Markdown document - you’ll notice a new option for creating an R Notebook. When you create a new R Notebook the primary differece you will notice at first is the YAML which will look like: --- title: &quot;R Notebook&quot; output: html_notebook --- Figure 8.16: Creating an R Notebook. The default notebook mode allows inline output on all R Markdown documents. If you prefer to use the traditional console method of interaction, you can disable notebook mode by clicking the gear in the editor toolbar and choosing Chunk Output in Console. You can also toggle between previewing the document in the Viewer Pane versus in a Window. Figure 8.17: R Notebook execution options. 8.4.2 Interactiveness of an R Notebook Writing an R Notebook document is no different than writing an R Markdown document. The text and code chunk syntax does not differ from what you learned in the previous section of this lesson. The primary difference is in the interactiveness of an R Notebook. Primarily that when executing chunks in an R Markdown document, all the code is sent to the console at once, but in an R Notebook, only one line at a time is sent. This allows execution to stop if a line raises an error. There are couple options for executing code chunks. You can execute code chunks individually by: Having the cursor within the code chunk and selecting ⌘ + enter Clicking the Run Current Chunk button in the first line (far right-hand side) of the code chunk Or selecting the Run Current Chunk option from the Run menu in the RStudio console toolbar You can also run all chunks in your document by: Selecting the Run All option from the Run menu in the RStudio console toolbar Using the keyboard shortcut ⌥ + ⌘ + R When a code chunk is waiting to be executed, you’ll notice a progress meter that appears to the left of the code chunk plus there will be a status in the editor’s status bar indicating the number of chunks remaining to be executed. You can click on this meter at any time to jump to the currently executing chunk. Figure 8.18: Interactiveness of an R Notebook. 8.4.3 Saving, Sharing, Previewing &amp; Knitting an R Notebook When a notebook .Rmd is saved, an .nb.html file is created alongside it. This file is a self-contained HTML file which contains all current code chunks (collapsable/expandable) and their respective outputs. You can view this .nb.html file directly in any browser along with sharing it with others who can also view it in any browser. Ordinary R Markdown documents are “knit”, but notebooks are “previewed”. So by default, when you select the preview option in the editor toolbar your document will be previewed in the Viewer Pane. You can preview your document in a window by selecting the desired option in the gear in the editor toolbar. Figure 8.19: Previewing an R Notebook. When you are ready to publish the document, you can share the .nb.html directly, or render it to a publication format by knitting the document to the desired format. Figure 8.20: Knitting an R Notebook. 8.4.4 Additional Resources Learn more about R Notebook at RStudio’s tutorial page. 8.4.5 Knowledge check Create a new notebook using File &gt; New File &gt; R Notebook. Read the instructions. Practice running the chunks. Verify that you can modify the code, re-run it, and see modified output. Compare and contrast the R notebook you just created with the R markdown file you created in the previous knowledge check. How are the outputs similar? How are they different? How are the inputs similar? How are they different? What happens if you copy the YAML header from one to the other? 8.5 Exercises If you have not already done so create an R project for this course so that all future scripts, inputs, and outputs are organized. Make sure the following RStudio preference settings are set: General: Set “Save workspace to .RData on exit: Never”. Code: In the display tab check the “Show margin” option and set “Margin Column: 80”. Code &gt;&gt; Diagnostics: Make sure the “Provide R style diagnostics” is checked. Create a new R Markdown document and include code and text from some of the activities we covered in lesson 3 of module 1. Now knit this document using each of the three built-in formats: HTML, PDF and Word. How does the output differ? You may need to install LaTeX in order to build the PDF output — RStudio will prompt you if this is necessary. 8.6 Computing environment At the end of of any notebook you make, you should include information about the computing environment including the version numbers of all packages you use. We can do that with the following. sessioninfo() will provide information on the operating system, version of R along with any specified packages. Here, we specify pkgs = 'attached' which means it will only list those packages that have been attached to the R search path with library(pkg_name). An alternative is to explicitly pass a vector of package names used. For this book you’ll find the computing environment consolidated into one notebook at the end of the book. sessioninfo::session_info(pkgs = &#39;attached&#39;) ## ─ Session info ──────────────────────────────────────────────────────────────────────────────────────────────────────── ## setting value ## version R version 4.2.0 (2022-04-22) ## os macOS Monterey 12.4 ## system x86_64, darwin17.0 ## ui RStudio ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/New_York ## date 2022-06-14 ## rstudio 2022.02.3+492 Prairie Trillium (desktop) ## pandoc 2.18 @ /usr/local/bin/ (via rmarkdown) ## ## ─ Packages ──────────────────────────────────────────────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## completejourney * 1.1.0 2019-09-28 [1] CRAN (R 4.2.0) ## dbplyr * 2.1.1 2021-04-06 [1] CRAN (R 4.2.0) ## dplyr * 1.0.9 2022-04-28 [1] CRAN (R 4.2.0) ## forcats * 0.5.1 2021-01-27 [1] CRAN (R 4.2.0) ## ggplot2 * 3.3.6 2022-05-03 [1] CRAN (R 4.2.0) ## ggrepel * 0.9.1 2021-01-15 [1] CRAN (R 4.2.0) ## gridExtra * 2.3 2017-09-09 [1] CRAN (R 4.2.0) ## here * 1.0.1 2020-12-13 [1] CRAN (R 4.2.0) ## jsonlite * 1.8.0 2022-02-22 [1] CRAN (R 4.2.0) ## lubridate * 1.8.0 2021-10-07 [1] CRAN (R 4.2.0) ## magrittr * 2.0.3 2022-03-30 [1] CRAN (R 4.2.0) ## naniar * 0.6.1 2021-05-14 [1] CRAN (R 4.2.0) ## nycflights13 * 1.0.2 2021-04-12 [1] CRAN (R 4.2.0) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.2.0) ## readr * 2.1.2 2022-01-30 [1] CRAN (R 4.2.0) ## readxl * 1.4.0 2022-03-28 [1] CRAN (R 4.2.0) ## RSQLite * 2.2.14 2022-05-07 [1] CRAN (R 4.2.0) ## stringr * 1.4.0 2019-02-10 [1] CRAN (R 4.2.0) ## tibble * 3.1.7 2022-05-03 [1] CRAN (R 4.2.0) ## tidyr * 1.2.0 2022-02-01 [1] CRAN (R 4.2.0) ## tidyverse * 1.3.1 2021-04-15 [1] CRAN (R 4.2.0) ## ## [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library ## ## ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── "],["lesson-2b-data-types-structures.html", "9 Lesson 2b: Data types &amp; structures 9.1 Learning objectives 9.2 Data types 9.3 Data structures 9.4 Exercises", " 9 Lesson 2b: Data types &amp; structures Until now, you’ve created fairly simple data in R and stored it as a vector! However, when wrangling data we often come across a variety of data types and require different data structures to manage them. This lesson serves to introduce you to the basic data types and structures in R that you’ll most commonly use. 9.1 Learning objectives Upon completing this module you will be able to: Explain the benefits of an R project and new ones. Explain the similarities and differences between R Markdown files and R Notebooks. Create both R Markdown and R Notebook deliverables. 9.2 Data types R has six basic types of data: numeric, integer, logical, character, complex and raw. However, it is very unlikely that in your time as a data analyst/scientist you’ll need to work with complex and raw data types so we’ll focus on the first four. Numeric data are numbers that contain a decimal. Actually they can also be whole numbers but we’ll gloss over that. Integers are whole numbers (those numbers without a decimal point). Logical data take on the value of either TRUE or FALSE. There’s also another special type of logical called NA to represent missing values. Character data are used to represent string values. You can think of character strings as something like a word (or multiple words). A special type of character string is a factor, which is a string but with additional attributes (like levels or an order). We’ll cover factors later. 9.2.1 Determining the type R is (usually) able to automatically distinguish between different classes of data by their nature and the context in which they’re used although you should bear in mind that R can’t actually read your mind and you may have to explicitly tell R how you want to treat a data type. You can find out the type (or class) of any object using the class() function. num &lt;- 2.2 class(num) ## [1] &quot;numeric&quot; char &lt;- &quot;hello world&quot; class(char) ## [1] &quot;character&quot; logi &lt;- TRUE class(logi) ## [1] &quot;logical&quot; Alternatively, you can ask if an object is a specific class using a logical test. The is.xxxx() family of functions will return either a TRUE or a FALSE. is.numeric(num) ## [1] TRUE is.character(num) ## [1] FALSE is.character(char) ## [1] TRUE is.logical(logi) ## [1] TRUE 9.2.2 Type conversion It can sometimes be useful to be able to change the class of a variable using the as.xxxx() family of coercion functions, although you need to be careful when doing this as you might receive some unexpected results (see what happens below when we try to convert a character string to a numeric). # coerce numeric to character class(num) ## [1] &quot;numeric&quot; num_char &lt;- as.character(num) num_char ## [1] &quot;2.2&quot; class(num_char) ## [1] &quot;character&quot; # coerce character to numeric! class(char) ## [1] &quot;character&quot; char_num &lt;- as.numeric(char) ## Warning: NAs introduced by coercion Functions to test and and coerce data types. Type Logical test Coercing Character is.character as.character Numeric is.numeric as.numeric Logical is.logical as.logical Factor is.factor as.factor Complex is.complex as.complex In later modules we will learn how to wrangle these different data types plus other special data types that are built on top of these classes (i.e. date-time stamps, missing values). For now, I just want you to understand the foundational data types built into R. 9.2.3 Knowledge check Check out the built-in object pi. What class is this object? What happens when you coerce this object to a character? What happens when you coerce it to a logical? Is there a coercion function that could convert this to an integer? What happens when you do this? 9.3 Data structures Now that you’ve been introduced to some of the most important classes of data in R, let’s have a look at some of main structures that we have for storing these data. 9.3.1 Scalars and vectors Perhaps the simplest type of data structure is the vector. You’ve already been introduced to vectors in module 1. Vectors that have a single value (length 1) are often referred to as scalars. Vectors can contain numbers, characters, factors or logicals, but the key thing to remember is that all the elements inside a vector must be of the same class. In other words, vectors can contain either numbers, characters or logicals but not mixtures of these types of data. There is one important exception to this, you can include NA (this is special type of logical) to denote missing data in vectors with other data types. All the elements inside a vector must be of the same class Figure 9.1: Scalars versus vectors. In R, the only difference is the number of elements. 9.3.2 Matrices and arrays Another useful data structure used in many disciplines such as population ecology, theoretical and applied statistics is the matrix. A matrix is simply a vector that has additional attributes called dimensions. Arrays are just multidimensional matrices. Again, matrices and arrays must contain elements all of the same data class. Figure 9.2: Matrices versus arrays. 9.3.2.1 Creating A convenient way to create a matrix or an array is to use the matrix() and array() functions respectively. Below, we will create a matrix from a sequence 1 to 16 in four rows (nrow = 4) and fill the matrix row-wise (byrow = TRUE) rather than the default column-wise. When using the array() function we define the dimensions using the dim = argument, in our case 2 rows, 4 columns in 2 different matrices. my_mat &lt;- matrix(1:16, nrow = 4, byrow = TRUE) my_mat ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 ## [4,] 13 14 15 16 my_array &lt;- array(1:16, dim = c(2, 4, 2)) my_array ## , , 1 ## ## [,1] [,2] [,3] [,4] ## [1,] 1 3 5 7 ## [2,] 2 4 6 8 ## ## , , 2 ## ## [,1] [,2] [,3] [,4] ## [1,] 9 11 13 15 ## [2,] 10 12 14 16 Sometimes it’s also useful to define row and column names for your matrix but this is not a requirement. To do this use the rownames() and colnames() functions. rownames(my_mat) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) colnames(my_mat) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) my_mat ## a b c d ## A 1 2 3 4 ## B 5 6 7 8 ## C 9 10 11 12 ## D 13 14 15 16 9.3.2.2 Indexing Similar to vectors, we can extract elements from our matrix using [] notation. The main difference is we now have to specify two dimensions in our indexing matrix[row, col]: # element located at the intersection of the # third row and second column my_mat[3, 2] ## [1] 10 We can also leave one dimension empty if we want to retrieve all elements for that particular dimension. # all elements in the second row my_mat[2, ] ## a b c d ## 5 6 7 8 # all elements in the third column my_mat[, 3] ## A B C D ## 3 7 11 15 And when rows and columns are named we can also index based on those names: # Element located in row &#39;A&#39; and column &#39;b&#39; my_mat[&#39;A&#39;, &#39;b&#39;] ## [1] 2 9.3.2.3 Operators Once you’ve created your matrices you can do useful stuff with them and as you’d expect. Many of the functions we used in the vector lesson can be applied across an entire matrix: # mean of all elements mean(my_mat) ## [1] 8.5 However, there are also unique functions that work on matrices but not vectors. For example, we can compute the mean of each column in a matrix: colMeans(my_mat) ## a b c d ## 7 8 9 10 R has numerous built in functions to perform matrix operations. Some of the most common are given below. For example, to transpose a matrix we use the transposition function t(): my_mat_t &lt;- t(my_mat) my_mat_t ## A B C D ## a 1 5 9 13 ## b 2 6 10 14 ## c 3 7 11 15 ## d 4 8 12 16 To extract the diagonal elements of a matrix and store them as a vector we can use the diag() function: my_mat_diag &lt;- diag(my_mat) my_mat_diag ## [1] 1 6 11 16 The usual matrix addition, multiplication etc can be performed. Note the use of the %*% operator to perform matrix multiplication. mat.1 &lt;- matrix(c(2, 0, 1, 1), nrow = 2) mat.1 ## [,1] [,2] ## [1,] 2 1 ## [2,] 0 1 mat.2 &lt;- matrix(c(1, 1, 0, 2), nrow = 2) mat.2 ## [,1] [,2] ## [1,] 1 0 ## [2,] 1 2 mat.1 + mat.2 # matrix addition ## [,1] [,2] ## [1,] 3 1 ## [2,] 1 3 mat.1 * mat.2 # element by element products ## [,1] [,2] ## [1,] 2 0 ## [2,] 0 2 mat.1 %*% mat.2 # matrix multiplication ## [,1] [,2] ## [1,] 3 2 ## [2,] 1 2 9.3.2.4 Knowledge check Check out the built-in VADeaths data matrix? Subset this matrix for only male death rates. Subset for males death rates over the age of 60. Calculate averages for each column and row. 9.3.3 Lists The next data structure we will quickly take a look at is a list. Whilst vectors and matrices are constrained to contain data of the same type, lists are able to store mixtures of data types. In fact we can even store other data structures such as vectors and arrays within a list or even have a list of a list. Lists a very flexible data structures which is ideal for storing irregular or non-rectangular data. Many statistical outputs are provided as a list as well; therefore, its critical to understand how to work with lists. 9.3.3.1 Creating To create a list we can use the list() function. Note how each of the three list elements are of different classes (character, logical, and numeric) and are of different lengths. list_1 &lt;- list(c(&quot;black&quot;, &quot;yellow&quot;, &quot;orange&quot;), c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE), matrix(1:6, nrow = 3)) list_1 ## [[1]] ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; ## ## [[2]] ## [1] TRUE TRUE FALSE TRUE FALSE FALSE ## ## [[3]] ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 Elements of the list can be named during the construction of the list list_2 &lt;- list(colours = c(&quot;black&quot;, &quot;yellow&quot;, &quot;orange&quot;), evaluation = c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE), time = matrix(1:6, nrow = 3)) list_2 ## $colours ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; ## ## $evaluation ## [1] TRUE TRUE FALSE TRUE FALSE FALSE ## ## $time ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 or after the list has been created using the names() function names(list_1) &lt;- c(&quot;colors&quot;, &quot;evaluation&quot;, &quot;time&quot;) list_1 ## $colors ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; ## ## $evaluation ## [1] TRUE TRUE FALSE TRUE FALSE FALSE ## ## $time ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 We can always get a quick glimpse of the structure of a list using str(): str(list_1) ## List of 3 ## $ colors : chr [1:3] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; ## $ evaluation: logi [1:6] TRUE TRUE FALSE TRUE FALSE FALSE ## $ time : int [1:3, 1:2] 1 2 3 4 5 6 9.3.3.2 Indexing To subset lists we can utilize the single bracket [ ], double brackets [[ ]], and dollar sign $ operators. Each approach provides a specific purpose and can be combined in different ways to achieve the following subsetting objectives: Subset list and preserve output as a list Subset list and simplify output Subset list to get elements out of a list To extract one or more list items while preserving the output in list format use the [ ] operator. Its important to understand the difference between simplifying and preserving subsetting. Simplifying subsets returns the simplest possible data structure that can represent the output. Preserving subsets keeps the structure of the output the same as the input. See Hadley Wickham’s section on Simplifying vs. Preserving Subsetting to learn more. # extract first list item list_1[1] ## $colors ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; # same as above but using the item&#39;s name list_1[&#39;colors&#39;] ## $colors ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; # extract multiple list items list_1[c(&#39;colors&#39;, &#39;time&#39;)] ## $colors ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; ## ## $time ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 To extract one or more list items while simplifying the output use the [[ ]] or $ operator: # extract first list item list_1[[1]] ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; # same as above but using the item&#39;s name list_1[[&#39;colors&#39;]] ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; # same as above but using $ list_1$colors ## [1] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; One thing that differentiates the [[ operator from the \\(&lt;/code&gt; is that the &lt;code&gt;[[&lt;/code&gt; operator can be used with computed indices. The &lt;code&gt;\\) operator can only be used with literal names. To extract individual elements out of a specific list item combine the [[ (or $) operator with the [ operator: # extract the third element of the first list item list_1[[&#39;colors&#39;]][3] ## [1] &quot;orange&quot; 9.3.3.3 Operators There are less operators that you typically use directly on a list. Most of the time you are trying to extract items out of a list. However, a few useful functions that are applied to a list include: # how many items are in a list length(list_1) ## [1] 3 # the name of the list items names(list_1) ## [1] &quot;colors&quot; &quot;evaluation&quot; &quot;time&quot; # the overall structure of a list str(list_1) ## List of 3 ## $ colors : chr [1:3] &quot;black&quot; &quot;yellow&quot; &quot;orange&quot; ## $ evaluation: logi [1:6] TRUE TRUE FALSE TRUE FALSE FALSE ## $ time : int [1:3, 1:2] 1 2 3 4 5 6 9.3.3.4 Knowledge check Install and load the nycflights13 package: install.packages(&#39;nycflights13&#39;) Using the flights data provided by this package create the following regression model: This line of code is performing a linear regression model and saving the results in a list called flight_lm. We’ll discuss linear regression and modeling in later modules. flight_lm &lt;- lm(arr_delay ~ dep_delay + month + carrier, data = flights) How many items are in this list? What are the names of these list items? Extract the coefficients of this model. Extract the departure delay (dep_delay) coefficient. 9.3.4 Data frames By far the most commonly used data structure to store data is the data frame. A data frame is a powerful two-dimensional object made up of rows and columns which looks superficially very similar to a matrix. However, whilst matrices are restricted to containing data all of the same type, data frames can contain a mixture of different types of data. Typically, in a data frame each row corresponds to an individual observation and each column corresponds to a different measured or recorded variable. This setup may be familiar to those of you who use Microsoft Excel to manage and store your data. Perhaps a useful way to think about data frames is that they are essentially made up of a bunch of vectors (columns) with each vector containing its own data type but the data type can be different between vectors. As an example, the data frame below contains total quantity and sales for a grocery product category (i.e. potatoes, popcorn, frozen pizza) for each household. The data frame has four variables (columns) and each row represents an individual household. The variables household_id, total_quantity, and total_sales are numeric, product_category is a character, and multiple_items is a Boolean representing if the household bought more than one item. Table 9.1: Sample data frame household_id product_category total_quantity total_sales multiple_items 1129 CONDIMENTS/SAUCES 1 0.89 FALSE 1136 NEWSPAPER 1 1.50 FALSE 1450 FD WRAPS/BAGS/TRSH BG 1 2.49 FALSE 2334 MAGAZINE 3 10.47 TRUE 2341 SOFT DRINKS 5 5.30 TRUE 2406 COLD CEREAL 1 2.99 FALSE 420 FLUID MILK PRODUCTS 1 1.00 FALSE 493 SALD DRSNG/SNDWCH SPRD 1 2.50 FALSE 788 BABY HBC 1 1.37 FALSE 842 SOFT DRINKS 11 14.66 TRUE There are a couple of important things to bear in mind about data frames. These types of objects are known as rectangular data as each column must have the same number of observations. Also, any missing data should be recorded as an NA just as we did with our vectors. 9.3.4.1 Creating Data frames are usually created by reading in a data set, which we’ll cover in a later lesson. However, data frames can also be created explicitly with the data.frame() function or they can be coerced from other types of objects like lists. In this case I’ll create a simple data frame df and assess its basic structure: df &lt;- data.frame(col1 = 1:3, col2 = c(&quot;this&quot;, &quot;is&quot;, &quot;text&quot;), col3 = c(TRUE, FALSE, TRUE), col4 = c(2.5, 4.2, pi)) # assess the structure of a data frame str(df) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ col1: int 1 2 3 ## $ col2: chr &quot;this&quot; &quot;is&quot; &quot;text&quot; ## $ col3: logi TRUE FALSE TRUE ## $ col4: num 2.5 4.2 3.14 # number of rows nrow(df) ## [1] 3 # number of columns ncol(df) ## [1] 4 Note how col2 in df was converted to a column of factors. This is because there is a default setting in data.frame() that converts character columns to factors. We can turn this off by setting the stringsAsFactors = FALSE argument: df &lt;- data.frame(col1 = 1:3, col2 = c(&quot;this&quot;, &quot;is&quot;, &quot;text&quot;), col3 = c(TRUE, FALSE, TRUE), col4 = c(2.5, 4.2, pi), stringsAsFactors = FALSE) # note how col2 now is of a character class str(df) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ col1: int 1 2 3 ## $ col2: chr &quot;this&quot; &quot;is&quot; &quot;text&quot; ## $ col3: logi TRUE FALSE TRUE ## $ col4: num 2.5 4.2 3.14 We can also convert pre-existing structures to a data frame. The following illustrates how we can turn multiple vectors into a data frame: v1 &lt;- 1:3 v2 &lt;-c(&quot;this&quot;, &quot;is&quot;, &quot;text&quot;) v3 &lt;- c(TRUE, FALSE, TRUE) # convert same length vectors to a data frame using data.frame() data.frame(col1 = v1, col2 = v2, col3 = v3) ## col1 col2 col3 ## 1 1 this TRUE ## 2 2 is FALSE ## 3 3 text TRUE 9.3.4.2 Indexing Data frames possess the characteristics of both lists and matrices: if you index with a single vector, they behave like lists and will return the selected columns with all rows; if you subset with two vectors, they behave like matrices and can be subset by row and column: df ## col1 col2 col3 col4 ## 1 1 this TRUE 2.5000 ## 2 2 is FALSE 4.2000 ## 3 3 text TRUE 3.1416 # subsetting by row numbers df[2:3, ] ## col1 col2 col3 col4 ## 2 2 is FALSE 4.2000 ## 3 3 text TRUE 3.1416 # subsetting by row names df[c(&quot;row2&quot;, &quot;row3&quot;), ] ## col1 col2 col3 col4 ## NA NA &lt;NA&gt; NA NA ## NA.1 NA &lt;NA&gt; NA NA # subset for both rows and columns df[1:2, c(1, 3)] ## col1 col3 ## 1 1 TRUE ## 2 2 FALSE You can also subset data frames based on conditional statements. To illustrate we’ll use the built-in mtcars data frame: head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 # all rows where mpg is greater than 20 mtcars[mtcars$mpg &gt; 20, ] ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 Its good to know that we can index and filter data frames in this fashion but later lessons will demonstrate an alternative, and more common approach to wrangle data frames. In fact, most of the lessons that follow are all focused on working with data frames! 9.4 Exercises Check out the built-in mtcars data set. What type of object is this? Apply the head() and summary() functions to mtcars, what do these functions return? Index for just the ‘mpg’ column in mtcars using three different approaches: single brackets ([), double brackets [[, and dollar sign $. How do the results differ? Use one of the methods in #3 to save the ‘mpg’ column as a vector. Now compute the mean of this vector. "],["lesson-2c-importing-data.html", "10 Lesson 2c: Importing data 10.1 Learning objectives 10.2 Data &amp; memory 10.3 Delimited files 10.4 Excel files 10.5 SQL databases 10.6 Many other file types 10.7 Exercises", " 10 Lesson 2c: Importing data The first step to any data analysis process is to get the data. Data can come from many sources but two of the most common include delimited and Excel files. This section covers how to import data from these common files; plus we cover other important topics such as understanding file paths, connecting to SQL databases, and to load data from saved R object files. 10.1 Learning objectives Upon completing this module you will be able to: Describe how imported data affects computer memory. Import tabular data with R. Assess some basic attributes of your imported data. Import alternative data files such as SQL tables and Rdata files. 10.2 Data &amp; memory R stores its data in memory - this makes it relatively quickly accessible but can cause size limitations in certain fields. In this class we will mainly work with small to moderate data sets, which means we should not run into any space limitations. R does provide tooling that allows you to work with big data via distributed data (i.e. sparklyr) and relational databrases (i.e. SQL). R memory is session-specific, so quitting R (i.e. shutting down RStudio) removes the data from memory. A general way to conceptualize data import into and use within R: Data sits in on the computer/server - this is frequently called “disk” R code can be used to copy a data file from disk to the R session’s memory R data then sits within R’s memory ready to be used by other R code Here is a visualization of this process: Figure 10.1: Conceptualizing how R imports data. 10.3 Delimited files Text files are a popular way to hold and exchange tabular data as almost any data application supports exporting data to the CSV (or other text file) format. Text file formats use delimiters to separate the different elements in a line, and each line of data is in its own line in the text file. Therefore, importing different kinds of text files can follow a fairly consistent process once you’ve identified the delimiter. There are three main groups of functions that we can use to read in text files: Base R functions *readr** package functions *vroom** package functions Here, we’ll focus on the middle one. All three functions will import a tabular delimited file (.csv, .tsv, .txt, etc.) and convert it to a data frame in R, but each has subtle differences. You can read why we favor readr and vroom over the base R importing functions (i.e. read.csv()) here. We will not cover the vroom package but it is good to know about as it can be extremely fast for very large data sets. Read more about vroom here. The following will import a data set describing the sale of individual residential property in Ames, Iowa from 2006 to 2010 (source). library(readr) ames &lt;- read_csv(&#39;data/ames_raw.csv&#39;) ## Rows: 2930 Columns: 82 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (45): PID, MS SubClass, MS Zoning, Street, Alley, Lot Shape, Land Contour, Utilities, Lot Config, Land Slope, Nei... ## dbl (37): Order, Lot Frontage, Lot Area, Overall Qual, Overall Cond, Year Built, Year Remod/Add, Mas Vnr Area, BsmtFi... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. We can look at it in our notebook or console and we see that it is displayed in a well-organized, concise manner. More on this in a second. ames ## # A tibble: 2,930 × 82 ## Order PID `MS SubClass` `MS Zoning` `Lot Frontage` `Lot Area` Street Alley `Lot Shape` `Land Contour` Utilities ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 0526301100 020 RL 141 31770 Pave &lt;NA&gt; IR1 Lvl AllPub ## 2 2 0526350040 020 RH 80 11622 Pave &lt;NA&gt; Reg Lvl AllPub ## 3 3 0526351010 020 RL 81 14267 Pave &lt;NA&gt; IR1 Lvl AllPub ## 4 4 0526353030 020 RL 93 11160 Pave &lt;NA&gt; Reg Lvl AllPub ## 5 5 0527105010 060 RL 74 13830 Pave &lt;NA&gt; IR1 Lvl AllPub ## 6 6 0527105030 060 RL 78 9978 Pave &lt;NA&gt; IR1 Lvl AllPub ## 7 7 0527127150 120 RL 41 4920 Pave &lt;NA&gt; Reg Lvl AllPub ## 8 8 0527145080 120 RL 43 5005 Pave &lt;NA&gt; IR1 HLS AllPub ## 9 9 0527146030 120 RL 39 5389 Pave &lt;NA&gt; IR1 Lvl AllPub ## 10 10 0527162130 060 RL 60 7500 Pave &lt;NA&gt; Reg Lvl AllPub ## # … with 2,920 more rows, and 71 more variables: `Lot Config` &lt;chr&gt;, `Land Slope` &lt;chr&gt;, Neighborhood &lt;chr&gt;, ## # `Condition 1` &lt;chr&gt;, `Condition 2` &lt;chr&gt;, `Bldg Type` &lt;chr&gt;, `House Style` &lt;chr&gt;, `Overall Qual` &lt;dbl&gt;, ## # `Overall Cond` &lt;dbl&gt;, `Year Built` &lt;dbl&gt;, `Year Remod/Add` &lt;dbl&gt;, `Roof Style` &lt;chr&gt;, `Roof Matl` &lt;chr&gt;, ## # `Exterior 1st` &lt;chr&gt;, `Exterior 2nd` &lt;chr&gt;, `Mas Vnr Type` &lt;chr&gt;, `Mas Vnr Area` &lt;dbl&gt;, `Exter Qual` &lt;chr&gt;, ## # `Exter Cond` &lt;chr&gt;, Foundation &lt;chr&gt;, `Bsmt Qual` &lt;chr&gt;, `Bsmt Cond` &lt;chr&gt;, `Bsmt Exposure` &lt;chr&gt;, ## # `BsmtFin Type 1` &lt;chr&gt;, `BsmtFin SF 1` &lt;dbl&gt;, `BsmtFin Type 2` &lt;chr&gt;, `BsmtFin SF 2` &lt;dbl&gt;, `Bsmt Unf SF` &lt;dbl&gt;, ## # `Total Bsmt SF` &lt;dbl&gt;, Heating &lt;chr&gt;, `Heating QC` &lt;chr&gt;, `Central Air` &lt;chr&gt;, Electrical &lt;chr&gt;, … If we check the class of our object we do see that it is a data.frame; however, we also see some additional information. That’s because this is a special kind of data frame known as a tibble. class(ames) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; 10.3.1 Tibbles Tibbles are data frames, but they tweak some older behaviors of data frames to make life a little easier. There are two main differences in the usage of a tibble vs. a classic data frame: printing and subsetting. Tibbles have a refined print method that shows only the first 10 rows, and all the columns that fit on your screen. This makes it much easier to work with large data. We can see this difference with the following: # printed output of a tibble ames ## # A tibble: 2,930 × 82 ## Order PID `MS SubClass` `MS Zoning` `Lot Frontage` `Lot Area` Street Alley `Lot Shape` `Land Contour` Utilities ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 0526301100 020 RL 141 31770 Pave &lt;NA&gt; IR1 Lvl AllPub ## 2 2 0526350040 020 RH 80 11622 Pave &lt;NA&gt; Reg Lvl AllPub ## 3 3 0526351010 020 RL 81 14267 Pave &lt;NA&gt; IR1 Lvl AllPub ## 4 4 0526353030 020 RL 93 11160 Pave &lt;NA&gt; Reg Lvl AllPub ## 5 5 0527105010 060 RL 74 13830 Pave &lt;NA&gt; IR1 Lvl AllPub ## 6 6 0527105030 060 RL 78 9978 Pave &lt;NA&gt; IR1 Lvl AllPub ## 7 7 0527127150 120 RL 41 4920 Pave &lt;NA&gt; Reg Lvl AllPub ## 8 8 0527145080 120 RL 43 5005 Pave &lt;NA&gt; IR1 HLS AllPub ## 9 9 0527146030 120 RL 39 5389 Pave &lt;NA&gt; IR1 Lvl AllPub ## 10 10 0527162130 060 RL 60 7500 Pave &lt;NA&gt; Reg Lvl AllPub ## # … with 2,920 more rows, and 71 more variables: `Lot Config` &lt;chr&gt;, `Land Slope` &lt;chr&gt;, Neighborhood &lt;chr&gt;, ## # `Condition 1` &lt;chr&gt;, `Condition 2` &lt;chr&gt;, `Bldg Type` &lt;chr&gt;, `House Style` &lt;chr&gt;, `Overall Qual` &lt;dbl&gt;, ## # `Overall Cond` &lt;dbl&gt;, `Year Built` &lt;dbl&gt;, `Year Remod/Add` &lt;dbl&gt;, `Roof Style` &lt;chr&gt;, `Roof Matl` &lt;chr&gt;, ## # `Exterior 1st` &lt;chr&gt;, `Exterior 2nd` &lt;chr&gt;, `Mas Vnr Type` &lt;chr&gt;, `Mas Vnr Area` &lt;dbl&gt;, `Exter Qual` &lt;chr&gt;, ## # `Exter Cond` &lt;chr&gt;, Foundation &lt;chr&gt;, `Bsmt Qual` &lt;chr&gt;, `Bsmt Cond` &lt;chr&gt;, `Bsmt Exposure` &lt;chr&gt;, ## # `BsmtFin Type 1` &lt;chr&gt;, `BsmtFin SF 1` &lt;dbl&gt;, `BsmtFin Type 2` &lt;chr&gt;, `BsmtFin SF 2` &lt;dbl&gt;, `Bsmt Unf SF` &lt;dbl&gt;, ## # `Total Bsmt SF` &lt;dbl&gt;, Heating &lt;chr&gt;, `Heating QC` &lt;chr&gt;, `Central Air` &lt;chr&gt;, Electrical &lt;chr&gt;, … # printed output of a regular data frame as.data.frame(ames) ## Order PID MS SubClass MS Zoning Lot Frontage Lot Area Street Alley Lot Shape Land Contour Utilities Lot Config ## 1 1 0526301100 020 RL 141 31770 Pave &lt;NA&gt; IR1 Lvl AllPub Corner ## 2 2 0526350040 020 RH 80 11622 Pave &lt;NA&gt; Reg Lvl AllPub Inside ## 3 3 0526351010 020 RL 81 14267 Pave &lt;NA&gt; IR1 Lvl AllPub Corner ## 4 4 0526353030 020 RL 93 11160 Pave &lt;NA&gt; Reg Lvl AllPub Corner ## 5 5 0527105010 060 RL 74 13830 Pave &lt;NA&gt; IR1 Lvl AllPub Inside ## 6 6 0527105030 060 RL 78 9978 Pave &lt;NA&gt; IR1 Lvl AllPub Inside ## 7 7 0527127150 120 RL 41 4920 Pave &lt;NA&gt; Reg Lvl AllPub Inside ## 8 8 0527145080 120 RL 43 5005 Pave &lt;NA&gt; IR1 HLS AllPub Inside ## 9 9 0527146030 120 RL 39 5389 Pave &lt;NA&gt; IR1 Lvl AllPub Inside ## 10 10 0527162130 060 RL 60 7500 Pave &lt;NA&gt; Reg Lvl AllPub Inside ## 11 11 0527163010 060 RL 75 10000 Pave &lt;NA&gt; IR1 Lvl AllPub Corner ## 12 12 0527165230 020 RL NA 7980 Pave &lt;NA&gt; IR1 Lvl AllPub Inside ## Land Slope Neighborhood Condition 1 Condition 2 Bldg Type House Style Overall Qual Overall Cond Year Built ## 1 Gtl NAmes Norm Norm 1Fam 1Story 6 5 1960 ## 2 Gtl NAmes Feedr Norm 1Fam 1Story 5 6 1961 ## 3 Gtl NAmes Norm Norm 1Fam 1Story 6 6 1958 ## 4 Gtl NAmes Norm Norm 1Fam 1Story 7 5 1968 ## 5 Gtl Gilbert Norm Norm 1Fam 2Story 5 5 1997 ## 6 Gtl Gilbert Norm Norm 1Fam 2Story 6 6 1998 ## 7 Gtl StoneBr Norm Norm TwnhsE 1Story 8 5 2001 ## 8 Gtl StoneBr Norm Norm TwnhsE 1Story 8 5 1992 ## 9 Gtl StoneBr Norm Norm TwnhsE 1Story 8 5 1995 ## 10 Gtl Gilbert Norm Norm 1Fam 2Story 7 5 1999 ## 11 Gtl Gilbert Norm Norm 1Fam 2Story 6 5 1993 ## 12 Gtl Gilbert Norm Norm 1Fam 1Story 6 7 1992 ## Year Remod/Add Roof Style Roof Matl Exterior 1st Exterior 2nd Mas Vnr Type Mas Vnr Area Exter Qual Exter Cond ## 1 1960 Hip CompShg BrkFace Plywood Stone 112 TA TA ## 2 1961 Gable CompShg VinylSd VinylSd None 0 TA TA ## 3 1958 Hip CompShg Wd Sdng Wd Sdng BrkFace 108 TA TA ## 4 1968 Hip CompShg BrkFace BrkFace None 0 Gd TA ## 5 1998 Gable CompShg VinylSd VinylSd None 0 TA TA ## 6 1998 Gable CompShg VinylSd VinylSd BrkFace 20 TA TA ## 7 2001 Gable CompShg CemntBd CmentBd None 0 Gd TA ## 8 1992 Gable CompShg HdBoard HdBoard None 0 Gd TA ## 9 1996 Gable CompShg CemntBd CmentBd None 0 Gd TA ## 10 1999 Gable CompShg VinylSd VinylSd None 0 TA TA ## 11 1994 Gable CompShg HdBoard HdBoard None 0 TA TA ## 12 2007 Gable CompShg HdBoard HdBoard None 0 TA Gd ## Foundation Bsmt Qual Bsmt Cond Bsmt Exposure BsmtFin Type 1 BsmtFin SF 1 BsmtFin Type 2 BsmtFin SF 2 Bsmt Unf SF ## 1 CBlock TA Gd Gd BLQ 639 Unf 0 441 ## 2 CBlock TA TA No Rec 468 LwQ 144 270 ## 3 CBlock TA TA No ALQ 923 Unf 0 406 ## 4 CBlock TA TA No ALQ 1065 Unf 0 1045 ## 5 PConc Gd TA No GLQ 791 Unf 0 137 ## 6 PConc TA TA No GLQ 602 Unf 0 324 ## 7 PConc Gd TA Mn GLQ 616 Unf 0 722 ## 8 PConc Gd TA No ALQ 263 Unf 0 1017 ## 9 PConc Gd TA No GLQ 1180 Unf 0 415 ## 10 PConc TA TA No Unf 0 Unf 0 994 ## 11 PConc Gd TA No Unf 0 Unf 0 763 ## 12 PConc Gd TA No ALQ 935 Unf 0 233 ## Total Bsmt SF Heating Heating QC Central Air Electrical 1st Flr SF 2nd Flr SF Low Qual Fin SF Gr Liv Area ## 1 1080 GasA Fa Y SBrkr 1656 0 0 1656 ## 2 882 GasA TA Y SBrkr 896 0 0 896 ## 3 1329 GasA TA Y SBrkr 1329 0 0 1329 ## 4 2110 GasA Ex Y SBrkr 2110 0 0 2110 ## 5 928 GasA Gd Y SBrkr 928 701 0 1629 ## 6 926 GasA Ex Y SBrkr 926 678 0 1604 ## 7 1338 GasA Ex Y SBrkr 1338 0 0 1338 ## 8 1280 GasA Ex Y SBrkr 1280 0 0 1280 ## 9 1595 GasA Ex Y SBrkr 1616 0 0 1616 ## 10 994 GasA Gd Y SBrkr 1028 776 0 1804 ## 11 763 GasA Gd Y SBrkr 763 892 0 1655 ## 12 1168 GasA Ex Y SBrkr 1187 0 0 1187 ## Bsmt Full Bath Bsmt Half Bath Full Bath Half Bath Bedroom AbvGr Kitchen AbvGr Kitchen Qual TotRms AbvGrd Functional ## 1 1 0 1 0 3 1 TA 7 Typ ## 2 0 0 1 0 2 1 TA 5 Typ ## 3 0 0 1 1 3 1 Gd 6 Typ ## 4 1 0 2 1 3 1 Ex 8 Typ ## 5 0 0 2 1 3 1 TA 6 Typ ## 6 0 0 2 1 3 1 Gd 7 Typ ## 7 1 0 2 0 2 1 Gd 6 Typ ## 8 0 0 2 0 2 1 Gd 5 Typ ## 9 1 0 2 0 2 1 Gd 5 Typ ## 10 0 0 2 1 3 1 Gd 7 Typ ## 11 0 0 2 1 3 1 TA 7 Typ ## 12 1 0 2 0 3 1 TA 6 Typ ## Fireplaces Fireplace Qu Garage Type Garage Yr Blt Garage Finish Garage Cars Garage Area Garage Qual Garage Cond ## 1 2 Gd Attchd 1960 Fin 2 528 TA TA ## 2 0 &lt;NA&gt; Attchd 1961 Unf 1 730 TA TA ## 3 0 &lt;NA&gt; Attchd 1958 Unf 1 312 TA TA ## 4 2 TA Attchd 1968 Fin 2 522 TA TA ## 5 1 TA Attchd 1997 Fin 2 482 TA TA ## 6 1 Gd Attchd 1998 Fin 2 470 TA TA ## 7 0 &lt;NA&gt; Attchd 2001 Fin 2 582 TA TA ## 8 0 &lt;NA&gt; Attchd 1992 RFn 2 506 TA TA ## 9 1 TA Attchd 1995 RFn 2 608 TA TA ## 10 1 TA Attchd 1999 Fin 2 442 TA TA ## 11 1 TA Attchd 1993 Fin 2 440 TA TA ## 12 0 &lt;NA&gt; Attchd 1992 Fin 2 420 TA TA ## Paved Drive Wood Deck SF Open Porch SF Enclosed Porch 3Ssn Porch Screen Porch Pool Area Pool QC Fence Misc Feature ## 1 P 210 62 0 0 0 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 Y 140 0 0 0 120 0 &lt;NA&gt; MnPrv &lt;NA&gt; ## 3 Y 393 36 0 0 0 0 &lt;NA&gt; &lt;NA&gt; Gar2 ## 4 Y 0 0 0 0 0 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 Y 212 34 0 0 0 0 &lt;NA&gt; MnPrv &lt;NA&gt; ## 6 Y 360 36 0 0 0 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 Y 0 0 170 0 0 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 Y 0 82 0 0 144 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 Y 237 152 0 0 0 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 Y 140 60 0 0 0 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 11 Y 157 84 0 0 0 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 12 Y 483 21 0 0 0 0 &lt;NA&gt; GdPrv Shed ## Misc Val Mo Sold Yr Sold Sale Type Sale Condition SalePrice ## 1 0 5 2010 WD Normal 215000 ## 2 0 6 2010 WD Normal 105000 ## 3 12500 6 2010 WD Normal 172000 ## 4 0 4 2010 WD Normal 244000 ## 5 0 3 2010 WD Normal 189900 ## 6 0 6 2010 WD Normal 195500 ## 7 0 4 2010 WD Normal 213500 ## 8 0 1 2010 WD Normal 191500 ## 9 0 3 2010 WD Normal 236500 ## 10 0 6 2010 WD Normal 189000 ## 11 0 4 2010 WD Normal 175900 ## 12 500 3 2010 WD Normal 185000 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 2918 rows ] The differences in the subsetting are not important at this time but the main takeaway is that tibbles are more strict and will behave more consistently then data frames for certain subsetting tasks. Read more about tibbles here. 10.3.2 File paths This is a good time to have a discussion on file paths. It’s important to understand where files exist on your computer and how to reference those paths. There are two main approaches: Absolute paths Relative paths An absolute path always contains the root elements and the complete list of directories to locate the specific file or folder. For the ames_raw.csv file, the absolute path on my computer is: library(here) absolute_path &lt;- here(&#39;data/ames_raw.csv&#39;) absolute_path ## [1] &quot;/Users/b294776/Desktop/workspace/training/UC/uc-bana-7025/data/ames_raw.csv&quot; I can always use the absolute path in read_csv(): ames &lt;- read_csv(absolute_path) ## Rows: 2930 Columns: 82 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (45): PID, MS SubClass, MS Zoning, Street, Alley, Lot Shape, Land Contour, Utilities, Lot Config, Land Slope, Nei... ## dbl (37): Order, Lot Frontage, Lot Area, Overall Qual, Overall Cond, Year Built, Year Remod/Add, Mas Vnr Area, BsmtFi... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. In contrast, a relative path is a path built starting from the current location. For example, say that I am operating in a directory called “Project A”. If I’m working in “my_notebook.Rmd” and I have a “my_data.csv” file in that same directory: # illustration of the directory layout Project A ├── my_notebook.Rmd └── my_data.csv Then I can use this relative path to import this file: read_csv('my_data.csv'). This just means to look for the ‘my_data.csv’ file relative to the current directory that I am in. Often, people store data in a “data” directory. If this directory is a subdirectory within my Project A directory: # illustration of the directory layout Project A ├── my_notebook.Rmd └── data └── my_data.csv Then I can use this relative path to import this file: read_csv('data/my_data.csv'). This just means to look for the ‘data’ subdirectory relative to the current directory that I am in and then look for the ‘my_data.csv’ file. Sometimes, the data directory may not be in the current directory. Sometimes a project directory will look like the following where there is a subdirectory containing multiple notebooks and then another subdirectory containing data assets. If you are working in “notebook1.Rmd” within the notebooks subdirectory, you will need to tell R to go up one directory relative to the notebook you are working in to the main Project A directory and then go down into the data directory. # illustration of the directory layout Project A ├── notebooks │ ├── notebook1.Rmd │ ├── notebook2.Rmd │ └── notebook3.Rmd └── data └── my_data.csv I can do this by using dot-notation in my relative path specification - here I use ‘..’ to imply “go up one directory relative to my current location”: read_csv('../data/my_data.csv'). Note that the path specified in pd.read_csv() does not need to be a local path. For example, the ames_raw.csv data is located online at https://raw.githubusercontent.com/bradleyboehmke/uc-bana-7025/main/data/ames_raw.csv. We can use read_csv() to import directly from this location: url &lt;- &#39;https://raw.githubusercontent.com/bradleyboehmke/uc-bana-7025/main/data/ames_raw.csv&#39; ames &lt;- read_csv(url) ## Rows: 2930 Columns: 82 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (45): PID, MS SubClass, MS Zoning, Street, Alley, Lot Shape, Land Contour, Utilities, Lot Config, Land Slope, Nei... ## dbl (37): Order, Lot Frontage, Lot Area, Overall Qual, Overall Cond, Year Built, Year Remod/Add, Mas Vnr Area, BsmtFi... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 10.3.3 Metadata Once we’ve imported the data we can get some descriptive metadata about our data frame. For example, we can get the dimensions of our data frame. Here, we see that we have 2,930 rows and 82 columns. dim(ames) ## [1] 2930 82 We can also see all the names of our columns: names(ames) ## [1] &quot;Order&quot; &quot;PID&quot; &quot;MS SubClass&quot; &quot;MS Zoning&quot; &quot;Lot Frontage&quot; &quot;Lot Area&quot; ## [7] &quot;Street&quot; &quot;Alley&quot; &quot;Lot Shape&quot; &quot;Land Contour&quot; &quot;Utilities&quot; &quot;Lot Config&quot; ## [13] &quot;Land Slope&quot; &quot;Neighborhood&quot; &quot;Condition 1&quot; &quot;Condition 2&quot; &quot;Bldg Type&quot; &quot;House Style&quot; ## [19] &quot;Overall Qual&quot; &quot;Overall Cond&quot; &quot;Year Built&quot; &quot;Year Remod/Add&quot; &quot;Roof Style&quot; &quot;Roof Matl&quot; ## [25] &quot;Exterior 1st&quot; &quot;Exterior 2nd&quot; &quot;Mas Vnr Type&quot; &quot;Mas Vnr Area&quot; &quot;Exter Qual&quot; &quot;Exter Cond&quot; ## [31] &quot;Foundation&quot; &quot;Bsmt Qual&quot; &quot;Bsmt Cond&quot; &quot;Bsmt Exposure&quot; &quot;BsmtFin Type 1&quot; &quot;BsmtFin SF 1&quot; ## [37] &quot;BsmtFin Type 2&quot; &quot;BsmtFin SF 2&quot; &quot;Bsmt Unf SF&quot; &quot;Total Bsmt SF&quot; &quot;Heating&quot; &quot;Heating QC&quot; ## [43] &quot;Central Air&quot; &quot;Electrical&quot; &quot;1st Flr SF&quot; &quot;2nd Flr SF&quot; &quot;Low Qual Fin SF&quot; &quot;Gr Liv Area&quot; ## [49] &quot;Bsmt Full Bath&quot; &quot;Bsmt Half Bath&quot; &quot;Full Bath&quot; &quot;Half Bath&quot; &quot;Bedroom AbvGr&quot; &quot;Kitchen AbvGr&quot; ## [55] &quot;Kitchen Qual&quot; &quot;TotRms AbvGrd&quot; &quot;Functional&quot; &quot;Fireplaces&quot; &quot;Fireplace Qu&quot; &quot;Garage Type&quot; ## [61] &quot;Garage Yr Blt&quot; &quot;Garage Finish&quot; &quot;Garage Cars&quot; &quot;Garage Area&quot; &quot;Garage Qual&quot; &quot;Garage Cond&quot; ## [67] &quot;Paved Drive&quot; &quot;Wood Deck SF&quot; &quot;Open Porch SF&quot; &quot;Enclosed Porch&quot; &quot;3Ssn Porch&quot; &quot;Screen Porch&quot; ## [73] &quot;Pool Area&quot; &quot;Pool QC&quot; &quot;Fence&quot; &quot;Misc Feature&quot; &quot;Misc Val&quot; &quot;Mo Sold&quot; ## [79] &quot;Yr Sold&quot; &quot;Sale Type&quot; &quot;Sale Condition&quot; &quot;SalePrice&quot; You may have also noticed the message each time we read in the data set that identified the delimiter and it also showed the following, which states that when we read in the data, 45 variables were read in as character strings and 37 were read in as double floating points. chr (45): PID, MS SubClass, MS Zoning, Street, Alley, Lot Shape, Land Contour, Utilities,... dbl (37): Order, Lot Frontage, Lot Area, Overall Qual, Overall Cond, Year Built, Year Rem... There was also a message that stated “use spec() to retrieve the full column specification for this data.” When we do so we see that it lists all the columns and the data type that were read in as. spec(ames) ## cols( ## Order = col_double(), ## PID = col_character(), ## `MS SubClass` = col_character(), ## `MS Zoning` = col_character(), ## `Lot Frontage` = col_double(), ## `Lot Area` = col_double(), ## Street = col_character(), ## Alley = col_character(), ## `Lot Shape` = col_character(), ## `Land Contour` = col_character(), ## Utilities = col_character(), ## `Lot Config` = col_character(), ## `Land Slope` = col_character(), ## Neighborhood = col_character(), ## `Condition 1` = col_character(), ## `Condition 2` = col_character(), ## `Bldg Type` = col_character(), ## `House Style` = col_character(), ## `Overall Qual` = col_double(), ## `Overall Cond` = col_double(), ## `Year Built` = col_double(), ## `Year Remod/Add` = col_double(), ## `Roof Style` = col_character(), ## `Roof Matl` = col_character(), ## `Exterior 1st` = col_character(), ## `Exterior 2nd` = col_character(), ## `Mas Vnr Type` = col_character(), ## `Mas Vnr Area` = col_double(), ## `Exter Qual` = col_character(), ## `Exter Cond` = col_character(), ## Foundation = col_character(), ## `Bsmt Qual` = col_character(), ## `Bsmt Cond` = col_character(), ## `Bsmt Exposure` = col_character(), ## `BsmtFin Type 1` = col_character(), ## `BsmtFin SF 1` = col_double(), ## `BsmtFin Type 2` = col_character(), ## `BsmtFin SF 2` = col_double(), ## `Bsmt Unf SF` = col_double(), ## `Total Bsmt SF` = col_double(), ## Heating = col_character(), ## `Heating QC` = col_character(), ## `Central Air` = col_character(), ## Electrical = col_character(), ## `1st Flr SF` = col_double(), ## `2nd Flr SF` = col_double(), ## `Low Qual Fin SF` = col_double(), ## `Gr Liv Area` = col_double(), ## `Bsmt Full Bath` = col_double(), ## `Bsmt Half Bath` = col_double(), ## `Full Bath` = col_double(), ## `Half Bath` = col_double(), ## `Bedroom AbvGr` = col_double(), ## `Kitchen AbvGr` = col_double(), ## `Kitchen Qual` = col_character(), ## `TotRms AbvGrd` = col_double(), ## Functional = col_character(), ## Fireplaces = col_double(), ## `Fireplace Qu` = col_character(), ## `Garage Type` = col_character(), ## `Garage Yr Blt` = col_double(), ## `Garage Finish` = col_character(), ## `Garage Cars` = col_double(), ## `Garage Area` = col_double(), ## `Garage Qual` = col_character(), ## `Garage Cond` = col_character(), ## `Paved Drive` = col_character(), ## `Wood Deck SF` = col_double(), ## `Open Porch SF` = col_double(), ## `Enclosed Porch` = col_double(), ## `3Ssn Porch` = col_double(), ## `Screen Porch` = col_double(), ## `Pool Area` = col_double(), ## `Pool QC` = col_character(), ## Fence = col_character(), ## `Misc Feature` = col_character(), ## `Misc Val` = col_double(), ## `Mo Sold` = col_double(), ## `Yr Sold` = col_double(), ## `Sale Type` = col_character(), ## `Sale Condition` = col_character(), ## SalePrice = col_double() ## ) Lastly, its always good to understand if, and how many, missing values are in the data set we can do this easily by running the following, which shows 13,997 elements are missing. That’s a lot! sum(is.na(ames)) ## [1] 13997 We can even apply some operators and indexing procedures we learned in previous lessons to quickly view all columns with missing values and get a total sum of the missing values within those columns. In future modules we’ll learn different ways we can handle these missing values. missing_values &lt;- colSums(is.na(ames)) sort(missing_values[missing_values &gt; 0], decreasing = TRUE) ## Pool QC Misc Feature Alley Fence Fireplace Qu Lot Frontage Garage Yr Blt Garage Finish ## 2917 2824 2732 2358 1422 490 159 159 ## Garage Qual Garage Cond Garage Type Bsmt Exposure BsmtFin Type 2 Bsmt Qual Bsmt Cond BsmtFin Type 1 ## 159 159 157 83 81 80 80 80 ## Mas Vnr Type Mas Vnr Area Bsmt Full Bath Bsmt Half Bath BsmtFin SF 1 BsmtFin SF 2 Bsmt Unf SF Total Bsmt SF ## 23 23 2 2 1 1 1 1 ## Electrical Garage Cars Garage Area ## 1 1 1 10.3.4 Knowledge check Check out the help documentation for read_csv() by executing ?read_csv. What parameter in read_csv() allows us to specify values that represent missing values? Read in this energy_consumption.csv file. What are the dimensions of this data? What data type is each column? Apply dplyr::glimpse(ames). What information does this provide? 10.4 Excel files With Excel still being the spreadsheet software of choice its important to be able to efficiently import and export data from these files. Often, many users will simply resort to exporting the Excel file as a CSV file and then import into R using readxl::read_excel; however, this is far from efficient. This section will teach you how to eliminate the CSV step and to import data directly from Excel using the readxl package. To illustrate, we’ll import so mock grocery store products data located in a products.xlsx file. To read in Excel data you will use the excel_sheets() functions. This allows you to read the names of the different worksheets in the Excel workbook and identify the specific worksheet of interest and then specify that in read_excel. library(readxl) excel_sheets(&#39;data/products.xlsx&#39;) ## [1] &quot;metadata&quot; &quot;products data&quot; &quot;grocery list&quot; If you don’t explicitly specify a sheet then the first worksheet will be imported. products &lt;- read_excel(&#39;data/products.xlsx&#39;, sheet = &#39;products data&#39;) products ## # A tibble: 151,141 × 5 ## product_num department commodity brand_ty x5 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 92993 NON-FOOD PET PRIVATE N ## 2 93924 NON-FOOD PET PRIVATE N ## 3 94272 NON-FOOD PET PRIVATE N ## 4 94299 NON-FOOD PET PRIVATE N ## 5 94594 NON-FOOD PET PRIVATE N ## 6 94606 NON-FOOD PET PRIVATE N ## 7 94613 NON-FOOD PET PRIVATE N ## 8 95625 NON-FOOD PET PRIVATE N ## 9 96152 NON-FOOD PET PRIVATE N ## 10 96153 NON-FOOD PET PRIVATE N ## # … with 151,131 more rows 10.5 SQL databases Many organizations continue to use relational databases along with SQL to interact with these data assets. If you are unfamiliar with relational databases and SQL then this is a quick read that explains the benefits of these tools. R can connect to almost any existing database type. Most common database types have R packages that allow you to connect to them (e.g., RSQLite, RMySQL, etc). Furthermore, the DBI and dbplyr packages support connecting to the widely-used open source databases sqlite, mysql and postgresql, as well as Google’s bigquery, and it can also be extended to other database types . RStudio has created a website that provides documentation and best practices to work on database interfaces. In this example I will illustrate connecting to a local sqlite database. First, let’s load the libraries we’ll need: library(dbplyr) library(dplyr) library(RSQLite) The following illustrates with the example Chinook Database, which I’ve downloaded to my data directory. The following uses 2 functions that talk to the SQLite database. dbconnect() comes from the DBI package and is not something that you’ll use directly as a user. It simply allows R to send commands to databases irrespective of the database management system used. The SQLite() function from the RSQLite package allows R to interface with SQLite databases. chinook &lt;- dbConnect(SQLite(), &quot;data/chinook.db&quot;) This command does not load the data into the R session (as the read_csv() function did). Instead, it merely instructs R to connect to the SQLite database contained in the chinook.db file. Using a similar approach, you could connect to many other database management systems that are supported by R including MySQL, PostgreSQL, BigQuery, etc. Let’s take a closer look at the chinook database we just connected to: src_dbi(chinook) ## src: sqlite 3.38.5 [/Users/b294776/Desktop/workspace/training/UC/uc-bana-7025/data/chinook.db] ## tbls: albums, artists, customers, employees, genres, invoice_items, invoices, media_types, playlist_track, playlists, ## sqlite_sequence, sqlite_stat1, tracks Just like a spreadsheet with multiple worksheets, a SQLite database can contain multiple tables. In this case there are several tables listed in the tbls row in the output above: albums artists customers etc. Once you’ve made the connection, you can use tbl() to read in the “tracks” table directly as a data frame. tracks &lt;- tbl(chinook, &#39;tracks&#39;) If you are familiar with SQL then you can even pass a SQL query directly in the tbl() call using dbplyr’s sql(). For example, the following SQL query: SELECTS the name, composer, and milliseconds columns, FROM the tracks table, WHERE observations in the milliseconds column are greater than 200,000 and WHERE observations in the composer column are not missing (NULL) sql_query &lt;- &quot;SELECT name, composer, milliseconds FROM tracks WHERE milliseconds &gt; 200000 and composer is not null&quot; long_tracks &lt;- tbl(chinook, sql(sql_query)) long_tracks ## # Source: SQL [?? x 3] ## # Database: sqlite 3.38.5 [/Users/b294776/Desktop/workspace/training/UC/uc-bana-7025/data/chinook.db] ## name composer milliseconds ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 For Those About To Rock (We Salute You) Angus Young, Malcolm Young, Brian Johnson 343719 ## 2 Fast As a Shark F. Baltes, S. Kaufman, U. Dirkscneider &amp; W. Hoffman 230619 ## 3 Restless and Wild F. Baltes, R.A. Smith-Diesel, S. Kaufman, U. Dirkscneider &amp; W. H… 252051 ## 4 Princess of the Dawn Deaffy &amp; R.A. Smith-Diesel 375418 ## 5 Put The Finger On You Angus Young, Malcolm Young, Brian Johnson 205662 ## 6 Let&#39;s Get It Up Angus Young, Malcolm Young, Brian Johnson 233926 ## 7 Inject The Venom Angus Young, Malcolm Young, Brian Johnson 210834 ## 8 Snowballed Angus Young, Malcolm Young, Brian Johnson 203102 ## 9 Evil Walks Angus Young, Malcolm Young, Brian Johnson 263497 ## 10 Breaking The Rules Angus Young, Malcolm Young, Brian Johnson 263288 ## # … with more rows 10.6 Many other file types There are many other file types that you may encounter in your career. Most of which we can import into R one way or another. For example the haven package reads SPSS, Stata, and SAS files, xml2 allows you to read in XML, and rvest helps to scrape data from HTML web pages. Two non-tabular file types you may experience in practice are JSON files and R object files. 10.6.1 JSON files JSON files are non-tabular data files that are popular in data engineering due to their space efficiency and flexibility. Here is an example JSON file: { &quot;planeId&quot;: &quot;1xc2345g&quot;, &quot;manufacturerDetails&quot;: { &quot;manufacturer&quot;: &quot;Airbus&quot;, &quot;model&quot;: &quot;A330&quot;, &quot;year&quot;: 1999 }, &quot;airlineDetails&quot;: { &quot;currentAirline&quot;: &quot;Southwest&quot;, &quot;previousAirlines&quot;: { &quot;1st&quot;: &quot;Delta&quot; }, &quot;lastPurchased&quot;: 2013 }, &quot;numberOfFlights&quot;: 4654 } JSON Files can be imported using the jsonlite library. library(jsonlite) imported_json &lt;- fromJSON(&#39;data/json_example.json&#39;) Since JSON files can have multiple dimensions, jsonlite will import JSON files as a list since that is the most flexible data structure in R. class(imported_json) ## [1] &quot;list&quot; And we can view the data: imported_json ## $planeId ## [1] &quot;1xc2345g&quot; ## ## $manufacturerDetails ## $manufacturerDetails$manufacturer ## [1] &quot;Airbus&quot; ## ## $manufacturerDetails$model ## [1] &quot;A330&quot; ## ## $manufacturerDetails$year ## [1] 1999 ## ## ## $airlineDetails ## $airlineDetails$currentAirline ## [1] &quot;Southwest&quot; ## ## $airlineDetails$previousAirlines ## $airlineDetails$previousAirlines$`1st` ## [1] &quot;Delta&quot; ## ## ## $airlineDetails$lastPurchased ## [1] 2013 ## ## ## $numberOfFlights ## [1] 4654 10.6.2 R object files Sometimes you may need to save data or other R objects outside of your workspace. You may want to share R data/objects with co-workers, transfer between projects or computers, or simply archive them. There are three primary ways that people tend to save R data/objects: as .RData, .rda, or as .rds files. You can read about the differences of these R objects here. If you have an .RData or .rda file you need to load you can do so with the following. You will not seen any output in your console or script because this simply loads the data objects within xy.RData into your global environment. load(&#39;data/xy.RData&#39;) For .rds files you can use readr’s read_rds() function: read_rds(&#39;data/x.rds&#39;) ## [1] 0.758310 0.999693 0.314913 0.978646 0.168522 0.597689 0.209016 0.848492 0.073632 0.106206 0.116106 0.696880 0.725726 ## [14] 0.011556 0.052708 0.472079 0.072446 0.786448 0.924837 0.228298 10.7 Exercises R stores its data in _______ . What happens to R’s data when the R or RStudio session is terminated? Load the hearts.csv data file into R using the readr library. What are the dimensions of this data? What data types are the variables in this data set? Use the head() and tail() functions to assess the first and last 15 rows of this data set. Now import the hearts_data_dictionary.csv file, which provides some information on each variable. Do the data types of the hearts.csv variables align with the description of each variable? "],["lab-1.html", "11 Lab", " 11 Lab TBD "],["overview-2.html", "12 Overview 12.1 Learning objectives 12.2 Tasks 12.3 Course readings", " 12 Overview In the last module we discussed general guidelines for first interacting with a new data set. In module 3 we want to build on those activities by learning how to clean and tidy our data, and then beginning our journey to creating insights with data through data manipulation. 12.1 Learning objectives By the end of this module you should be able to: Explain the basic concepts of “tidy” data. Perform data tidying tasks with R such as reshaping, splitting, and combining data along with handling missing values. Manipulate your data by applying filters, selecting and renaming columns, creating new variables, and more. Compute descriptive statistics across all observations and within different grouped levels of observations. 12.2 Tasks TBD 12.3 Course readings TBD "],["lesson-3a-pipe-operator.html", "13 Lesson 3a: Pipe operator 13.1 Learning objectives 13.2 Pipe (%&gt;%) operator 13.3 Additional pipe operators (optional) 13.4 Additional resources 13.5 Exercises", " 13 Lesson 3a: Pipe operator Removing duplication is an important principle to keep in mind with your code; however, equally important is to keep your code efficient and readable. Efficiency is often accomplished by leveraging functions and iteration in your code (which we cover later in this class). However, efficiency also includes eliminating the creation and saving of unnecessary objects that often result when you are trying to make your code more readable, clear, and explicit. Consequently, writing code that is simple, readable, and efficient is often considered contradictory. For this reason, the magrittr package is a powerful tool to have in your data wrangling toolkit. The magrittr package was created by Stefan Milton Bache and, in Stefan’s words, has two primary aims: “to decrease development time and to improve readability and maintainability of code.” Hence, it aims to increase efficiency and improve readability; and in the process it greatly simplifies your code. This lesson covers the basics of the magrittr toolkit. The primary function in the magrittr package is the pipe operator (%&gt;%), this operator has been incorporated in many packages and you will see it used often throughout this class. 13.1 Learning objectives Upon completing this module you will be able to: Explain the different approaches commonly used to chain multiple expressions together. Understand how the pipe (%&gt;%) operator works along with some alternative pipe operators. 13.2 Pipe (%&gt;%) operator The principal function provided by the magrittr package is %&gt;%, or what’s called the “pipe” operator. This operator will forward a value, or the result of an expression, into the next function call/expression. For instance a function to filter data can be written as: filter(data, variable == numeric_value) or data %&gt;% filter(variable == numeric_value) Both functions complete the same task and the benefit of using %&gt;% may not be immediately evident; however, when you desire to perform multiple functions its advantage becomes obvious. For instance, if we want to filter some data, group it by categories, summarize it, and then order the summarized results we could write it out three different ways. Don’t worry about the details of this code, this is mainly for illustration purposes. You will learn all about these functions in later lessons! Nested Option: # provides the various arrange, summarize, etc functions library(dplyr) # perform nested functions arrange( summarize( group_by( filter(df, quantity &gt; 1), store_id ), avg_sales = mean(sales_value) ), desc(avg_sales) ) ## # A tibble: 188 × 2 ## store_id avg_sales ## &lt;chr&gt; &lt;dbl&gt; ## 1 736 86.0 ## 2 197 43.5 ## 3 901 43.5 ## 4 2903 43 ## 5 476 32.8 ## 6 634 25.7 ## 7 3149 24.9 ## 8 3479 19.6 ## 9 656 19.1 ## 10 3131 15 ## # … with 178 more rows This first option is considered a “nested” option such the functions are nested within one another. Historically, this has been the traditional way of integrating code; however, it becomes extremely difficult to read what exactly the code is doing and it also becomes easier to make mistakes when making updates to your code. Although not in violation of the DRY principle6, it definitely violates the basic principle of readability and clarity, which makes communication of your analysis more difficult. To make things more readable, people often move to the following approach… Multiple Object Option: a &lt;- filter(df, quantity &gt; 1) b &lt;- group_by(a, store_id) c &lt;- summarise(b, avg_sales = mean(sales_value)) d &lt;- arrange(c, desc(avg_sales)) print(d) ## # A tibble: 188 × 2 ## store_id avg_sales ## &lt;chr&gt; &lt;dbl&gt; ## 1 736 86.0 ## 2 197 43.5 ## 3 901 43.5 ## 4 2903 43 ## 5 476 32.8 ## 6 634 25.7 ## 7 3149 24.9 ## 8 3479 19.6 ## 9 656 19.1 ## 10 3131 15 ## # … with 178 more rows This second option helps in making the data wrangling steps more explicit and obvious but definitely violates the DRY principle. By sequencing multiple functions in this way you are likely saving multiple outputs that are not very informative to you or others; rather, the only reason you save them is to insert them into the next function to eventually get the final output you desire. This inevitably creates unnecessary copies and wrecks havoc on properly managing your objects…basically it results in a global environment charlie foxtrot! To provide the same readability (or even better), we can use %&gt;% to string these arguments together without unnecessary object creation… %&gt;% Option: df %&gt;% filter(quantity &gt; 1) %&gt;% group_by(store_id) %&gt;% summarise(avg_sales = mean(sales_value)) %&gt;% arrange(desc(avg_sales)) ## # A tibble: 188 × 2 ## store_id avg_sales ## &lt;chr&gt; &lt;dbl&gt; ## 1 736 86.0 ## 2 197 43.5 ## 3 901 43.5 ## 4 2903 43 ## 5 476 32.8 ## 6 634 25.7 ## 7 3149 24.9 ## 8 3479 19.6 ## 9 656 19.1 ## 10 3131 15 ## # … with 178 more rows This final option which integrates %&gt;% operators makes for more efficient and legible code. Its efficient in that it doesn’t save unnecessary objects (as in option 2) and performs as effectively (as both option 1 &amp; 2) but makes your code more readable in the process. Its legible in that you can read this as you would read normal prose (we read the %&gt;% as “and then”): “take df and then filter and then group by and then summarize and then arrange.” Notice how above we didn’t have to load the magrittr package to use the pipe operator (%&gt;%)? This is because the pipe operator has been incorporated into the dplyr package and since we loaded that package we have direct access to %&gt;%. In fact, all tidyverse packages have incorporated the pipe operator. And since R is a functional programming language, meaning that everything you do is basically built on functions, you can use the pipe operator to feed into just about any argument call. For example, we can pipe into a linear regression function and then get the summary of the regression parameters. Note in this case I insert “data = .” into the lm() function. When using the %&gt;% operator the default is the argument that you are forwarding will go in as the first argument of the function that follows the %&gt;%. However, in some functions the argument you are forwarding does not go into the default first position. In these cases, you place “.” to signal which argument you want the forwarded expression to go to. df %&gt;% filter(store_id == &quot;367&quot;) %&gt;% lm(sales_value ~ week + retail_disc, data = .) %&gt;% summary() ## ## Call: ## lm(formula = sales_value ~ week + retail_disc, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.62 -2.19 -1.16 0.34 47.42 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.10845 0.20549 15.13 &lt;2e-16 *** ## week 0.00185 0.00641 0.29 0.77 ## retail_disc 1.09089 0.10396 10.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.5 on 2126 degrees of freedom ## Multiple R-squared: 0.0493, Adjusted R-squared: 0.0484 ## F-statistic: 55.1 on 2 and 2126 DF, p-value: &lt;2e-16 You can also use %&gt;% to feed into plots: You will learn more about plotting techniques in week 5. # visualization package library(ggplot2) df %&gt;% filter(store_id == &quot;367&quot;, week &lt;= 10) %&gt;% ggplot(aes(x = factor(week), y = sales_value)) + geom_jitter(width = .05, alpha = .4) + geom_boxplot(alpha = .1) 13.3 Additional pipe operators (optional) You should only review this section after you have a firm grasp on how to perform basic data transformation and tidying procedures. Consequently, this is a good section to come back to later in the week. Don’t worry, you will not be quizzed on any of the content that follows! magrittr also offers some alternative pipe operators. Some functions, such as plotting functions, will cause the string of piped arguments to terminate. The tee (%T&gt;%) operator allows you to continue piping functions that normally cause termination. # normal piping terminates with the plot() function resulting in # NULL results for the summary() function df %&gt;% filter(store_id == &quot;367&quot;) %&gt;% select(quantity, sales_value) %&gt;% plot() %&gt;% summary() ## Length Class Mode ## 0 NULL NULL # load magrittr to use additional pipe operators library(magrittr) # inserting %T&gt;% allows you to plot and perform the functions that # follow the plotting function df %&gt;% filter(store_id == &quot;367&quot;) %&gt;% select(quantity, sales_value) %T&gt;% plot() %&gt;% summary() ## quantity sales_value ## Min. : 0 Min. : 0.00 ## 1st Qu.: 1 1st Qu.: 1.49 ## Median : 1 Median : 2.50 ## Mean : 166 Mean : 3.62 ## 3rd Qu.: 1 3rd Qu.: 3.99 ## Max. :23735 Max. :53.14 The compound assignment %&lt;&gt;% operator is used to update a value by first piping it into one or more expressions, and then assigning the result. For instance, let’s say you want to transform the sales_value variable to a logarithmic measurement. Using %&lt;&gt;% will perform the functions to the right of %&lt;&gt;% and save the changes these functions perform to the variable or data frame called to the left of %&lt;&gt;%. # note that sales_value is in its typical measurement head(df$sales_value) ## [1] 3.86 1.59 1.00 11.87 1.29 2.50 # we can log transform sales_value and save this change using %&lt;&gt;% df$sales_value %&lt;&gt;% log head(df$sales_value) ## [1] 1.35067 0.46373 0.00000 2.47401 0.25464 0.91629 You should be cautious in your use of %&lt;&gt;% since it does not change the name of the variable and you are overwriting the original variable’s values. Some functions (e.g. lm, aggregate, cor) have a data argument, which allows the direct use of names inside the data as part of the call. The exposition (%$%) operator is useful when you want to pipe a data frame, which may contain many columns, into a function that is only applied to some of the columns. For example, the correlation (cor) function only requires an x and y argument so if you pipe the customer transaction data into the cor function using %&gt;% you will get an error because cor doesn’t know how to handle df. However, using %$% allows you to say “take this data frame and then perform cor() on these specified columns within df.” # regular piping results in an error df %&gt;% filter(store_id == &quot;367&quot;) %&gt;% cor(retail_disc, quantity) ## Error in pmatch(use, c(&quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;pairwise.complete.obs&quot;, : object &#39;quantity&#39; not found # using %$% allows you to specify variables of interest df %&gt;% filter(store_id == &quot;367&quot;) %$% cor(retail_disc, quantity) ## [1] 0.10263 13.4 Additional resources The magrittr package and its pipe operators are a great tool for making your code simple, efficient, and readable. There are limitations, or at least suggestions, on when and how you should use the operators. Garrett Grolemund and Hadley Wickham offer some advice on the proper use of pipe operators in their R for Data Science book. However, the %&gt;% has greatly transformed our ability to write “simplified” code in R. As the pipe gains in popularity you will likely find it in more future packages and being familiar will likely result in better communication of your code. Some additional resources regarding magrittr and the pipe operators you may find useful: The magrittr vignette (vignette(\"magrittr\")) in your console) provides additional examples of using pipe operators and functions provided by magrittr. A blog post by Stefan Milton Bache regarding the past, present and future of magrittr magrittr questions on Stack Overflow The ensurer package, also written by Stefan Milton Bache, provides a useful way of verifying and validating data outputs in a sequence of pipe operators. 13.5 Exercises Look at the code that follows. How would you rewrite this code using the pipe operator? foo_foo_1 &lt;- hop(foo_foo, through = forest) foo_foo_2 &lt;- scoop(foo_foo_1, up = field_mice) foo_foo_3 &lt;- bop(foo_foo_2, on = head) Don’t repeat yourself (DRY) is a software development principle aimed at reducing repetition. Formulated by Andy Hunt and Dave Thomas in their book The Pragmatic Programmer, the DRY principle states that “every piece of knowledge must have a single, unambiguous, authoritative representation within a system.” This principle has been widely adopted to imply that you should not duplicate code. Although the principle was meant to be far grander than that, there’s plenty of merit behind this slight misinterpretation.↩︎ "],["lesson-3b-data-transformation.html", "14 Lesson 3b: Data transformation 14.1 Learning objectives 14.2 Prerequisites 14.3 Filtering observations 14.4 Selecting variables 14.5 Computing summary statistics 14.6 Sorting observations 14.7 Creating new variables 14.8 Putting it altogether 14.9 Exercises 14.10 Additional resources", " 14 Lesson 3b: Data transformation When wrangling data you often need to create some new variables and filter for certain observations of interest, or maybe you just want to rename the variables or reorder the observations in order to make the data a little easier to work with. You’ll learn how to do all that (and more!) in this lesson, which will teach you how to transform your data using the dplyr package 14.1 Learning objectives Upon completing this module you will be able to: Filter a data frame for observations of interest. Select and/or rename specific variables. Compute summary statistics. Sort observations. Create new variables. 14.2 Prerequisites Load the dplyr package to provide you access to the functions we’ll cover in this lesson. library(dplyr) Alternatively, you could load the tidyverse package, which automatically loads the dplyr package. To illustrate various transformation tasks we will use the following customer transaction data from the completejourney package: library(completejourney) (df &lt;- transactions_sample) ## # A tibble: 75,000 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc coupon_match_disc week ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2261 309 31625220889 940996 1 3.86 0.43 0 0 5 ## 2 2131 368 32053127496 873902 1 1.59 0.9 0 0 10 ## 3 511 316 32445856036 847901 1 1 0.69 0 0 13 ## 4 400 388 31932241118 13094913 2 11.9 2.9 0 0 8 ## 5 918 340 32074655895 1085604 1 1.29 0 0 0 10 ## 6 718 324 32614612029 883203 1 2.5 0.49 0 0 15 ## 7 868 323 32074722463 9884484 1 3.49 0 0 0 10 ## 8 1688 450 34850403304 1028715 1 2 1.79 0 0 33 ## 9 467 31782 31280745102 896613 2 6.55 4.44 0 0 2 ## 10 1947 32004 32744181707 978497 1 3.99 0 0 0 16 ## # … with 74,990 more rows, and 1 more variable: transaction_timestamp &lt;dttm&gt; 14.3 Filtering observations Filtering data is a common task to identify and select observations in which a particular variable matches a specific value or condition. The filter() function provides this capability. The first argument is the name of the data frame. The second and subsequent arguments are the expressions that filter the data frame. For example, if we want to filter for only transactions at the store with ID 309: filter(df, store_id == &quot;309&quot;) ## # A tibble: 426 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc coupon_match_disc week ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2261 309 31625220889 940996 1 3.86 0.43 0 0 5 ## 2 519 309 32931585175 1029743 1 2.69 0 0 0 18 ## 3 1073 309 40532931248 1058997 1 1.89 0 0 0 44 ## 4 517 309 32989806742 5590158 1 1 0.9 0 0 19 ## 5 1023 309 35713891195 15596520 1 4.59 0.4 0 0 37 ## 6 1770 309 32505226867 12263600 1 1.5 0.29 0 0 14 ## 7 1257 309 33983222000 999090 1 3.49 0 0 0 28 ## 8 1509 309 32090471323 1016800 2 6.67 2.51 0 0 10 ## 9 1167 309 34103947117 1037417 1 3.65 0 0 0 29 ## 10 1836 309 40532932017 879008 1 6.99 1 0 0 44 ## # … with 416 more rows, and 1 more variable: transaction_timestamp &lt;dttm&gt; You can pipe data into the filter function using the following syntax: df %&gt;% filter(store_id == “309”) When you run that line of code, dplyr executes the filtering operation and returns a new data frame. dplyr functions never modify their inputs, so if you want to save the result, you’ll need to use the assignment operator, &lt;-: store_309 &lt;- filter(df, store_id == &quot;309&quot;) store_309 ## # A tibble: 426 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc coupon_match_disc week ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2261 309 31625220889 940996 1 3.86 0.43 0 0 5 ## 2 519 309 32931585175 1029743 1 2.69 0 0 0 18 ## 3 1073 309 40532931248 1058997 1 1.89 0 0 0 44 ## 4 517 309 32989806742 5590158 1 1 0.9 0 0 19 ## 5 1023 309 35713891195 15596520 1 4.59 0.4 0 0 37 ## 6 1770 309 32505226867 12263600 1 1.5 0.29 0 0 14 ## 7 1257 309 33983222000 999090 1 3.49 0 0 0 28 ## 8 1509 309 32090471323 1016800 2 6.67 2.51 0 0 10 ## 9 1167 309 34103947117 1037417 1 3.65 0 0 0 29 ## 10 1836 309 40532932017 879008 1 6.99 1 0 0 44 ## # … with 416 more rows, and 1 more variable: transaction_timestamp &lt;dttm&gt; To use filtering effectively, you have to know how to select the observations that you want using comparison operators. R provides the standard suite of comparison operators to include: Comparison operators. Operator Description &lt; less than &gt; greater than == equal to != not equal to &lt;= less than or equal to &gt;= greater than or equal to %in% group membership is.na is NA !is.na is not NA We can apply several of these comparison operators in the filter function using logical operators. The primary logical operators you will use are: Boolean logical operators. Operator Example Description &amp; x &amp; y intersection of x and y | x \\| y union of x or y ! x &amp; !y x but exclude y and intersect of y xor xor(x, y) only values in x and y that are disjointed with one another The following visual representation helps to differentiate how these operators work with fictional x and y data: Figure 14.1: Complete set of boolean operations. x is the left-hand circle, y is the right-hand circle, and the shaded region show which parts each operator selects. R4DS For example, we can filter for transactions for a particular store ID or household ID: filter(df, store_id == &quot;309&quot; | household_id == &quot;1762&quot;) Or if we wanted to filter for transactions for a particular store ID and household ID: filter(df, store_id == &quot;309&quot; &amp; household_id == &quot;1762&quot;) A comma between comparison operators acts just like and &amp; operator. So filter(df, store_id == “309”, household_id == “1762”) is the same as filter(df, store_id == “309” &amp; household_id == “1762”) We can continue to add additional operations. In this example we filter for transactions made: at a particular store ID, by a particular household, who purchased more than 4 of a certain product or the product cost more than $10. df %&gt;% filter( store_id == &quot;309&quot;, household_id == &quot;1167&quot;, quantity &gt; 4 | sales_value &gt; 10 ) ## # A tibble: 3 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc coupon_match_disc week ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1167 309 34642351799 8249262 1 15.0 0 0 0 32 ## 2 1167 309 31993100768 941036 1 14.9 0 0 0 9 ## 3 1167 309 40341135791 1051093 6 3 1.74 0 0 42 ## # … with 1 more variable: transaction_timestamp &lt;dttm&gt; A useful shortcut for writing multiple or statements is to use %in%. For example, the following code finds all transactions made at store 309 or 400. filter(df, store_id == &quot;309&quot; | store_id == &quot;400&quot;) ## # A tibble: 1,203 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc coupon_match_disc week ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2261 309 31625220889 940996 1 3.86 0.43 0 0 5 ## 2 1113 400 34204801923 998119 1 3.19 0.11 0 0 30 ## 3 519 309 32931585175 1029743 1 2.69 0 0 0 18 ## 4 1946 400 41352204380 1120213 3 5 0 0 0 52 ## 5 709 400 40911786843 923559 1 1 0.39 0 0 49 ## 6 725 400 33106940650 1000664 1 0.34 0.15 0 0 20 ## 7 1662 400 34258865155 1114465 2 2.08 0.7 0 0 31 ## 8 1662 400 32065200596 1085939 1 2.19 0 0 0 10 ## 9 1957 400 34338650191 933835 1 0.6 0 0 0 31 ## 10 1925 400 34133653463 13072776 1 1.99 0 0 0 29 ## # … with 1,193 more rows, and 1 more variable: transaction_timestamp &lt;dttm&gt; The %in% operator allows us to select every row where x is one of the values in y. We could use it to rewrite the code above as: filter(df, store_id %in% c(&quot;309&quot;, &quot;400&quot;)) ## # A tibble: 1,203 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc coupon_match_disc week ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2261 309 31625220889 940996 1 3.86 0.43 0 0 5 ## 2 1113 400 34204801923 998119 1 3.19 0.11 0 0 30 ## 3 519 309 32931585175 1029743 1 2.69 0 0 0 18 ## 4 1946 400 41352204380 1120213 3 5 0 0 0 52 ## 5 709 400 40911786843 923559 1 1 0.39 0 0 49 ## 6 725 400 33106940650 1000664 1 0.34 0.15 0 0 20 ## 7 1662 400 34258865155 1114465 2 2.08 0.7 0 0 31 ## 8 1662 400 32065200596 1085939 1 2.19 0 0 0 10 ## 9 1957 400 34338650191 933835 1 0.6 0 0 0 31 ## 10 1925 400 34133653463 13072776 1 1.99 0 0 0 29 ## # … with 1,193 more rows, and 1 more variable: transaction_timestamp &lt;dttm&gt; There are additional filtering and subsetting functions that are quite useful: # remove duplicate rows df %&gt;% distinct() # random sample, 50% sample size without replacement df %&gt;% sample_frac(size = 0.5, replace = FALSE) # random sample of 10 rows with replacement df %&gt;% sample_n(size = 10, replace = TRUE) # select rows 3-5 df %&gt;% slice(3:5) # select top n entries - in this case ranks variable net_spend_amt and selects # the rows with the top 5 values df %&gt;% top_n(n = 5, wt = sales_value) 14.3.1 Knowledge check Using the completejourney sample transactions data as we did above… Filter for transactions with greater than 2 units. Filter for transactions with greater than 2 units during week 25 that occurred at store 441. Filter for transactions with greater than 2 units during week 25 that occurred at store 343 or 441. Filter for transactions with greater than 2 units during week 25 that occurred at store 343 or 441 but excludes household 253. 14.4 Selecting variables It’s not uncommon for us to use data sets with hundreds of variables. In this case, the first challenge is often narrowing in on the variables you’re actually interested in. select allows you to rapidly zoom in on a useful subset using operations based on the names of the variables. select is not terribly useful with our transaction data because we only have 8 variables, but you can still get the general idea: # Select columns by name select(df, household_id, store_id, sales_value) ## # A tibble: 75,000 × 3 ## household_id store_id sales_value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2261 309 3.86 ## 2 2131 368 1.59 ## 3 511 316 1 ## 4 400 388 11.9 ## 5 918 340 1.29 ## 6 718 324 2.5 ## 7 868 323 3.49 ## 8 1688 450 2 ## 9 467 31782 6.55 ## 10 1947 32004 3.99 ## # … with 74,990 more rows # Select all columns between household_id and sales_value (inclusive) select(df, household_id:sales_value) ## # A tibble: 75,000 × 6 ## household_id store_id basket_id product_id quantity sales_value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2261 309 31625220889 940996 1 3.86 ## 2 2131 368 32053127496 873902 1 1.59 ## 3 511 316 32445856036 847901 1 1 ## 4 400 388 31932241118 13094913 2 11.9 ## 5 918 340 32074655895 1085604 1 1.29 ## 6 718 324 32614612029 883203 1 2.5 ## 7 868 323 32074722463 9884484 1 3.49 ## 8 1688 450 34850403304 1028715 1 2 ## 9 467 31782 31280745102 896613 2 6.55 ## 10 1947 32004 32744181707 978497 1 3.99 ## # … with 74,990 more rows # Select all columns except those between household_id and sales_value select(df, -c(household_id:sales_value)) ## # A tibble: 75,000 × 5 ## retail_disc coupon_disc coupon_match_disc week transaction_timestamp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dttm&gt; ## 1 0.43 0 0 5 2017-01-28 14:06:53 ## 2 0.9 0 0 10 2017-02-28 22:31:57 ## 3 0.69 0 0 13 2017-03-26 13:22:21 ## 4 2.9 0 0 8 2017-02-18 13:13:10 ## 5 0 0 0 10 2017-03-02 15:05:57 ## 6 0.49 0 0 15 2017-04-05 18:14:17 ## 7 0 0 0 10 2017-03-02 17:45:37 ## 8 1.79 0 0 33 2017-08-11 22:41:02 ## 9 4.44 0 0 2 2017-01-06 07:47:01 ## 10 0 0 0 16 2017-04-13 17:30:04 ## # … with 74,990 more rows There are a number of helper functions you can use within select: starts_with(\"abc\"): matches names that begin with “abc”. ends_with(\"xyz\"): matches names that end with “xyz”. contains(\"ijk\"): matches names that contain “ijk”. matches(\"(.)\\\\1\"): selects variables that match a regular expression. This one matches any variables that contain repeated characters. You’ll learn more about regular expressions in the character strings chapter. num_range(\"x\", 1:3): matches x1, x2 and x3. See ?select for more details. For example, we can select all variables that contain “id” in their name: select(df, contains(&quot;id&quot;)) ## # A tibble: 75,000 × 4 ## household_id store_id basket_id product_id ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2261 309 31625220889 940996 ## 2 2131 368 32053127496 873902 ## 3 511 316 32445856036 847901 ## 4 400 388 31932241118 13094913 ## 5 918 340 32074655895 1085604 ## 6 718 324 32614612029 883203 ## 7 868 323 32074722463 9884484 ## 8 1688 450 34850403304 1028715 ## 9 467 31782 31280745102 896613 ## 10 1947 32004 32744181707 978497 ## # … with 74,990 more rows select can be used to rename variables, but it’s rarely useful because it drops all of the variables not explicitly mentioned. Instead, use rename, which is a variant of select that keeps all the variables that aren’t explicitly mentioned: rename(df, store = store_id, product = product_id) ## # A tibble: 75,000 × 11 ## household_id store basket_id product quantity sales_value retail_disc coupon_disc coupon_match_disc week ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2261 309 31625220889 940996 1 3.86 0.43 0 0 5 ## 2 2131 368 32053127496 873902 1 1.59 0.9 0 0 10 ## 3 511 316 32445856036 847901 1 1 0.69 0 0 13 ## 4 400 388 31932241118 13094913 2 11.9 2.9 0 0 8 ## 5 918 340 32074655895 1085604 1 1.29 0 0 0 10 ## 6 718 324 32614612029 883203 1 2.5 0.49 0 0 15 ## 7 868 323 32074722463 9884484 1 3.49 0 0 0 10 ## 8 1688 450 34850403304 1028715 1 2 1.79 0 0 33 ## 9 467 31782 31280745102 896613 2 6.55 4.44 0 0 2 ## 10 1947 32004 32744181707 978497 1 3.99 0 0 0 16 ## # … with 74,990 more rows, and 1 more variable: transaction_timestamp &lt;dttm&gt; Another option is to use select in conjunction with the everything helper. This is useful if you have a handful of variables you’d like to move to the start of the data frame. select(df, household_id, quantity, sales_value, everything()) ## # A tibble: 75,000 × 11 ## household_id quantity sales_value store_id basket_id product_id retail_disc coupon_disc coupon_match_disc week ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2261 1 3.86 309 31625220889 940996 0.43 0 0 5 ## 2 2131 1 1.59 368 32053127496 873902 0.9 0 0 10 ## 3 511 1 1 316 32445856036 847901 0.69 0 0 13 ## 4 400 2 11.9 388 31932241118 13094913 2.9 0 0 8 ## 5 918 1 1.29 340 32074655895 1085604 0 0 0 10 ## 6 718 1 2.5 324 32614612029 883203 0.49 0 0 15 ## 7 868 1 3.49 323 32074722463 9884484 0 0 0 10 ## 8 1688 1 2 450 34850403304 1028715 1.79 0 0 33 ## 9 467 2 6.55 31782 31280745102 896613 4.44 0 0 2 ## 10 1947 1 3.99 32004 32744181707 978497 0 0 0 16 ## # … with 74,990 more rows, and 1 more variable: transaction_timestamp &lt;dttm&gt; 14.4.1 Knowledge check Check out the completejourney::demographics data set. Select all columns that start with “household_”. Select all columns that contain “_” and filter for observations where the household has one or more kids. 14.5 Computing summary statistics Obviously the goal of all this data wrangling is to be able to perform statistical analysis on our data. The summarize function allows us to perform the majority of the initial summary statistics when performing exploratory data analysis. For example, we can compute the mean sales_value across all observations: summarise(df, avg_sales_value = mean(sales_value)) ## # A tibble: 1 × 1 ## avg_sales_value ## &lt;dbl&gt; ## 1 3.12 These data have no missing values. However, if there are missing values you will need to use na.rm = TRUE in the function to remove missing values prior to computing the summary statistic: summarize(df, avg_sales_value = mean(sales_value, na.rm = TRUE)) There are a wide variety of functions you can use within summarize(). For example, the following lists just a few examples: Example summary functions. Function Description min(), max() min, max values in vector mean() mean value median() median value sum() sum of all vector values var(), sd() variance/std of vector first(), last() first/last value in vector nth() nth value in vector n() number of values in vector n_distinct() number of distinct values in vector As long as the function reduces a vector of values down to a single summarized value, you can use it in summarize(). Figure 14.2: Summarize functions need to condense a vector input down to a single summarized output value. summarize is not terribly useful unless we pair it with another function called group_by. This changes the unit of analysis from the complete data set to individual groups. Then, when you use the dplyr verbs on a grouped data frame they’ll be automatically applied “by group”. For example, if we applied exactly the same code to a data frame grouped by store_id, we get the average sales value for each store_id level: by_store &lt;- group_by(df, store_id) summarize(by_store, avg_sales_value = mean(sales_value)) ## # A tibble: 293 × 2 ## store_id avg_sales_value ## &lt;chr&gt; &lt;dbl&gt; ## 1 108 14.0 ## 2 1089 1 ## 3 1098 2.28 ## 4 1102 1.17 ## 5 112 1.39 ## 6 1132 0.71 ## 7 1240 1 ## 8 1247 0.39 ## 9 1252 0.99 ## 10 134 1.89 ## # … with 283 more rows A more efficient way to write this same code is to use the pipe operator: df %&gt;% group_by(store_id) %&gt;% summarize(avg_sales_value = mean(sales_value)) ## # A tibble: 293 × 2 ## store_id avg_sales_value ## &lt;chr&gt; &lt;dbl&gt; ## 1 108 14.0 ## 2 1089 1 ## 3 1098 2.28 ## 4 1102 1.17 ## 5 112 1.39 ## 6 1132 0.71 ## 7 1240 1 ## 8 1247 0.39 ## 9 1252 0.99 ## 10 134 1.89 ## # … with 283 more rows We can compute multiple summary statistics: df %&gt;% group_by(store_id) %&gt;% summarize( `10%` = quantile(sales_value, .1), `50%` = quantile(sales_value, .5), avg_sales_value = mean(sales_value), `90%` = quantile(sales_value, .9), sd = sd(sales_value), n() ) ## # A tibble: 293 × 7 ## store_id `10%` `50%` avg_sales_value `90%` sd `n()` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 108 14.0 14.0 14.0 14.0 0 2 ## 2 1089 1 1 1 1 NA 1 ## 3 1098 2.28 2.28 2.28 2.28 NA 1 ## 4 1102 0.55 0.825 1.17 2.07 0.916 4 ## 5 112 0.94 1.09 1.39 2.13 0.652 6 ## 6 1132 0.39 0.39 0.71 1.35 0.716 5 ## 7 1240 1 1 1 1 NA 1 ## 8 1247 0.39 0.39 0.39 0.39 NA 1 ## 9 1252 0.99 0.99 0.99 0.99 NA 1 ## 10 134 1.89 1.89 1.89 1.89 NA 1 ## # … with 283 more rows There are additional summarize alternative functions that are quite useful. Test these out and see how they work: # compute the average for multiple specified variables df %&gt;% summarize_at(c(&quot;sales_value&quot;, &quot;quantity&quot;), mean) # compute the average for all numeric variables df %&gt;% summarize_if(is.numeric, mean) summarize is a very universal function that can be used on continuous and categorical variables; however, count is a great function to use to compute the number of observations for each level of a categorical variable (or a combination of categorical variables): # number of observations in each level of store_id count(df, store_id) ## # A tibble: 293 × 2 ## store_id n ## &lt;chr&gt; &lt;int&gt; ## 1 108 2 ## 2 1089 1 ## 3 1098 1 ## 4 1102 4 ## 5 112 6 ## 6 1132 5 ## 7 1240 1 ## 8 1247 1 ## 9 1252 1 ## 10 134 1 ## # … with 283 more rows # number of observations in each level of store_id and sort output count(df, store_id, sort = TRUE) ## # A tibble: 293 × 2 ## store_id n ## &lt;chr&gt; &lt;int&gt; ## 1 367 2129 ## 2 406 1634 ## 3 356 1386 ## 4 292 1333 ## 5 31782 1238 ## 6 343 1220 ## 7 381 1211 ## 8 361 1149 ## 9 32004 1128 ## 10 321 1106 ## # … with 283 more rows # number of observations in each combination of sort_id &amp; product_id count(df, store_id, product_id, sort = TRUE) ## # A tibble: 63,652 × 3 ## store_id product_id n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 367 1082185 34 ## 2 375 6534178 33 ## 3 422 6534178 33 ## 4 429 6534178 32 ## 5 343 6534178 28 ## 6 406 6534178 28 ## 7 367 6534178 24 ## 8 361 6534178 23 ## 9 406 1082185 23 ## 10 31862 1082185 21 ## # … with 63,642 more rows 14.5.1 Knowledge check Using the completejourney sample transactions data as we did above… Compute the total quantity of items purchased across all transactions. Compute the total quantity of items purchased by household. Compute the total quantity of items purchased by household for only transactions at store 309 where the quantity purchased was greater than one. 14.6 Sorting observations Often, we desire to view observations in rank order for a particular variable(s). The arrange function allows us to order data by variables in ascending or descending order. For example, we can sort our observations based on sales_value: # default is ascending order arrange(df, sales_value) ## # A tibble: 75,000 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc coupon_match_disc week ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1038 340 31389956808 983659 0 0 0 0 0 3 ## 2 1166 408 31969185576 5978656 0 0 0 1.4 0 9 ## 3 1397 381 40800715180 8020001 0 0 0 0 0 48 ## 4 1889 369 33433310971 903325 0 0 0 0 0 23 ## 5 1241 368 31834162699 7441102 0 0 0 0 0 7 ## 6 867 369 40436331223 5978656 0 0 0 1.25 0 43 ## 7 2204 367 33397465730 887782 0 0 0 0 0 23 ## 8 40 406 40085429046 5978648 0 0 0 0 0 39 ## 9 1100 358 41125111338 976199 0 0 0 0 0 50 ## 10 1829 445 33655105183 1014948 1 0 3.99 0 0 24 ## # … with 74,990 more rows, and 1 more variable: transaction_timestamp &lt;dttm&gt; # use desc() for descending order arrange(df, desc(sales_value)) ## # A tibble: 75,000 × 11 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc coupon_match_disc week ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1246 334 32005986123 12484608 1 100 0 0 0 9 ## 2 2318 381 31993236318 13040176 1 100. 0 0 0 9 ## 3 1172 396 34338411595 15630122 1 88.9 0 0 0 31 ## 4 1959 736 33510101062 5716076 30080 86.0 0 0 0 24 ## 5 1959 323 34811925575 6544236 26325 75 3.95 0 0 33 ## 6 2133 433 35727256342 6534178 32623 75 3.26 0 0 37 ## 7 1764 327 33971056246 6534178 24583 68.8 2.46 0 0 28 ## 8 1959 384 34103546818 6544236 23519 67.0 3.52 0 0 29 ## 9 2312 442 41351830986 916561 1 66.1 18.9 0 0 52 ## 10 2360 323 35463937998 12812261 8 65.6 63.1 0 0 35 ## # … with 74,990 more rows, and 1 more variable: transaction_timestamp &lt;dttm&gt; This function becomes particularly useful when combining with summary statistics. For example, we can quickly find the product with the largest average sales value amount by adding arrange to the end of this sequence of functions: df %&gt;% group_by(product_id) %&gt;% summarize(avg_sales_value = mean(sales_value)) %&gt;% arrange(desc(avg_sales_value)) ## # A tibble: 20,902 × 2 ## product_id avg_sales_value ## &lt;chr&gt; &lt;dbl&gt; ## 1 13040176 100. ## 2 15630122 88.9 ## 3 5716076 86.0 ## 4 1100869 63.7 ## 5 904021 63.7 ## 6 1775642 62.9 ## 7 5668996 62.0 ## 8 6544236 60.0 ## 9 5571881 60.0 ## 10 839075 56.0 ## # … with 20,892 more rows Missing values (NAs) will always be moved to the end of the list regardless if you perform ascending or descending sorting. 14.6.1 Knowledge check Compute the average sales value by household and arrange in descending order to find the household with the largest average spend. Find the products with the largest median spend. Compute the total quantity of items purchased by household for only transactions at store 309 where the quantity purchased was greater than one. Which household purchased the largest quantity of items? 14.7 Creating new variables Often, we want to create a new variable that is a function of the current variables in our data frame. The mutate function allows us to add new variables while preserving the existing variables. For example, we can compute the net spend per item by dividing sales_value by quantity. mutate(df, spend_per_item = sales_value / quantity) ## # A tibble: 75,000 × 12 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc coupon_match_disc week ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2261 309 31625220889 940996 1 3.86 0.43 0 0 5 ## 2 2131 368 32053127496 873902 1 1.59 0.9 0 0 10 ## 3 511 316 32445856036 847901 1 1 0.69 0 0 13 ## 4 400 388 31932241118 13094913 2 11.9 2.9 0 0 8 ## 5 918 340 32074655895 1085604 1 1.29 0 0 0 10 ## 6 718 324 32614612029 883203 1 2.5 0.49 0 0 15 ## 7 868 323 32074722463 9884484 1 3.49 0 0 0 10 ## 8 1688 450 34850403304 1028715 1 2 1.79 0 0 33 ## 9 467 31782 31280745102 896613 2 6.55 4.44 0 0 2 ## 10 1947 32004 32744181707 978497 1 3.99 0 0 0 16 ## # … with 74,990 more rows, and 2 more variables: transaction_timestamp &lt;dttm&gt;, spend_per_item &lt;dbl&gt; mutate always adds new columns at the end of your dataset. There are many functions for creating new variables that you can use with mutate. The key property is that the function must be vectorized: it must take a vector of values as input, return a vector with the same number of values as output. There’s no way to list every possible function that you might use, but here’s a selection of functions that are frequently useful: Example window (aka vectorized) functions. Function Description +,-,*,/,^ arithmetic x / sum(x) arithmetic w/aggregation %/%, %% modular arithmetic log, exp, sqrt transformations lag, lead offsets cumsum, cumprod, cum... cum/rolling aggregates &gt;, &gt;=, &lt;, &lt;=, !=, == logical comparisons min_rank, dense_rank ranking between are values between a and b? ntile bin values into n buckets The following provides a few examples of integrating these functions with mutate. # reduce the number of variables so you can see the transformations df2 &lt;- select(df, household_id, quantity, sales_value, transaction_timestamp) # compute total net spend amount per item df2 %&gt;% mutate(spend_per_item = sales_value / quantity) ## # A tibble: 75,000 × 5 ## household_id quantity sales_value transaction_timestamp spend_per_item ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 2261 1 3.86 2017-01-28 14:06:53 3.86 ## 2 2131 1 1.59 2017-02-28 22:31:57 1.59 ## 3 511 1 1 2017-03-26 13:22:21 1 ## 4 400 2 11.9 2017-02-18 13:13:10 5.94 ## 5 918 1 1.29 2017-03-02 15:05:57 1.29 ## 6 718 1 2.5 2017-04-05 18:14:17 2.5 ## 7 868 1 3.49 2017-03-02 17:45:37 3.49 ## 8 1688 1 2 2017-08-11 22:41:02 2 ## 9 467 2 6.55 2017-01-06 07:47:01 3.28 ## 10 1947 1 3.99 2017-04-13 17:30:04 3.99 ## # … with 74,990 more rows # log transform sales_value df2 %&gt;% mutate(log_sales_value = log(sales_value)) ## # A tibble: 75,000 × 5 ## household_id quantity sales_value transaction_timestamp log_sales_value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 2261 1 3.86 2017-01-28 14:06:53 1.35 ## 2 2131 1 1.59 2017-02-28 22:31:57 0.464 ## 3 511 1 1 2017-03-26 13:22:21 0 ## 4 400 2 11.9 2017-02-18 13:13:10 2.47 ## 5 918 1 1.29 2017-03-02 15:05:57 0.255 ## 6 718 1 2.5 2017-04-05 18:14:17 0.916 ## 7 868 1 3.49 2017-03-02 17:45:37 1.25 ## 8 1688 1 2 2017-08-11 22:41:02 0.693 ## 9 467 2 6.55 2017-01-06 07:47:01 1.88 ## 10 1947 1 3.99 2017-04-13 17:30:04 1.38 ## # … with 74,990 more rows # order by date and compute the cumulative sum df2 %&gt;% arrange(transaction_timestamp) %&gt;% mutate(cumsum_sales_value = cumsum(sales_value)) ## # A tibble: 75,000 × 5 ## household_id quantity sales_value transaction_timestamp cumsum_sales_value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 906 1 1.5 2017-01-01 07:30:27 1.5 ## 2 1873 1 1.88 2017-01-01 08:47:20 3.38 ## 3 993 1 3.49 2017-01-01 09:16:13 6.87 ## 4 1465 1 1.38 2017-01-01 09:46:04 8.25 ## 5 239 1 1.59 2017-01-01 10:05:51 9.84 ## 6 58 1 2.49 2017-01-01 10:14:16 12.3 ## 7 1519 2 3.55 2017-01-01 10:32:09 15.9 ## 8 1130 1 3.77 2017-01-01 10:38:18 19.6 ## 9 2329 1 3.99 2017-01-01 10:46:03 23.6 ## 10 2329 1 3.59 2017-01-01 10:46:03 27.2 ## # … with 74,990 more rows # compute sum of sales_value for each product and # rank order totals across 25 bins df %&gt;% group_by(product_id) %&gt;% summarize(total = sum(sales_value)) %&gt;% mutate(bins = ntile(total, 25)) ## # A tibble: 20,902 × 3 ## product_id total bins ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1000002 18.4 23 ## 2 1000050 33.9 24 ## 3 1000106 6.47 15 ## 4 1000140 3.29 9 ## 5 1000148 6 15 ## 6 1000165 3.34 9 ## 7 1000204 6.17 15 ## 8 1000205 3.98 10 ## 9 1000228 2.34 6 ## 10 1000236 5.39 14 ## # … with 20,892 more rows 14.7.1 Knowledge check Using the completejourney sample transactions data as we did above… Create a new column (total_disc) that is the sum of all discounts applied to each transaction (total_disc = coupon_disc + retail_disc + coupon_match_disc). Create a new column (disc_to_sales) that computes the ratio of total discount to total sales value (total_disc / sales_value). Using the results from #2, create a new column bins that bins the disc_to_sales column into 10 bins. Filter for those transactions with the highest discount to sales ratio (bin 10). 14.8 Putting it altogether The beauty of dplyr is how it makes exploratory data analysis very simple and efficient. For example, we can combine all the above functions to: group the data by store_id and product_id, compute the total sales_value, create a rank order variable, filter for the top 5 products within each store with the highest total sales value, sort total by descending order. df %&gt;% group_by(store_id, product_id) %&gt;% summarize(total = sum(sales_value)) %&gt;% mutate(rank = min_rank(desc(total))) %&gt;% filter(rank &lt;= 5) %&gt;% arrange(desc(total)) ## `summarise()` has grouped output by &#39;store_id&#39;. You can override using the `.groups` argument. ## # A tibble: 1,064 × 4 ## # Groups: store_id [293] ## store_id product_id total rank ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 375 6534178 1057. 1 ## 2 429 6534178 980. 1 ## 3 406 6534178 922 1 ## 4 343 6534178 711. 1 ## 5 422 6534178 658. 1 ## 6 367 6534178 631. 1 ## 7 329 6534178 582. 1 ## 8 361 6534178 553. 1 ## 9 429 6533889 494. 2 ## 10 33923 6534178 478. 1 ## # … with 1,054 more rows 14.9 Exercises Using what you’ve learned thus far, can you find the store and week that experienced the greatest week over week growth in the number of units sold? The steps to follow include: Group by store and week Compute sum of the quantity of items sold Create week over week percent growth in total units sold. You may want to check out the lag() function for this step. Arrange in descending order See the code chunk hint below. # hint df %&gt;% group_by(______, ______) %&gt;% summarize(______) %&gt;% mutate(______) %&gt;% arrange(______) 14.10 Additional resources dplyr is an extremely powerful package with many data transformation capabilities. This chapter discusses the more commonly applied functions but there are many more capabilities provided by dplyr not discussed. Here are some resources to help you learn more about dplyr: R for Data Science book, chapter 5 RStudio’s Data wrangling with R webinar dplyr vignette DataCamp’s Data Manipulation in R with dplyr course RStudio’s Data wrangling cheat sheet "],["lesson-3c-tidy-data.html", "15 Lesson 3c: Tidy data 15.1 Learning objectives 15.2 Prerequisites 15.3 Making wide data longer 15.4 Making long data wider 15.5 Separate one variable into multiple 15.6 Combine multiple variables into one 15.7 Additional tidying functions 15.8 Putting it altogether 15.9 Exercises 15.10 Additional resources", " 15 Lesson 3c: Tidy data “Cannot emphasize enough how much time you save by putting analysis efforts into tidying data first.” - Hilary Parker Jenny Bryan stated that “classroom data are like teddy bears and real data are like a grizzley bear with salmon blood dripping out its mouth.” In essence, she was getting to the point that often when we learn how to perform a modeling approach in the classroom, the data used is provided in a format that appropriately feeds into the modeling tool of choice. In reality, datasets are messy and “every messy dataset is messy in its own way.”7 The concept of “tidy data” was established by Hadley Wickham and represents a “standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).”8 The objective should always to be to get a dataset into a tidy form which consists of: Each variable forms a column Each observation forms a row Each type of observational unit forms a table Figure 15.1: Tidy data. To create tidy data you need to be able to reshape your data; preferably via efficient and simple code. To help with this process Hadley created the tidyr package. This lesson covers the basics of tidyr to help you reshape your data as necessary. If you’d like to learn more about the underlying theory, you might enjoy the Tidy Data paper published in the Journal of Statistical Software, http://www.jstatsoft.org/v59/i10/paper. 15.1 Learning objectives Upon completing this module you will be able to: Make wide data long and long data wide. Separate and combine parts of columns. Impute missing values 15.2 Prerequisites Load the tidyr package to provide you access to the functions we’ll cover in this lesson. We’ll also use dplyr for a few examples. library(tidyr) library(dplyr) To illustrate various tidying tasks we will use several untidy datasets provided in the data directory (or via Canvas material download). These are artificial customer transaction datasets that are designed to mimick actual data. 15.3 Making wide data longer There are times when our data is considered “wide” or “unstacked” and a common attribute/variable of concern is spread out across columns. To reformat the data such that these common attributes are gathered together as a single variable, the pivot_longer() function will take multiple columns and collapse them into key-value pairs, duplicating all other columns as needed. For example, let’s say we have the given data frame. (untidy1 &lt;- readr::read_csv(&quot;data/untidy1.csv&quot;)) ## Rows: 3144 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): prod_desc ## dbl (4): hshd_id, Mar, Apr, May ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 3,144 × 5 ## hshd_id prod_desc Mar Apr May ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3376002291 HNTS MEAT FLV PASTA SAUCE 1 0 0 ## 2 3376163223 RAGU SPICY ITL STYL SAUCE 1.89 0 0 ## 3 3376542041 HMK CARD MDAY MOTHER 0 0 5.29 ## 4 3377032853 HMK CARD BRTHDAY CROWN 3.99 0 0 ## 5 3377032853 MZTA MARINARA PASTA SAUCE 8.78 0 0 ## 6 3377285583 HMK CARD ADMIN ANYONE 0 2 0 ## 7 3377285583 HMK CARD MDAY 0 0 4.99 ## 8 3377285583 HMK CARD MDAY ANYONE 0 3.99 0 ## 9 3377285583 HMK CARD MDAY MOTHER 0 2.99 7.99 ## 10 3377285583 SLV PLT TOMTO MARNARA SCE 8.97 0 0 ## # … with 3,134 more rows This data is considered untidy in the wide sense since the month variable is structured such that each month represents a variable. If we wanted to compute the total amount each household spent by month, this data set does not provide a convenient shape to work with. To re-structure the month component as an individual variable, we can pivot this data to be longer such that each month is within one column variable and the values associated with each month in a second column variable. untidy1 %&gt;% pivot_longer(cols = Mar:May, names_to = &quot;month&quot;, values_to = &quot;net_spend_amt&quot;) ## # A tibble: 9,432 × 4 ## hshd_id prod_desc month net_spend_amt ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 3376002291 HNTS MEAT FLV PASTA SAUCE Mar 1 ## 2 3376002291 HNTS MEAT FLV PASTA SAUCE Apr 0 ## 3 3376002291 HNTS MEAT FLV PASTA SAUCE May 0 ## 4 3376163223 RAGU SPICY ITL STYL SAUCE Mar 1.89 ## 5 3376163223 RAGU SPICY ITL STYL SAUCE Apr 0 ## 6 3376163223 RAGU SPICY ITL STYL SAUCE May 0 ## 7 3376542041 HMK CARD MDAY MOTHER Mar 0 ## 8 3376542041 HMK CARD MDAY MOTHER Apr 0 ## 9 3376542041 HMK CARD MDAY MOTHER May 5.29 ## 10 3377032853 HMK CARD BRTHDAY CROWN Mar 3.99 ## # … with 9,422 more rows This new structure allows us to perform follow-on analysis in a much easier fashion. For example, if we wanted to compute the total amount each household spent by month, we could simply do the following sequence: untidy1 %&gt;% pivot_longer(cols = Mar:May, names_to = &quot;month&quot;, values_to = &quot;net_spend_amt&quot;) %&gt;% group_by(hshd_id, month) %&gt;% summarize(monthly_spend = sum(net_spend_amt)) ## # A tibble: 3,708 × 3 ## # Groups: hshd_id [1,236] ## hshd_id month monthly_spend ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 3376002291 Apr 0 ## 2 3376002291 Mar 1 ## 3 3376002291 May 0 ## 4 3376163223 Apr 0 ## 5 3376163223 Mar 1.89 ## 6 3376163223 May 0 ## 7 3376542041 Apr 0 ## 8 3376542041 Mar 0 ## 9 3376542041 May 5.29 ## 10 3377032853 Apr 0 ## # … with 3,698 more rows It’s important to note that there is flexibility in how you specify the columns you would like to gather. In our example we used cols = Mar:May to imply we want to use all the columns including and between Mar and May. We could also have used the following to produce the same results: untidy1 %&gt;% pivot_longer(cols = Mar:May, names_to = &quot;month&quot;, values_to = &quot;net_spend_amt&quot;) untidy1 %&gt;% pivot_longer(cols = c(Mar, Apr, May), names_to = &quot;month&quot;, values_to = &quot;net_spend_amt&quot;) untidy1 %&gt;% pivot_longer(cols = 3:5, names_to = &quot;month&quot;, values_to = &quot;net_spend_amt&quot;) untidy1 %&gt;% pivot_longer(cols = -c(hshd_id, prod_desc), names_to = &quot;month&quot;, values_to = &quot;net_spend_amt&quot;) 15.3.1 Knowledge check Using the data provided by tidyr::table4b: Is this data untidy? If so, why? Compute the sum of the values across the years for each country. Now pivot this data set so that it is tidy with the year values represented in their own column called ‘year’ and the values listed in their own ‘population’ column. Using the pipe operator to chain together a sequence of functions, perform #3 and then compute the mean population for each country. 15.4 Making long data wider There are also times when we are required to turn long formatted data into wide formatted data – in other words, we want to pivot data to be wider. As a complement to pivot_longer, the pivot_wider function spreads a key-value pair across multiple columns. For example, the given data frame captures total household purchases for each product along with the household’s shopping habit (Premium Loyal, Valuable, etc.). (untidy2 &lt;- readr::read_csv(&quot;data/untidy2.csv&quot;)) ## Rows: 1536 Columns: 4 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): shabit, prod_merch_l20_desc ## dbl (2): hshd_id, net_spend_amt ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 1,536 × 4 ## hshd_id shabit prod_merch_l20_desc net_spend_amt ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 3376002291 Valuable PASTA &amp; PIZZA SAUCE 1 ## 2 3376163223 Premium Loyal PASTA &amp; PIZZA SAUCE 1.89 ## 3 3376542041 Potential GREETING CARDS 5.29 ## 4 3377032853 Uncommitted GREETING CARDS 3.99 ## 5 3377032853 Uncommitted PASTA &amp; PIZZA SAUCE 8.78 ## 6 3377285583 Premium Loyal GREETING CARDS 22.0 ## 7 3377285583 Premium Loyal PASTA &amp; PIZZA SAUCE 11.4 ## 8 3377528247 Valuable GREETING CARDS 11.0 ## 9 3378020265 Valuable GREETING CARDS 6.99 ## 10 3379767653 Uncommitted GREETING CARDS 3.99 ## # … with 1,526 more rows Currently, this data set is in a tidy format. But say we wanted to perform a classification model where we try to use the amount spent on each product to predict the shopping habit of each household, we would need to reorganize the data to make it compatible with future algorithms. To address this, we would need to pivot this data such that each product as its own variable with the total dollar amount spent on each product as the values. In essence, we are transposing the values in prod_merch_l20_desc to be the new variable names (aka key) and then adding the values in net_spend_amt to be the value under each variable name. untidy2 %&gt;% pivot_wider(names_from = prod_merch_l20_desc, values_from = net_spend_amt) ## # A tibble: 1,236 × 5 ## hshd_id shabit `PASTA &amp; PIZZA SAUCE` `GREETING CARDS` `NF SHORTENING AND OIL` ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3376002291 Valuable 1 NA NA ## 2 3376163223 Premium Loyal 1.89 NA NA ## 3 3376542041 Potential NA 5.29 NA ## 4 3377032853 Uncommitted 8.78 3.99 NA ## 5 3377285583 Premium Loyal 11.4 22.0 NA ## 6 3377528247 Valuable NA 11.0 NA ## 7 3378020265 Valuable NA 6.99 NA ## 8 3379767653 Uncommitted 2.19 3.99 NA ## 9 3379815863 Premium Loyal 1.49 NA NA ## 10 3380253171 Uncommitted NA 7.99 NA ## # … with 1,226 more rows This results in each household (hshd_id) having its own observation. You probably notice that there are now a lot of missing values which causes NA to be populated. If you were to apply this data to one of the many machine learning algorithms, you would likely run into errors due to the NAs. In this example, we could replace those NAs with zeros using values_fill = 0, since the household did not purchase any of those products. untidy2 %&gt;% pivot_wider(names_from = prod_merch_l20_desc, values_from = net_spend_amt, values_fill = 0) ## # A tibble: 1,236 × 5 ## hshd_id shabit `PASTA &amp; PIZZA SAUCE` `GREETING CARDS` `NF SHORTENING AND OIL` ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3376002291 Valuable 1 0 0 ## 2 3376163223 Premium Loyal 1.89 0 0 ## 3 3376542041 Potential 0 5.29 0 ## 4 3377032853 Uncommitted 8.78 3.99 0 ## 5 3377285583 Premium Loyal 11.4 22.0 0 ## 6 3377528247 Valuable 0 11.0 0 ## 7 3378020265 Valuable 0 6.99 0 ## 8 3379767653 Uncommitted 2.19 3.99 0 ## 9 3379815863 Premium Loyal 1.49 0 0 ## 10 3380253171 Uncommitted 0 7.99 0 ## # … with 1,226 more rows 15.4.1 Knowledge check Using the data provided by tidyr::table2: Is this data untidy? If so, why? Compute the cases-to-population ratio for each country by year. Now pivot this data set so that it is tidy with 4 columns: country, year, cases, and population. Using the pipe operator to chain together a sequence of functions, perform #3 and then compute the cases-to-population ratio for each country by year. 15.5 Separate one variable into multiple Many times a single column variable will capture multiple variables, or even parts of a variable you just don’t care about. This is exemplified in the following untidy3 data frame. Here, the product variable combines two variables, the product category (i.e. greeting cards, Pasta &amp; pizza sauce) along with the product description (i.e. graduation cards, birthday cards). For many reasons (summary statistics, visualization, modeling) we would likely want to separate these parts into their own variables. (untidy3 &lt;- readr::read_csv(&quot;data/untidy3.csv&quot;)) ## Rows: 3884 Columns: 4 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): shabit, product ## dbl (2): hshd_id, net_spend_amt ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 3,884 × 4 ## hshd_id shabit net_spend_amt product ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 8614328969 Premium Loyal 0.99 GREETING CARDS: CREPE ## 2 8614328969 Premium Loyal 0.99 GREETING CARDS: CREPE ## 3 3479082131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 4 3479082131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 5 3479082131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 6 3479082131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 7 5003651017 Uncommitted 1.89 GREETING CARDS: CREPE ## 8 5003651017 Uncommitted 1.89 GREETING CARDS: CREPE ## 9 5003651017 Uncommitted 1.89 GREETING CARDS: CREPE ## 10 8614328969 Premium Loyal 1.49 GREETING CARDS: BALLOONS ## # … with 3,874 more rows This can be accomplished using the separate function which separates a single column into multiple columns based on a separator. Additional arguments provide some flexibility with separating columns. # separate product column into two variables named &quot;prod_category&quot; &amp; &quot;prod_desc&quot; untidy3 %&gt;% separate(col = product, into = c(&quot;prod_category&quot;, &quot;prod_desc&quot;), sep = &quot;: &quot;) ## # A tibble: 3,884 × 5 ## hshd_id shabit net_spend_amt prod_category prod_desc ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 8614328969 Premium Loyal 0.99 GREETING CARDS CREPE ## 2 8614328969 Premium Loyal 0.99 GREETING CARDS CREPE ## 3 3479082131 Premium Loyal 1.89 GREETING CARDS CREPE ## 4 3479082131 Premium Loyal 1.89 GREETING CARDS CREPE ## 5 3479082131 Premium Loyal 1.89 GREETING CARDS CREPE ## 6 3479082131 Premium Loyal 1.89 GREETING CARDS CREPE ## 7 5003651017 Uncommitted 1.89 GREETING CARDS CREPE ## 8 5003651017 Uncommitted 1.89 GREETING CARDS CREPE ## 9 5003651017 Uncommitted 1.89 GREETING CARDS CREPE ## 10 8614328969 Premium Loyal 1.49 GREETING CARDS BALLOONS ## # … with 3,874 more rows The default separator is any non alpha-numeric character. In this example there are two: white space ” “ and colon : so we need to specify. You can also keep the original column that you are separating by including remove = FALSE. You can also pass a vector of integers to sep and separate() will interpret the integers as positions to split at. For example, say our household ID (hshd_id) value actually represents the the household and user. So, let’s say the first 7 digits is the household identifier and the last 3 digits is the user identifier. We can split this into two new variables with the following: untidy3 %&gt;% separate(hshd_id, into = c(&quot;hshd_id&quot;, &quot;member_id&quot;), sep = 7) ## # A tibble: 3,884 × 5 ## hshd_id member_id shabit net_spend_amt product ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 8614328 969 Premium Loyal 0.99 GREETING CARDS: CREPE ## 2 8614328 969 Premium Loyal 0.99 GREETING CARDS: CREPE ## 3 3479082 131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 4 3479082 131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 5 3479082 131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 6 3479082 131 Premium Loyal 1.89 GREETING CARDS: CREPE ## 7 5003651 017 Uncommitted 1.89 GREETING CARDS: CREPE ## 8 5003651 017 Uncommitted 1.89 GREETING CARDS: CREPE ## 9 5003651 017 Uncommitted 1.89 GREETING CARDS: CREPE ## 10 8614328 969 Premium Loyal 1.49 GREETING CARDS: BALLOONS ## # … with 3,874 more rows You can use positive and negative values to split the column. Positive values start at 1 on the far-left of the strings; negative value start at -1 on the far-right of the strings. 15.5.1 Knowledge check Using the data provided by tidyr::table3: Is this data untidy? If so, why? The rate variable is actually combining the number of cases and the population. Split this column such that you have four columns: country, year, cases, and population. Using the pipe operator to chain together a sequence of functions, perform #2 and then compute the cases-to-population ratio for each country by year. 15.6 Combine multiple variables into one Similarly, there are times when we would like to combine the values of two variables. As a compliment to separate, the unite function is a convenient function to paste together multiple variable values into one. Consider the following data frame that has separate date variables. To perform time series analysis or for visualizations we may desire to have a single date column. (untidy4 &lt;- readr::read_csv(&quot;data/untidy4.csv&quot;)) ## Rows: 3884 Columns: 7 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): shabit, prod_desc ## dbl (5): hshd_id, net_spend_amt, year, month, day ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 3,884 × 7 ## hshd_id shabit net_spend_amt prod_desc year month day ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8614328969 Premium Loyal 0.99 CREPE 2017 5 30 ## 2 8614328969 Premium Loyal 0.99 CREPE 2017 5 30 ## 3 3479082131 Premium Loyal 1.89 CREPE 2017 3 6 ## 4 3479082131 Premium Loyal 1.89 CREPE 2017 3 6 ## 5 3479082131 Premium Loyal 1.89 CREPE 2017 3 6 ## 6 3479082131 Premium Loyal 1.89 CREPE 2017 3 6 ## 7 5003651017 Uncommitted 1.89 CREPE 2017 3 28 ## 8 5003651017 Uncommitted 1.89 CREPE 2017 3 28 ## 9 5003651017 Uncommitted 1.89 CREPE 2017 3 28 ## 10 8614328969 Premium Loyal 1.49 BALLOONS 2017 5 30 ## # … with 3,874 more rows We can accomplish this by uniting these columns into one variable with unite. untidy4 %&gt;% unite(col = &quot;date&quot;, year:day, sep = &quot;-&quot;) ## # A tibble: 3,884 × 5 ## hshd_id shabit net_spend_amt prod_desc date ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 8614328969 Premium Loyal 0.99 CREPE 2017-5-30 ## 2 8614328969 Premium Loyal 0.99 CREPE 2017-5-30 ## 3 3479082131 Premium Loyal 1.89 CREPE 2017-3-6 ## 4 3479082131 Premium Loyal 1.89 CREPE 2017-3-6 ## 5 3479082131 Premium Loyal 1.89 CREPE 2017-3-6 ## 6 3479082131 Premium Loyal 1.89 CREPE 2017-3-6 ## 7 5003651017 Uncommitted 1.89 CREPE 2017-3-28 ## 8 5003651017 Uncommitted 1.89 CREPE 2017-3-28 ## 9 5003651017 Uncommitted 1.89 CREPE 2017-3-28 ## 10 8614328969 Premium Loyal 1.49 BALLOONS 2017-5-30 ## # … with 3,874 more rows Don’t worry, we’ll learn more appropriate ways to deal with dates in a later lesson. 15.6.1 Knowledge check Using the data provided by tidyr::table5: Is this data untidy? If so, why? Unite and separate the necessary columns such that you have four columns: country, year, cases, and population. Using the pipe operator to chain together a sequence of functions, perform #2 and then compute the cases-to-population ratio for each country by year. 15.7 Additional tidying functions The previous four functions (pivot_longer, pivot_wider, separate and unite) are the primary functions you will find yourself using on a continuous basis; however, there are some handy functions that are lesser known with the tidyr package. Consider this untidy data frame. expenses &lt;- tibble::as_tibble(read.table(header = TRUE, text = &quot; Dept Year Month Day Cost A 2015 01 01 $500.00 NA NA 02 05 $90.00 NA NA 02 22 $1,250.45 NA NA 03 NA $325.10 B NA 01 02 $260.00 NA NA 02 05 $90.00 &quot;, stringsAsFactors = FALSE)) Often Excel reports will not repeat certain variables. When we read these reports in, the empty cells are typically filled in with NA such as in the Dept and Year columns of our expense data frame. We can fill these values in with the previous entry using fill(): expenses %&gt;% fill(Dept, Year) ## # A tibble: 6 × 5 ## Dept Year Month Day Cost ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 A 2015 1 1 $500.00 ## 2 A 2015 2 5 $90.00 ## 3 A 2015 2 22 $1,250.45 ## 4 A 2015 3 NA $325.10 ## 5 B 2015 1 2 $260.00 ## 6 B 2015 2 5 $90.00 Also, sometimes accounting values in Excel spreadsheets get read in as a character value, which is the case for the Cost variable. We may wish to extract only the numeric part of this regular expression, which can be done with readr::parse_number. Note that parse_number works on a single variable so when you pipe the expense data frame into the function you need to use %$% operator as discussed in the Pipe Operator lesson. library(magrittr) expenses %$% readr::parse_number(Cost) ## [1] 500.0 90.0 1250.5 325.1 260.0 90.0 # you can use this to convert and save the Cost column to a numeric variable expenses %&gt;% dplyr::mutate(Cost = readr::parse_number(Cost)) ## # A tibble: 6 × 5 ## Dept Year Month Day Cost ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 A 2015 1 1 500 ## 2 &lt;NA&gt; NA 2 5 90 ## 3 &lt;NA&gt; NA 2 22 1250. ## 4 &lt;NA&gt; NA 3 NA 325. ## 5 B NA 1 2 260 ## 6 &lt;NA&gt; NA 2 5 90 You can also easily replace missing (or NA) values with a specified value: # replace the missing Day value expenses %&gt;% replace_na(replace = list(Day = 31)) ## # A tibble: 6 × 5 ## Dept Year Month Day Cost ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 A 2015 1 1 $500.00 ## 2 &lt;NA&gt; NA 2 5 $90.00 ## 3 &lt;NA&gt; NA 2 22 $1,250.45 ## 4 &lt;NA&gt; NA 3 31 $325.10 ## 5 B NA 1 2 $260.00 ## 6 &lt;NA&gt; NA 2 5 $90.00 # replace both the missing Day and Year values expenses %&gt;% replace_na(replace = list(Year = 2015, Day = 31)) ## # A tibble: 6 × 5 ## Dept Year Month Day Cost ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 A 2015 1 1 $500.00 ## 2 &lt;NA&gt; 2015 2 5 $90.00 ## 3 &lt;NA&gt; 2015 2 22 $1,250.45 ## 4 &lt;NA&gt; 2015 3 31 $325.10 ## 5 B 2015 1 2 $260.00 ## 6 &lt;NA&gt; 2015 2 5 $90.00 15.8 Putting it altogether Since the %&gt;% operator is embedded in tidyr, we can string multiple operations together to efficiently tidy data and make the process easy to read and follow. To illustrate, let’s use the following data, which has multiple messy attributes. a_mess &lt;- tibble::as_tibble(read.table(header = TRUE, text = &quot; Dep_Unt Year Q1 Q2 Q3 Q4 A.1 2006 15 NA 19 17 B.1 NA 12 13 27 23 A.2 NA 22 22 24 20 B.2 NA 12 13 25 18 A.1 2007 16 14 21 19 B.2 NA 13 11 16 15 A.2 NA 23 20 26 20 B.2 NA 11 12 22 16 &quot;)) In this case, a tidy data set should result in columns of Dept, Unit, Year, Quarter, and Cost. Furthermore, we want to fill in the year column where NAs currently exist. And we’ll assume that we know the missing value that exists in the Q2 column, and we’d like to update it. a_mess %&gt;% fill(Year) %&gt;% pivot_longer(cols = Q1:Q4, names_to = &quot;Quarter&quot;, values_to = &quot;Cost&quot;) %&gt;% separate(Dep_Unt, into = c(&quot;Dept&quot;, &quot;Unit&quot;)) %&gt;% replace_na(replace = list(Cost = 17)) ## # A tibble: 32 × 5 ## Dept Unit Year Quarter Cost ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 A 1 2006 Q1 15 ## 2 A 1 2006 Q2 17 ## 3 A 1 2006 Q3 19 ## 4 A 1 2006 Q4 17 ## 5 B 1 2006 Q1 12 ## 6 B 1 2006 Q2 13 ## 7 B 1 2006 Q3 27 ## 8 B 1 2006 Q4 23 ## 9 A 2 2006 Q1 22 ## 10 A 2 2006 Q2 22 ## # … with 22 more rows 15.9 Exercises Is the following data set “tidy”? Why or why not? people &lt;- tribble( ~name, ~names, ~values, #-----------------|--------|------ &quot;Phillip Woods&quot;, &quot;age&quot;, 45, &quot;Phillip Woods&quot;, &quot;height&quot;, 186, &quot;Phillip Woods&quot;, &quot;age&quot;, 50, &quot;Jessica Cordero&quot;, &quot;age&quot;, 37, &quot;Jessica Cordero&quot;, &quot;height&quot;, 156 ) Using the data set above, convert the “name” column into “first_name” and “last_name” columns. Now pivot this data set such that the “names” and “values” columns are converted into “age” and “height” columns. 15.10 Additional resources This chapter covers most, but not all, of what tidyr provides. There are several other resources you can check out to learn more. R for Data Science book, chapter 12 R Studio’s Data wrangling with R webinar tidyr vignette RStudio’s Data wrangling cheat sheet DataCamp’s Cleaning Data in R course Wickham, H. (2014). “Tidy data.” Journal of Statistical Software, 59(10). [document]↩︎ Ibid↩︎ "],["lab-2.html", "16 Lab", " 16 Lab TBD "],["overview-3.html", "17 Overview 17.1 Learning objectives 17.2 Tasks 17.3 Course readings", " 17 Overview It’s rare that a data analysis involves only a single table of data. Typically you have many tables of data, and you must combine them to answer the questions that you’re interested in. Collectively, multiple tables of data are called relational data because it is the relations, not just the individual datasets, that are important. Moreover, as data scientists work across multiple datasets they often find themselves working with different data types (i.e. numeric, categorical, date-times) and this can present many challenges. In this module we’ll explore how to use different join operations when working with relational data along with introduce additional Tidyverse packages that will simplify working with different data types. 17.1 Learning objectives By the end of this module you should be able to: Describe and apply the different join operations. Identify and manipulate a variety of different data types. 17.2 Tasks TBD 17.3 Course readings TBD "],["lesson-4a-relational-data.html", "18 Lesson 4a: Relational data 18.1 Learning objectives 18.2 Prerequisites 18.3 Keys 18.4 Mutating joins 18.5 Filtering joins 18.6 Exercises 18.7 Additional resources", " 18 Lesson 4a: Relational data It’s rare that a data analysis involves only a single table of data. Typically you have many tables of data, and you must combine them to answer the questions that you’re interested in. Collectively, multiple tables of data are called relational data because its the relations, not just the individual data sets, that are important. To work with relational data you need join operations that work with pairs of tables. There are two families of verbs designed to work with relational data: Mutating joins: add new variables to one data frame by matching observations in another. Filter joins: filter observations from one data frame based on whether or not they match an observation in the other table. In this lesson, we are going to look at different ways to apply mutating and filtering joins to relational data sets. 18.1 Learning objectives By the end of this lesson you’ll be able to: Use various mutating joins to combine variables from two tables. Use filtering joins to filter one data set based on observations in another data set. 18.2 Prerequisites Load the dplyr package to provide you access to the join functions we’ll cover in this lesson. library(dplyr) To illustrate various joining tasks we will use two very simple data frames x &amp; y. The colored column represents the “key” variable: these are used to match the rows between the tables. We’ll talk more about keys in a second. The grey column represents the “value” column that is carried along for the ride. Figure 18.1: Two simple data frames. x &lt;- tribble( ~id, ~val_x, 1, &quot;x1&quot;, 2, &quot;x2&quot;, 3, &quot;x3&quot; ) y &lt;- tribble( ~id, ~val_y, 1, &quot;y1&quot;, 2, &quot;y2&quot;, 4, &quot;y3&quot; ) However, we will also build upon the simple examples by using various data sets from the completejourney library: library(completejourney) Take some time to read about the various data sets available via completejourney. What different data sets are available and what do they represent? What are the common variables between each table? 18.3 Keys The variables used to connect two tables are called keys. A key is a variable (or set of variables) that uniquely identifies an observation. There are two primary types of keys we’ll consider in this lesson: A primary key uniquely identifies an observation in its own table A foreign key uniquely identifies an observation in another table Variables can be both a primary key and a foreign key. For example, within the transactions data household_id is a primary key to represent a household identifier for each transaction. household_id is also a foreign key in the demographics data set where it can be used to align household demographics to each transaction. A primary key and the corresponding foreign key in another table form a relation. Relations are typically one-to-many. For example, each transaction has one household, but each household has many transactions. In other data, you’ll occasionally see a 1-to-1 relationship. When data is cleaned appropriately the keys used to match two tables will be commonly named. For example, the variable that can link our x and y data sets is named id: intersect(colnames(x), colnames(y)) ## [1] &quot;id&quot; We can easily see this by looking at the data but when working with larger data sets this becomes more appropriate than just viewing the data. intersect(colnames(transactions_sample), colnames(demographics)) ## [1] &quot;household_id&quot; Although it is preferred, keys do not need to have the same name in both tables. For example, our household identifier could be named household_id in the transaction data but be hshd_id in the demographics table. The names would be different but they represent the same information. 18.3.1 Knowledge check Using the completejourney data, programmatically identify the common key(s) that between: transactions_sample and products tables. demographics and campaigns tables. campaigns and campaign_descriptions tables. Is there a common key between transactions_sample and coupon_redemptions? Does this mean we can or cannot join these two data sets? 18.4 Mutating joins Often we have separate data frames that can have common and differing variables for similar observations and we wish to join these data frames together. A mutating join allows you to combine variables from two tables. It first matches observations by their keys, then copies across variables from one table to the other. dplyr offers multiple mutating join functions (xxx_join()) that provide alternative ways to join two data frames: inner_join(): keeps only observations in x that match in y. left_join(): keeps all observations in x and adds available information from y. right_join(): keeps all observations in y and adds available information from x. full_join(): keeps all observations in both x and y. Let’s explore each of these a little more closely. 18.4.1 Inner join The simplest type of join is the inner join. An inner join matches pairs of observations whenever their keys are equal. Consequently, the output of an inner join is all rows from x where there are matching values in y, and all columns from x and y. An inner join is the most restrictive of the joins - it returns only rows with matches across both data frames. The following provides a nice illustration: Figure 18.2: Inner join (source). x %&gt;% inner_join(y, by = &quot;id&quot;) ## # A tibble: 2 × 3 ## id val_x val_y ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 18.4.2 Outer joins An inner join keeps observations that appear in both tables. However, we often want to retain all observations in at least one of the tables. Consequently, we can apply various outer joins to retain observations that appear in at least one of the tables. There are three types of outer joins: A left join keeps all observations in x. A right join keeps all observations in y. A full join keeps all observations in x and y. 18.4.2.1 Left join With a left join we retain all observations from x, and we add columns y. Rows in x where there is no matching key value in y will have NA values in the new columns. Figure 18.3: Left join (source). x %&gt;% left_join(y, by = &quot;id&quot;) ## # A tibble: 3 × 3 ## id val_x val_y ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 &lt;NA&gt; 18.4.2.2 Right join A right join is just a flipped left join where we retain all observations from y, and we add columns x. Similar to a left join, rows in y where there is no matching key value in x will have NA values in the new columns. Should I use a right join, or a left join? To answer this, ask yourself “which data frame should retain all of its rows?” - and use this one as the baseline. A left join keep all the rows in the first data frame written in the command, whereas a right join keeps all the rows in the second data frame. Figure 18.4: Right join (source). x %&gt;% right_join(y, by = &quot;id&quot;) ## # A tibble: 3 × 3 ## id val_x val_y ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 4 &lt;NA&gt; y3 18.4.2.3 Full join We can also perform a full join where we keep all observations in x and y. This join will match observations where the key variable(s) have matching information in both tables and then fill in non-matching values as NA. A full join is the most inclusive of the joins - it returns all rows from both data frames. Figure 18.5: Full join (source). x %&gt;% full_join(y, by = &quot;id&quot;) ## # A tibble: 4 × 3 ## id val_x val_y ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 &lt;NA&gt; ## 4 4 &lt;NA&gt; y3 18.4.3 Differing keys So far, the keys we’ve used to join two data frames have had the same name. This was encoded by using by = \"id\". However, this is not a requirement. In fact, if we exclude the by argument then our xxx_join() functions will identify all common variable names in both tables and join by those. When this happens we get a message: x %&gt;% inner_join(y) ## Joining, by = &quot;id&quot; ## # A tibble: 2 × 3 ## id val_x val_y ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 But what happens we our common key variable is named differently in each data frame? a &lt;- tribble( ~id_1, ~val_a, 1, &quot;x1&quot;, 2, &quot;x2&quot;, 3, &quot;x3&quot; ) b &lt;- tribble( ~id_2, ~val_b, 1, &quot;y1&quot;, 2, &quot;y2&quot;, 4, &quot;y3&quot; ) In this case, since our common key variable has different names in each table (id_1 in a and id_2 in b), our inner join function doesn’t know how to join these two data frames. a %&gt;% inner_join(b) ## Error in `inner_join()`: ## ! `by` must be supplied when `x` and `y` have no common variables. ## ℹ use by = character()` to perform a cross-join. When this happens, we can explicitly tell our join function to use two unique key names as a common key: a %&gt;% inner_join(b, by = c(&quot;id_1&quot; = &quot;id_2&quot;)) ## # A tibble: 2 × 3 ## id_1 val_a val_b ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 18.4.4 Bigger example So far we’ve used small simple examples to illustrate the differences between joins. Now let’s use our completejourney data to look at some larger examples. Say we wanted to add product information (via products) to each transaction (transaction_sample); however, we want to retain all transactions. This would suggest a left join so we can keep all transaction observations but simply add product information where possible to each transaction. First, let’s get the common key: intersect(colnames(transactions_sample), colnames(products)) ## [1] &quot;product_id&quot; This aligns to the data dictionary so we can trust this is the accurate common key. We can now perform a left join using product_id as the common key: Like mutate(), the join functions add variables to the right, so if you have a lot of variables already, the new variables won’t get printed out. You can also use View() on the output to show the resulting table in a spreadsheet like view. transactions_sample %&gt;% left_join(products, by = &quot;product_id&quot;) ## # A tibble: 75,000 × 17 ## household_id store_id basket_id product_id quantity sales_value retail_disc coupon_disc coupon_match_disc week ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2261 309 31625220889 940996 1 3.86 0.43 0 0 5 ## 2 2131 368 32053127496 873902 1 1.59 0.9 0 0 10 ## 3 511 316 32445856036 847901 1 1 0.69 0 0 13 ## 4 400 388 31932241118 13094913 2 11.9 2.9 0 0 8 ## 5 918 340 32074655895 1085604 1 1.29 0 0 0 10 ## 6 718 324 32614612029 883203 1 2.5 0.49 0 0 15 ## 7 868 323 32074722463 9884484 1 3.49 0 0 0 10 ## 8 1688 450 34850403304 1028715 1 2 1.79 0 0 33 ## 9 467 31782 31280745102 896613 2 6.55 4.44 0 0 2 ## 10 1947 32004 32744181707 978497 1 3.99 0 0 0 16 ## # … with 74,990 more rows, and 7 more variables: transaction_timestamp &lt;dttm&gt;, manufacturer_id &lt;chr&gt;, department &lt;chr&gt;, ## # brand &lt;fct&gt;, product_category &lt;chr&gt;, product_type &lt;chr&gt;, package_size &lt;chr&gt; This has now added product information to each transaction. Consequently, if we wanted to get the total sales across the meat department but summarized at the product_category level so that we can identify which products generate the greatest sales we could follow this joining procedure with additional skills we learned in previous lessons: transactions_sample %&gt;% left_join(products, by = &quot;product_id&quot;) %&gt;% filter(department == &#39;MEAT&#39;) %&gt;% group_by(product_category) %&gt;% summarize(total_spend = sum(`sales_value`)) %&gt;% arrange(desc(total_spend)) ## # A tibble: 9 × 2 ## product_category total_spend ## &lt;chr&gt; &lt;dbl&gt; ## 1 BEEF 8800. ## 2 CHICKEN 2540. ## 3 PORK 2527. ## 4 SMOKED MEATS 784. ## 5 TURKEY 625. ## 6 EXOTIC GAME/FOWL 61 ## 7 LAMB 34.7 ## 8 MEAT - MISC 4.63 ## 9 RW FRESH PROCESSED MEAT 1.69 18.4.5 Knowledge check Join the transactions_sample and demographics data so that you have household demographics for each transaction. Now compute the total sales by age category to identify which age group generates the most sales. Use successive joins to join transactions_sample with coupons and then with coupon_redemptions. Use the proper join that will only retain those transactions that have coupon and coupon redemption data. 18.5 Filtering joins In certain situations, we may want to filter one data set based on observations in another data set but not add new information. Whereas mutating joins are for the purpose of adding columns and rows from one data set to another, filtering joins are for the purpose of filtering. Filtering joins include: semi_join(): keeps all observations in x that have a match in y. anti_join(): drops all observations in x that have a match in y. 18.5.1 Semi join A semi-join keeps all observations in the baseline data frame that have a match in the secondary data frame (but does not add new columns nor duplicate any rows for multiple matches). Figure 18.6: Semi join (source). x %&gt;% semi_join(y, by = &quot;id&quot;) ## # A tibble: 2 × 2 ## id val_x ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 x1 ## 2 2 x2 18.5.2 Anti join The anti join is another “filtering join” that returns rows in the baseline data frame that do not have a match in the secondary data frame. Common scenarios for an anti-join include identifying records not present in another data frame, troubleshooting spelling in a join (reviewing records that should have matched), and examining records that were excluded after another join. Figure 18.7: Anti join (source). x %&gt;% anti_join(y, by = &quot;id&quot;) ## # A tibble: 1 × 2 ## id val_x ## &lt;dbl&gt; &lt;chr&gt; ## 1 3 x3 18.5.3 Bigger example We can use the completejourney data to highlight the purpose behind filtering joins. In our transactions_sample we have a total of 75,000 transactions. tally(transactions_sample) ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 75000 Now say our manager came to us and asked – “of all our transactions, how many of them are related to households that we have demographic information on?” To answer this question we would use a semi-join, which shows that 42,199 (56%) of our transactions are for customers that we have demographics on. transactions_sample %&gt;% semi_join(demographics, by = &quot;household_id&quot;) %&gt;% tally() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 42199 Now, what if our manager asked us the same question but with a slightly different angle – “of all our transactions, how many of them are by customers that we don’t have demographic information on?” To answer this question we would use an anti-join, which shows that 32,801 (44%) of our transactions are for customers that we do not have demographics on. transactions_sample %&gt;% anti_join(demographics, by = &quot;household_id&quot;) %&gt;% tally() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 32801 18.5.4 Knowledge check Using the products and transactions_sample data, how many products have and have not sold? In other words, of all the products we have in our inventory, how many have a been involved in a transaction? How many have not been involved in a transaction? Using the demographics and transactions_sample data, identify which income level buys the most quantity of goods. 18.6 Exercises Get demographic information for all households that have total sales (sales_value) of $100 or more. Of the households that have total sales of $100 or more, how many of these customers do we not have demographic information on? Using the promotions_sample and transactions_sample data, compute the total sales for all products that were in a display in the front of the store (display_location –&gt; 1). 18.7 Additional resources dplyr’s join operators are extremely powerful and easy to use once you understand what each xxx_join() is doing. Here are some resources to help you learn more about joins: R for Data Science book, chapter 13 Article on the difference between mutating and filtering joins dplyr vignette RStudio’s data wrangling cheat sheet "],["lesson-4b-handling-text-data.html", "19 Lesson 4b: Handling text data 19.1 Learning objectives 19.2 Prerequisites 19.3 String basics 19.4 Regular expressions 19.5 Exercises 19.6 Additional resources", " 19 Lesson 4b: Handling text data Dealing with character strings is often under-emphasized in data analysis training. The focus typically remains on numeric values; however, the growth in data collection is also resulting in greater bits of information embedded in text. Consequently, handling, cleaning and processing character strings is becoming a prerequisite in daily data analysis. This lesson is meant to give you the foundation of working with character strings. 19.1 Learning objectives By the end of this lesson you’ll be able to: Perform basic character string manipulations. Use regular expressions to identify and manipulate patterns in character strings. 19.2 Prerequisites The following packages will be used throughout this lesson. Base R contains many functions to work with strings but we’ll avoid them because they can be inconsistent, which makes them hard to remember. Instead we’ll use functions from stringr for text manipulation and we’ll also combine this with the use of dplyr for data frame manipulation. library(dplyr) library(stringr) For data, we’ll leverage the completejourney data. For example, the products data within completejourney provides various variables (e.g. product_category and product_type) that contain character strings we may need to clean, normalize, or identify patterns within. library(completejourney) products ## # A tibble: 92,331 × 7 ## product_id manufacturer_id department brand product_category product_type package_size ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 25671 2 GROCERY National FRZN ICE ICE - CRUSHED/CUBED 22 LB ## 2 26081 2 MISCELLANEOUS National &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 26093 69 PASTRY Private BREAD BREAD:ITALIAN/FRENCH &lt;NA&gt; ## 4 26190 69 GROCERY Private FRUIT - SHELF STABLE APPLE SAUCE 50 OZ ## 5 26355 69 GROCERY Private COOKIES/CONES SPECIALTY COOKIES 14 OZ ## 6 26426 69 GROCERY Private SPICES &amp; EXTRACTS SPICES &amp; SEASONINGS 2.5 OZ ## 7 26540 69 GROCERY Private COOKIES/CONES TRAY PACK/CHOC CHIP COOKIES 16 OZ ## 8 26601 69 DRUG GM Private VITAMINS VITAMIN - MINERALS 300 CT(1) ## 9 26636 69 PASTRY Private BREAKFAST SWEETS SW GDS: SW ROLLS/DAN &lt;NA&gt; ## 10 26691 16 GROCERY Private PNT BTR/JELLY/JAMS HONEY 12 OZ ## # … with 92,321 more rows 19.3 String basics Basic string manipulation typically includes case conversion, counting characters, and extracting parts of a string. These operations can all be performed with base R functions; however, some operations (or at least their syntax) are greatly simplified with the stringr package. Moreover, stringr is typically very useful when combined with dplyr data manipulation functions. 19.3.1 Case conversion To change the case of a character string we can use str_to_xxx: # we&#39;ll focus on the product_category column head(products$product_category) ## [1] &quot;FRZN ICE&quot; NA &quot;BREAD&quot; &quot;FRUIT - SHELF STABLE&quot; &quot;COOKIES/CONES&quot; ## [6] &quot;SPICES &amp; EXTRACTS&quot; # force everything to lower case str_to_lower(products$product_category) %&gt;% head() ## [1] &quot;frzn ice&quot; NA &quot;bread&quot; &quot;fruit - shelf stable&quot; &quot;cookies/cones&quot; ## [6] &quot;spices &amp; extracts&quot; # force everything to upper case str_to_upper(products$product_category) %&gt;% head() ## [1] &quot;FRZN ICE&quot; NA &quot;BREAD&quot; &quot;FRUIT - SHELF STABLE&quot; &quot;COOKIES/CONES&quot; ## [6] &quot;SPICES &amp; EXTRACTS&quot; Typically we want to normalize text by converting everything to lower case but keep the data in our original data frame. We can combine str_ functions with dplyr::mutate to accomplish this: products %&gt;% mutate(product_category = str_to_lower(product_category)) ## # A tibble: 92,331 × 7 ## product_id manufacturer_id department brand product_category product_type package_size ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 25671 2 GROCERY National frzn ice ICE - CRUSHED/CUBED 22 LB ## 2 26081 2 MISCELLANEOUS National &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 26093 69 PASTRY Private bread BREAD:ITALIAN/FRENCH &lt;NA&gt; ## 4 26190 69 GROCERY Private fruit - shelf stable APPLE SAUCE 50 OZ ## 5 26355 69 GROCERY Private cookies/cones SPECIALTY COOKIES 14 OZ ## 6 26426 69 GROCERY Private spices &amp; extracts SPICES &amp; SEASONINGS 2.5 OZ ## 7 26540 69 GROCERY Private cookies/cones TRAY PACK/CHOC CHIP COOKIES 16 OZ ## 8 26601 69 DRUG GM Private vitamins VITAMIN - MINERALS 300 CT(1) ## 9 26636 69 PASTRY Private breakfast sweets SW GDS: SW ROLLS/DAN &lt;NA&gt; ## 10 26691 16 GROCERY Private pnt btr/jelly/jams HONEY 12 OZ ## # … with 92,321 more rows When we apply mutate() to an existing column we do not add a new column, we simply overwrite the existing column. 19.3.2 Counting characters There may be times where we are interested in how many characters exist in a string. Whether you need to extract this information or filter for observations that exceed a certain character length you can use str_count to accomplish this: # count number of characters in product_category str_count(products$product_category) %&gt;% head() ## [1] 8 NA 5 20 13 17 # create a new variable with character length products %&gt;% mutate(category_length = str_count(product_category)) %&gt;% select(product_category, category_length) ## # A tibble: 92,331 × 2 ## product_category category_length ## &lt;chr&gt; &lt;int&gt; ## 1 FRZN ICE 8 ## 2 &lt;NA&gt; NA ## 3 BREAD 5 ## 4 FRUIT - SHELF STABLE 20 ## 5 COOKIES/CONES 13 ## 6 SPICES &amp; EXTRACTS 17 ## 7 COOKIES/CONES 13 ## 8 VITAMINS 8 ## 9 BREAKFAST SWEETS 16 ## 10 PNT BTR/JELLY/JAMS 18 ## # … with 92,321 more rows # filter for product categories that are greater than the mean character length products %&gt;% filter(str_count(product_category) &gt; mean(str_count(product_category), na.rm = TRUE)) ## # A tibble: 45,192 × 7 ## product_id manufacturer_id department brand product_category product_type package_size ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 26190 69 GROCERY Private FRUIT - SHELF STABLE APPLE SAUCE 50 OZ ## 2 26426 69 GROCERY Private SPICES &amp; EXTRACTS SPICES &amp; SEASONINGS 2.5 OZ ## 3 26636 69 PASTRY Private BREAKFAST SWEETS SW GDS: SW ROLLS/DAN &lt;NA&gt; ## 4 26691 16 GROCERY Private PNT BTR/JELLY/JAMS HONEY 12 OZ ## 5 26738 69 GROCERY Private ICE CREAM/MILK/SHERBTS TRADITIONAL 56 OZ ## 6 26941 69 GROCERY Private ICE CREAM/MILK/SHERBTS TRADITIONAL 56 OZ ## 7 27030 69 GROCERY Private ICE CREAM/MILK/SHERBTS TRADITIONAL 56 OZ ## 8 27152 69 GROCERY Private SPICES &amp; EXTRACTS SPICES &amp; SEASONINGS 4 OZ ## 9 27158 69 GROCERY Private ICE CREAM/MILK/SHERBTS TRADITIONAL 56 OZ ## 10 27334 69 DRUG GM Private DIETARY AID PRODUCTS DIET CNTRL LIQS NUTRITIONAL &lt;NA&gt; ## # … with 45,182 more rows 19.3.3 Extracting parts of character strings If you need to extract parts of a character string you can use str_sub. By default start = 1 and end = 3 will tell the function to extract the first through third characters. If you want to start from the right side of the string then negative numbers such as start = -1 and end = -5 will tell the function to extract the last five characters. # extract parts of a string (example: extract first three characters) str_sub(products$product_category, start = 1, end = 3) %&gt;% head() ## [1] &quot;FRZ&quot; NA &quot;BRE&quot; &quot;FRU&quot; &quot;COO&quot; &quot;SPI&quot; # create new variable for last five characters products %&gt;% select(product_category) %&gt;% mutate(last_five = str_sub(product_category, start = -5, end = -1)) ## # A tibble: 92,331 × 2 ## product_category last_five ## &lt;chr&gt; &lt;chr&gt; ## 1 FRZN ICE N ICE ## 2 &lt;NA&gt; &lt;NA&gt; ## 3 BREAD BREAD ## 4 FRUIT - SHELF STABLE TABLE ## 5 COOKIES/CONES CONES ## 6 SPICES &amp; EXTRACTS RACTS ## 7 COOKIES/CONES CONES ## 8 VITAMINS AMINS ## 9 BREAKFAST SWEETS WEETS ## 10 PNT BTR/JELLY/JAMS /JAMS ## # … with 92,321 more rows 19.3.4 Knowledge check Using product_type, which product has the longest description? How about the shortest? Using product_id, get all products that start with “222”. 19.4 Regular expressions A regular expression (aka regex) is a sequence of characters that define a search pattern, mainly for use in pattern matching with text strings. Typically, regex patterns consist of a combination of alphanumeric characters as well as special characters. The pattern can also be as simple as a single character or it can be more complex and include several characters. To understand how to work with regular expressions in R, we need to consider two primary features of regular expressions. One has to do with the syntax, or the way regex patterns are expressed in R. The other has to do with the functions used for regex matching in R. You will be exposed to both of these in the following sections. 19.4.1 Regex basics The stringr package provides us a convenient approach to regular expressions. The common function call will consist of str_*(string, pattern) where: str_* represents a wide variety of regex functions depending on what you want to do, string represents the character string of concern (i.e. products$product_category), pattern represents the regex pattern you are looking to match. For example let’s say you are looking for observations where the word “FRUIT” was used in the product_category description. str_detect detects the presence or absence of a pattern (“FRUIT” in this example) and returns a logical vector. Since the output is TRUE or FALSE, this is a handy function to combine with filter to filter for observations that have that pattern. # detect if the word &quot;FRUIT&quot; is used in each product category description str_detect(products$product_category, &quot;FRUIT&quot;) %&gt;% head() ## [1] FALSE NA FALSE TRUE FALSE FALSE # filter for those observations that include the use of &quot;FRUIT&quot; products %&gt;% filter(str_detect(product_category, &quot;FRUIT&quot;)) ## # A tibble: 1,153 × 7 ## product_id manufacturer_id department brand product_category product_type package_size ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 26190 69 GROCERY Private FRUIT - SHELF STABLE APPLE SAUCE 50 OZ ## 2 27503 69 GROCERY Private FRUIT - SHELF STABLE CRANBERRY SAUCE 16 OZ ## 3 27960 69 GROCERY Private FRZN FRUITS FROZEN FRUIT 16 OZ ## 4 33048 5820 PRODUCE National ORGANICS FRUIT &amp; VEGETABLES ORGANIC CITRUS 2 LB BAG ## 5 37543 876 NUTRITION National DRIED FRUIT RAISINS 24 OZ ## 6 44599 69 GROCERY Private FRUIT - SHELF STABLE FRUIT COCKTAIL FRUIT SALAD 15.25 OZ ## 7 45218 69 GROCERY Private FRUIT - SHELF STABLE PEARS 15.25 OZ ## 8 45607 69 GROCERY Private FRUIT - SHELF STABLE PINEAPPLE 8 OZ ## 9 49748 69 GROCERY Private FRUIT - SHELF STABLE MANDARIN ORANGES/CITRUS SECT 15 OZ ## 10 49930 2 PRODUCE National STONE FRUIT PEACHES YELLOW FLESH &lt;NA&gt; ## # … with 1,143 more rows There are wide variety of str_ functions (check them out by typing str_ + tab in the console). For example, check out the following functions that all look for the pattern “FRZN” but provide different outputs: # count number of instances in each observation str_count(products$product_category, &quot;FRZN&quot;) %&gt;% head() ## [1] 1 NA 0 0 0 0 # extract the first instance in each observation (NA results when no instance exists) str_extract(products$product_category, &quot;FRZN&quot;) %&gt;% head() ## [1] &quot;FRZN&quot; NA NA NA NA NA # locate the start/stop position of the first instance in each observation str_locate(products$product_category, &quot;FRZN&quot;) %&gt;% head() ## start end ## [1,] 1 4 ## [2,] NA NA ## [3,] NA NA ## [4,] NA NA ## [5,] NA NA ## [6,] NA NA # replace all instances of the word &quot;FRZN&quot; with &quot;FROZEN&quot; str_replace_all(products$product_category, pattern = &quot;FRZN&quot;, replacement = &quot;FROZEN&quot;) %&gt;% head() ## [1] &quot;FROZEN ICE&quot; NA &quot;BREAD&quot; &quot;FRUIT - SHELF STABLE&quot; &quot;COOKIES/CONES&quot; ## [6] &quot;SPICES &amp; EXTRACTS&quot; The patterns used in the str_ functions are case sensitive but you can use regex to control options (i.e. str_count(products$product_category, regex(“frzn”, ignore_case = TRUE))). 19.4.2 Multiple Words In the previous section you saw that you can search for a given word using any of the str_ functions. We can build onto this and search for multiple words. For example, you can search for multiple phrases to identify seasonal products. Say you wanted to identify all products targeting summer or fall, rather than search for these words in separate str_detect() calls… # filter for summer or fall items products %&gt;% filter( str_detect(product_category, regex(&quot;summer&quot;, ignore_case = TRUE)) | str_detect(product_category, regex(&quot;fall&quot;, ignore_case = TRUE)) ) ## # A tibble: 651 × 7 ## product_id manufacturer_id department brand product_category product_type package_size ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 37073 69 DRUG GM Private SPRING/SUMMER SEASONAL GRILL ACCESSORIES &lt;NA&gt; ## 2 91614 1277 DRUG GM National SPRING/SUMMER SEASONAL DISPOSABLE FOILWARE 3 CT ## 3 99090 1277 DRUG GM National SPRING/SUMMER SEASONAL DISPOSABLE FOILWARE 2 CT ## 4 99172 1277 DRUG GM National SPRING/SUMMER SEASONAL DISPOSABLE FOILWARE 2 CT ## 5 106668 1277 DRUG GM National SPRING/SUMMER SEASONAL DISPOSABLE FOILWARE &lt;NA&gt; ## 6 182402 2160 DRUG GM National SPRING/SUMMER SEASONAL COOLERS 28 QT ## 7 276046 1277 DRUG GM National SPRING/SUMMER SEASONAL DISPOSABLE FOILWARE &lt;NA&gt; ## 8 461689 5759 DRUG GM National SPRING/SUMMER SEASONAL MISC HOLIDAYS &lt;NA&gt; ## 9 504050 1277 DRUG GM National SPRING/SUMMER SEASONAL DISPOSABLE FOILWARE &lt;NA&gt; ## 10 555735 1277 DRUG GM National SPRING/SUMMER SEASONAL DISPOSABLE FOILWARE &lt;NA&gt; ## # … with 641 more rows We can simply search for observations that include the words “summer” or “fall” within a single str_detect() call by using | between the search terms, which is equivalent to an or statement. products %&gt;% filter(str_detect(product_category, regex(&quot;summer|fall&quot;, ignore_case = TRUE))) ## # A tibble: 651 × 7 ## product_id manufacturer_id department brand product_category product_type package_size ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 37073 69 DRUG GM Private SPRING/SUMMER SEASONAL GRILL ACCESSORIES &lt;NA&gt; ## 2 91614 1277 DRUG GM National SPRING/SUMMER SEASONAL DISPOSABLE FOILWARE 3 CT ## 3 99090 1277 DRUG GM National SPRING/SUMMER SEASONAL DISPOSABLE FOILWARE 2 CT ## 4 99172 1277 DRUG GM National SPRING/SUMMER SEASONAL DISPOSABLE FOILWARE 2 CT ## 5 106668 1277 DRUG GM National SPRING/SUMMER SEASONAL DISPOSABLE FOILWARE &lt;NA&gt; ## 6 182402 2160 DRUG GM National SPRING/SUMMER SEASONAL COOLERS 28 QT ## 7 276046 1277 DRUG GM National SPRING/SUMMER SEASONAL DISPOSABLE FOILWARE &lt;NA&gt; ## 8 461689 5759 DRUG GM National SPRING/SUMMER SEASONAL MISC HOLIDAYS &lt;NA&gt; ## 9 504050 1277 DRUG GM National SPRING/SUMMER SEASONAL DISPOSABLE FOILWARE &lt;NA&gt; ## 10 555735 1277 DRUG GM National SPRING/SUMMER SEASONAL DISPOSABLE FOILWARE &lt;NA&gt; ## # … with 641 more rows 19.4.3 Line anchors Line anchors are used to identify patterns at the beginning or end of an element. To find a pattern at the beginning of the element we use ^ and a pattern at the end of the element is found with $. For example, if you wanted to find any observations with the word “fruit” in the product_category column we can use the following: # contains &quot;juice&quot; products %&gt;% filter(str_detect(product_category, regex(&quot;fruit&quot;, ignore_case = TRUE))) ## # A tibble: 1,153 × 7 ## product_id manufacturer_id department brand product_category product_type package_size ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 26190 69 GROCERY Private FRUIT - SHELF STABLE APPLE SAUCE 50 OZ ## 2 27503 69 GROCERY Private FRUIT - SHELF STABLE CRANBERRY SAUCE 16 OZ ## 3 27960 69 GROCERY Private FRZN FRUITS FROZEN FRUIT 16 OZ ## 4 33048 5820 PRODUCE National ORGANICS FRUIT &amp; VEGETABLES ORGANIC CITRUS 2 LB BAG ## 5 37543 876 NUTRITION National DRIED FRUIT RAISINS 24 OZ ## 6 44599 69 GROCERY Private FRUIT - SHELF STABLE FRUIT COCKTAIL FRUIT SALAD 15.25 OZ ## 7 45218 69 GROCERY Private FRUIT - SHELF STABLE PEARS 15.25 OZ ## 8 45607 69 GROCERY Private FRUIT - SHELF STABLE PINEAPPLE 8 OZ ## 9 49748 69 GROCERY Private FRUIT - SHELF STABLE MANDARIN ORANGES/CITRUS SECT 15 OZ ## 10 49930 2 PRODUCE National STONE FRUIT PEACHES YELLOW FLESH &lt;NA&gt; ## # … with 1,143 more rows However, if we only wanted those products where the category starts with “fruit” than we can use the ^ anchor: # starts with &quot;fruit&quot; products %&gt;% filter(str_detect(product_category, regex(&quot;^fruit&quot;, ignore_case = TRUE))) ## # A tibble: 396 × 7 ## product_id manufacturer_id department brand product_category product_type package_size ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 26190 69 GROCERY Private FRUIT - SHELF STABLE APPLE SAUCE 50 OZ ## 2 27503 69 GROCERY Private FRUIT - SHELF STABLE CRANBERRY SAUCE 16 OZ ## 3 44599 69 GROCERY Private FRUIT - SHELF STABLE FRUIT COCKTAIL FRUIT SALAD 15.25 OZ ## 4 45218 69 GROCERY Private FRUIT - SHELF STABLE PEARS 15.25 OZ ## 5 45607 69 GROCERY Private FRUIT - SHELF STABLE PINEAPPLE 8 OZ ## 6 49748 69 GROCERY Private FRUIT - SHELF STABLE MANDARIN ORANGES/CITRUS SECT 15 OZ ## 7 52240 69 GROCERY Private FRUIT - SHELF STABLE FRUIT COCKTAIL FRUIT SALAD 15 OZ ## 8 53854 170 GROCERY National FRUIT - SHELF STABLE FRUIT BOWL AND CUPS 4 OZ ## 9 54776 69 GROCERY Private FRUIT - SHELF STABLE FRUIT COCKTAIL FRUIT SALAD 15 OZ ## 10 58782 170 GROCERY National FRUIT - SHELF STABLE FRUIT BOWL AND CUPS 6/3.9 OZ ## # … with 386 more rows Alternatively, if we only wanted those products where the category ends with “fruit” than we can use the $ anchor: # starts with &quot;fruit&quot; products %&gt;% filter(str_detect(product_category, regex(&quot;fruit$&quot;, ignore_case = TRUE))) ## # A tibble: 398 × 7 ## product_id manufacturer_id department brand product_category product_type package_size ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 37543 876 NUTRITION National DRIED FRUIT RAISINS 24 OZ ## 2 49930 2 PRODUCE National STONE FRUIT PEACHES YELLOW FLESH &lt;NA&gt; ## 3 54460 69 NUTRITION Private DRIED FRUIT RAISINS 24 OZ BAG ## 4 64583 2 PRODUCE National TROPICAL FRUIT TROPICAL FRUIT - OTHER CTN ## 5 71432 876 NUTRITION National DRIED FRUIT DRIED FRUIT - OTHER 7 OZ ## 6 71792 876 NUTRITION National DRIED FRUIT DRIED FRUIT - OTHER 7 OZ ## 7 78306 612 NUTRITION National DRIED FRUIT DRIED FRUIT - OTHER 6 OZ ## 8 84804 612 NUTRITION National DRIED FRUIT DRIED FRUIT - OTHER 6 OZ ## 9 119875 1587 NUTRITION National DRIED FRUIT DRIED FRUIT - OTHER 6 OZ ## 10 136217 2 PRODUCE National STONE FRUIT CHERRIES RED 2 LB ## # … with 388 more rows And we can combine the two if we only wanted those products where the category starts or ends with “fruit”: # starts with &quot;fruit&quot; products %&gt;% filter(str_detect(product_category, regex(&quot;^fruit|fruit$&quot;, ignore_case = TRUE))) ## # A tibble: 794 × 7 ## product_id manufacturer_id department brand product_category product_type package_size ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 26190 69 GROCERY Private FRUIT - SHELF STABLE APPLE SAUCE 50 OZ ## 2 27503 69 GROCERY Private FRUIT - SHELF STABLE CRANBERRY SAUCE 16 OZ ## 3 37543 876 NUTRITION National DRIED FRUIT RAISINS 24 OZ ## 4 44599 69 GROCERY Private FRUIT - SHELF STABLE FRUIT COCKTAIL FRUIT SALAD 15.25 OZ ## 5 45218 69 GROCERY Private FRUIT - SHELF STABLE PEARS 15.25 OZ ## 6 45607 69 GROCERY Private FRUIT - SHELF STABLE PINEAPPLE 8 OZ ## 7 49748 69 GROCERY Private FRUIT - SHELF STABLE MANDARIN ORANGES/CITRUS SECT 15 OZ ## 8 49930 2 PRODUCE National STONE FRUIT PEACHES YELLOW FLESH &lt;NA&gt; ## 9 52240 69 GROCERY Private FRUIT - SHELF STABLE FRUIT COCKTAIL FRUIT SALAD 15 OZ ## 10 53854 170 GROCERY National FRUIT - SHELF STABLE FRUIT BOWL AND CUPS 4 OZ ## # … with 784 more rows 19.4.4 Metacharacters Metacharacters consist of non-alphanumeric symbols such as: .    \\    |    (    )    [    {    $    *    +   ? To match metacharacters in regex you need to escape. In R, we escape them with a double backslash “\\”. The following displays the general escape syntax for the most common metacharacters: Escaping metacharacters. Metacharacter Literal Meaning Escape Syntax . period or dot \\\\. $ dollar sign \\\\$ * asterisk \\\\* + plus sign \\\\+ ? question mark \\\\? | vertical bar \\\\\\| ^ caret \\\\^ [ square bracket \\\\[ { curly brace \\\\{ ( parenthesis \\\\( The reason we need to escape these characters is because most of these actually have meaning when declaring regular expressions. For example, say we wanted to identify any product_category that contains a period (“.”). If we simply use the following we actually get all our records back. This is because in regex expressions “.” is used to match any character. products %&gt;% filter(str_detect(product_category, regex(&quot;.&quot;, ignore_case = TRUE))) ## # A tibble: 91,791 × 7 ## product_id manufacturer_id department brand product_category product_type package_size ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 25671 2 GROCERY National FRZN ICE ICE - CRUSHED/CUBED 22 LB ## 2 26093 69 PASTRY Private BREAD BREAD:ITALIAN/FRENCH &lt;NA&gt; ## 3 26190 69 GROCERY Private FRUIT - SHELF STABLE APPLE SAUCE 50 OZ ## 4 26355 69 GROCERY Private COOKIES/CONES SPECIALTY COOKIES 14 OZ ## 5 26426 69 GROCERY Private SPICES &amp; EXTRACTS SPICES &amp; SEASONINGS 2.5 OZ ## 6 26540 69 GROCERY Private COOKIES/CONES TRAY PACK/CHOC CHIP COOKIES 16 OZ ## 7 26601 69 DRUG GM Private VITAMINS VITAMIN - MINERALS 300 CT(1) ## 8 26636 69 PASTRY Private BREAKFAST SWEETS SW GDS: SW ROLLS/DAN &lt;NA&gt; ## 9 26691 16 GROCERY Private PNT BTR/JELLY/JAMS HONEY 12 OZ ## 10 26738 69 GROCERY Private ICE CREAM/MILK/SHERBTS TRADITIONAL 56 OZ ## # … with 91,781 more rows So, we need to use an escape (“\\”) to tell the regular expression you want to match a literal metacharacter. products %&gt;% filter(str_detect(product_category, regex(&quot;\\\\.&quot;, ignore_case = TRUE))) ## # A tibble: 247 × 7 ## product_id manufacturer_id department brand product_category product_type package_size ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 43361 1916 GROCERY National MISC. DAIRY MEXICAN SOFT TORTILLAS AND WRA 33 OZ ## 2 50776 69 GROCERY Private MISC. DAIRY REFRIGERATED PUDDING 24 OZ ## 3 53070 69 GROCERY Private MISC. DAIRY REFRIGERATED PUDDING 24 OZ ## 4 60924 435 GROCERY National MISC. DAIRY REFRIGERATED PASTA 9 OZ ## 5 64623 289 GROCERY National MISC. DAIRY MISC DAIRY REFIGERATED 20 OZ ## 6 68011 908 GROCERY National MISC. DAIRY REFRIGERATED MEXICAN PRODUCTS 14.1 OZ ## 7 68924 289 GROCERY National MISC. DAIRY MISC DAIRY REFIGERATED 20 OZ ## 8 79411 1046 GROCERY National MISC. DAIRY REFRIGERATED PUDDING 6 PK/4 OZ ## 9 81194 69 GROCERY Private MISC. DAIRY REFRIGERATED MEXICAN PRODUCTS 20 OZ ## 10 82250 69 GROCERY Private MISC. DAIRY REFRIGERATED MEXICAN PRODUCTS 6 OZ ## # … with 237 more rows 19.4.5 Character classes To match one of several characters in a specified set we can enclose the characters of concern with square brackets [ ]. In addition, to matching any characters not in a specified character set we can include the caret ^ at the beginning of the set within the brackets. The following displays the general syntax for common character classes but these can be altered easily as shown in the examples that follow: Common character classes. Character class Description [aeiou] match any specified lower case vowel [AEIOU] match any specified upper case vowel [0123456789] match any specified numeric values [0-9] match any range specified numeric values [a-z] match any range of lowercase letters [A-Z] match any range of uppercase letters [a-zA-Z0-9] match any of the above [^aeiou] match anything other than a lowercase vowel [^0-9] match anything other than the specified numeric values For example, say we wanted to find any products where the package_size is not a round numeric size in ounces. The following identifies any rows where package_size contains a dot (remember, we need to escape that character with \\\\.) followed by “oz”. products %&gt;% select(product_type, package_size) %&gt;% filter(str_detect(package_size, regex(&quot;\\\\.[0-9] oz&quot;, ignore_case = TRUE))) ## # A tibble: 10,454 × 2 ## product_type package_size ## &lt;chr&gt; &lt;chr&gt; ## 1 SPICES &amp; SEASONINGS 2.5 OZ ## 2 SKILLET DINNERS 5.8 OZ ## 3 SKILLET DINNERS 6.5 OZ ## 4 GROUND COFFEE 34.5 OZ ## 5 SKILLET DINNERS 6.5 OZ ## 6 SW GDS: MUFFINS 10.8 OZ ## 7 GELATIN .3 OZ ## 8 GELATIN .3 OZ ## 9 GELATIN .3 OZ ## 10 SKILLET DINNERS 6.2 OZ ## # … with 10,444 more rows Now, say we wanted to do the same but we are interested in any packages that are in ounces (“OZ”) or pounds (“LB”). Your first reaction is probably to do something like: products %&gt;% select(product_type, package_size) %&gt;% filter(str_detect(package_size, regex(&quot;\\\\.[0-9] oz|lb&quot;, ignore_case = TRUE))) ## # A tibble: 13,012 × 2 ## product_type package_size ## &lt;chr&gt; &lt;chr&gt; ## 1 ICE - CRUSHED/CUBED 22 LB ## 2 SPICES &amp; SEASONINGS 2.5 OZ ## 3 SKILLET DINNERS 5.8 OZ ## 4 SKILLET DINNERS 6.5 OZ ## 5 GROUND COFFEE 34.5 OZ ## 6 SKILLET DINNERS 6.5 OZ ## 7 SW GDS: MUFFINS 10.8 OZ ## 8 GELATIN .3 OZ ## 9 GELATIN .3 OZ ## 10 GELATIN .3 OZ ## # … with 13,002 more rows Wait! The first observation is in pounds but its a round number and not a decimal. This is because our regex (\\\\.[0-9] oz|lb) is actually looking for any package size where its a decimal of ounces (\\\\.[0-9] oz) or in pounds (lb). We need to modify our regex just a tad. If we change it to \\\\.[0-9] (oz|lb) (note that oz|lb is now in parenthesis), we are now specifying to search for \\\\.[0-9] followed by “oz” or “lb”. products %&gt;% select(product_type, package_size) %&gt;% filter(str_detect(package_size, regex(&quot;\\\\.[0-9] (oz|lb)&quot;, ignore_case = TRUE))) ## # A tibble: 10,712 × 2 ## product_type package_size ## &lt;chr&gt; &lt;chr&gt; ## 1 SPICES &amp; SEASONINGS 2.5 OZ ## 2 SKILLET DINNERS 5.8 OZ ## 3 SKILLET DINNERS 6.5 OZ ## 4 GROUND COFFEE 34.5 OZ ## 5 SKILLET DINNERS 6.5 OZ ## 6 SW GDS: MUFFINS 10.8 OZ ## 7 GELATIN .3 OZ ## 8 GELATIN .3 OZ ## 9 GELATIN .3 OZ ## 10 SKILLET DINNERS 6.2 OZ ## # … with 10,702 more rows Now, say we wanted to find any package size that contains a decimal between 0-.4: products %&gt;% select(product_type, package_size) %&gt;% filter(str_detect(package_size, regex(&quot;\\\\.[0-4] (oz|lb)&quot;, ignore_case = TRUE))) ## # A tibble: 3,092 × 2 ## product_type package_size ## &lt;chr&gt; &lt;chr&gt; ## 1 GELATIN .3 OZ ## 2 GELATIN .3 OZ ## 3 GELATIN .3 OZ ## 4 SKILLET DINNERS 6.2 OZ ## 5 SKILLET DINNERS 6.4 OZ ## 6 RICE SIDE DISH MIXES DRY 4.3 OZ ## 7 ADULT CEREAL 17.3 OZ ## 8 NOODLE SIDE DISH MIXES 6.2 OZ ## 9 CANDY BARS (SINGLES)(INCLUDING 1.4 OZ ## 10 DRINKS - CARB JUICE (OVER 50% 25.4 OZ ## # … with 3,082 more rows 19.4.6 Shorthand character classes Since certain character classes are used often, a series of shorthand character classes are available. For example, rather than use [0-9] every time we are searching for a number in a regex, we can use just use \\\\d to match any digit. The following are a few of the commonly used shorthand character classes: Common shorthand character classes. Syntax Description \\\\d match any digit \\\\D match any non-digit \\\\s match a space character \\\\S match a non-space character \\\\w match a word \\\\W match a non-word \\\\b match a word boundary \\\\B match a non-word boundary We can use these to find patterns such as… Find all products where the package_size starts with a numeric digit: products %&gt;% select(product_type, package_size) %&gt;% filter(str_detect(package_size, regex(&quot;^\\\\d&quot;))) ## # A tibble: 59,790 × 2 ## product_type package_size ## &lt;chr&gt; &lt;chr&gt; ## 1 ICE - CRUSHED/CUBED 22 LB ## 2 APPLE SAUCE 50 OZ ## 3 SPECIALTY COOKIES 14 OZ ## 4 SPICES &amp; SEASONINGS 2.5 OZ ## 5 TRAY PACK/CHOC CHIP COOKIES 16 OZ ## 6 VITAMIN - MINERALS 300 CT(1) ## 7 HONEY 12 OZ ## 8 TRADITIONAL 56 OZ ## 9 TRADITIONAL 56 OZ ## 10 TRADITIONAL 56 OZ ## # … with 59,780 more rows Or all products where the package_size starts with a non-digit: products %&gt;% select(product_type, package_size) %&gt;% filter(str_detect(package_size, regex(&quot;^\\\\D&quot;))) ## # A tibble: 1,955 × 2 ## product_type package_size ## &lt;chr&gt; &lt;chr&gt; ## 1 NATURAL CHEESE EXACT WT CHUNKS A B D 8 OZ ## 2 GELATIN .3 OZ ## 3 GELATIN .3 OZ ## 4 GELATIN .3 OZ ## 5 AMMONIA .5 GAL ## 6 EGGS - X-LARGE A D 1 DZ ## 7 STRING CHEESE AB 8 OZ ## 8 EGGS - MEDIUM ABD 1 DZ ## 9 SPICES &amp; SEASONINGS .56 OZ ## 10 STRING CHEESE AB 8 OZ ## # … with 1,945 more rows 19.4.7 POSIX character classes Closely related to regex character classes are POSIX character classes which are expressed in double brackets [[ ]]. Some commonly used POSIX character classes include: Common POSIX character classes. Syntax Description [[:lower:]] lower case letters [[:upper:]] upper case letters [[:alpha:]] alphabetic characters [[:lower:]] + [[:alpha:]] [[:digit:]] numeric values [[:alnum:]] alpha numeric values [[:alpha:]] + [[:digit:]] [[:blank:]] blank characters (e.g. space, tab) [[:punct:]] punctuation characters (e.g. ! . , ? &amp; / ( [ ) For example, if we wanted to find any products where the package_size includes a punctuation in the description: products %&gt;% select(product_type, package_size) %&gt;% filter(str_detect(package_size, regex(&quot;[[:punct:]]&quot;))) ## # A tibble: 17,891 × 2 ## product_type package_size ## &lt;chr&gt; &lt;chr&gt; ## 1 SPICES &amp; SEASONINGS 2.5 OZ ## 2 VITAMIN - MINERALS 300 CT(1) ## 3 TRADITIONAL 1/2 GAL ## 4 SKILLET DINNERS 5.8 OZ ## 5 SKILLET DINNERS 6.5 OZ ## 6 GROUND COFFEE 34.5 OZ ## 7 SKILLET DINNERS 6.5 OZ ## 8 AMMONIA 1/2 GAL ## 9 SW GDS: MUFFINS 10.8 OZ ## 10 GELATIN .3 OZ ## # … with 17,881 more rows 19.4.8 Repetition When we want to match a certain number of characters that meet a certain criteria we can apply repetition operators to our pattern searches. Common repetition operators include: Common repetition operators. Syntax Description . wildcard to match any character once ? the preceding item is optional and will be matched at most once * the preceding item will be matched zero or more times + the preceding item will be matched one or more times {n} the preceding item will be matched exactly n times {n,} the preceding item will be matched n or more times {n,m} the preceding item will be matched at least n times but not more than m times For example, say we want to find all products where the package_size contains at least 3 digits: products %&gt;% select(product_category, package_size) %&gt;% filter(str_detect(package_size, regex(&quot;\\\\d{3,}&quot;))) ## # A tibble: 3,313 × 2 ## product_category package_size ## &lt;chr&gt; &lt;chr&gt; ## 1 VITAMINS 300 CT(1) ## 2 FACIAL TISS/DNR NAPKIN 250 CT ## 3 CANDY - CHECKLANE 1.625 OZ ## 4 TEAS 100 CT ## 5 ANALGESICS 100 CT ## 6 COFFEE FILTERS 200 CT ## 7 CANDY - PACKAGED 10.125 OZ ## 8 TEAS 100 CT ## 9 TEAS 100 CT ## 10 ISOTONIC DRINKS 128 OZ ## # … with 3,303 more rows One thing you probably notice is that the above syntax will match three or more digits within the entire character string. But what if we wanted to identify repetition of a pattern sequence. For example, say we wanted to find product_ids where the number “8” is repeated. We can use a backreference to do so. A backreference will match the same text as previously matched within parentheses. So, in this example, we look for any repeated sequences of the number 8 in product_id. products %&gt;% filter(str_detect(product_id, regex(&quot;(8)\\\\1&quot;))) ## # A tibble: 4,750 × 7 ## product_id manufacturer_id department brand product_category product_type package_size ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 26889 32 DRUG GM National MAGAZINE TV/MOVIE-MAGAZINE &lt;NA&gt; ## 2 28889 69 SEAFOOD-PCKGD Private SEAFOOD - FROZEN SEAFOOD-FRZ-RAW FILLETS 12 OZ ## 3 28892 69 GROCERY Private FROZEN PIZZA SNACKS/APPETIZERS 20 OZ ## 4 28897 69 GROCERY Private EGGS EGGS - X-LARGE A D 1 DZ ## 5 32888 1075 GROCERY National CRACKERS/MISC BKD FD SNACK CRACKERS 8.5 OZ ## 6 33882 544 GROCERY National BAG SNACKS TORTILLA/NACHO CHIPS 13 OZ ## 7 33883 764 GROCERY National LAUNDRY ADDITIVES FABRIC SOFTENER LIQUID 40 LOAD ## 8 36880 69 GROCERY Private CRACKERS/MISC BKD FD SOUP CRACKERS (SALTINE/OYSTER) 16 OZ ## 9 37889 317 GROCERY National SALD DRSNG/SNDWCH SPRD POURABLE SALAD DRESSINGS 8 OZ ## 10 38877 1884 GROCERY National MILK BY-PRODUCTS SOUR CREAMS 16 OZ ## # … with 4,740 more rows What if we wanted to look for product_ids that contain three “8”s in a row then we need to repeat that pattern: products %&gt;% filter(str_detect(product_id, regex(&quot;(8)\\\\1{2}&quot;))) ## # A tibble: 343 × 7 ## product_id manufacturer_id department brand product_category product_type package_size ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 28889 69 SEAFOOD-PCKGD Private SEAFOOD - FROZEN SEAFOOD-FRZ-RAW FILLETS 12 OZ ## 2 32888 1075 GROCERY National CRACKERS/MISC BKD FD SNACK CRACKERS 8.5 OZ ## 3 62888 69 GROCERY Private LAUNDRY ADDITIVES FABRIC SOFTENER LIQUID 60 LOADS ## 4 75888 1448 GROCERY National CAT LITTER LITTER ACCESSORIES 12 CT ## 5 78888 1051 DRUG GM National ELECTRICAL SUPPPLIES DECOR BULBS 2 CT ## 6 88832 348 DRUG GM National CANDY - CHECKLANE CHEWING GUM 12 CT ## 7 108889 69 MEAT-PCKGD Private LUNCHMEAT POULTRY 1 LB ## 8 124888 857 DRUG GM National CANDY - CHECKLANE CANDY BARS (SINGLES)(INCLUDING 1 CT ## 9 238885 3830 SALAD BAR National SALAD BAR SALAD BAR FRESH FRUIT &lt;NA&gt; ## 10 255888 5474 DRUG GM National NEWSPAPER NEWSPAPER &lt;NA&gt; ## # … with 333 more rows 19.4.9 Putting it altogether Ok, let’s use a few tools we’ve learned to answer a question we may get asked by our boss. Say we were asked to identify the top 5 products that have the most total sales; however, we only want to focus on those products that weigh 10lbs or more. We can applly the following steps: filter for regex \"^\\\\d{2,}(\\\\.)?.*lb\" which means: ^\\\\d{2,}: starts with at least 2 numeric digits (\\\\.)?.: followed by an optional decimal .*lb: followed by a character zero or more times take the resulting product list and inner join with transactions so we only retain those transactions and products that have a matching product ID in both tables, compute total sales grouped by product (here we use product type just to provide us more context over the product ID), and then use slice_max to get the top 5 total_sales values (you could also just use arrange(desc(total_sales))). products %&gt;% filter(str_detect(package_size, regex(&quot;^\\\\d{2,}(\\\\.)?.*lb&quot;, ignore_case = TRUE))) %&gt;% inner_join(transactions_sample, by = &quot;product_id&quot;) %&gt;% group_by(product_type) %&gt;% summarize(total_sales = sum(sales_value)) %&gt;% slice_max(total_sales, n = 5) ## # A tibble: 5 × 2 ## product_type total_sales ## &lt;chr&gt; &lt;dbl&gt; ## 1 BANANAS 887. ## 2 GRAPES RED 527. ## 3 GRAPES WHITE 507. ## 4 TOMATOES HOTHOUSE ON THE VINE 364. ## 5 POTATOES RUSSET (BULK&amp;BAG) 346. 19.4.10 Knowledge check How many products contain the word “bulk” in product_type? How many products do not contain punctuation in their package_size? Find all frozen pizza products. Be careful, this is not straight forward! 19.5 Exercises To answer these questions you’ll need to use the products and transactions_sample data frames. Identify all different products that contain “pizza” in their product_type description. Which product produces the greatest amount of total sales? Identify all products that are categorized (product_category) as pizza but are considered a snack or appetizer (product_type). Which of these products have the most sales (measured by quantity)? How many products contain package_sizes that do not contain a numeric value. 19.6 Additional resources Character string data are often considered semi-structured data. Text can be structured in a specified field; however, the quality and consistency of the text input can be far from structured. Consequently, managing and manipulating character strings can be extremely tedious and unique to each data wrangling process. As a result, taking the time to learn the nuances of dealing with character strings and regex functions can provide a great return on investment; however, the functions and techniques required will likely be greater than what I could offer here. So here are additional resources that are worth reading and learning from: stringr Package Vignette R for Data Science, ch. 14 Text Mining with R Regular Expressions Mastering Regular Expressions "],["lesson-4c-handling-dates-times.html", "20 Lesson 4c: Handling dates &amp; times 20.1 Learning objectives 20.2 Prerequisites 20.3 Getting current date &amp; time 20.4 Creating dates 20.5 Extract &amp; manipulate parts of dates 20.6 Calculations with dates 20.7 Exercises 20.8 Additional resources", " 20 Lesson 4c: Handling dates &amp; times Real world data are often associated with dates and time; however, dealing with dates accurately can appear to be a complicated task due to the variety in formats and accounting for time-zone differences and leap years. R has a range of functions that allow you to work with dates and times. Furthermore, packages such as lubridate make it easier to work with dates and times. 20.1 Learning objectives By the end of this lesson you’ll be able to: Create date and time data. Extract &amp; manipulate parts of dates and times. Perform calculations with dates and times. 20.2 Prerequisites For this lesson, we will use the packages supplied by tidyverse for supporting roles; however, the bulk of the emphasis will be on using the lubridate package which provides useful functions for working with dates and times. library(tidyverse) # provides supporting data wrangling functions library(lubridate) # provides functions for working with dates &amp; times To illustrate various transformation tasks on dates we will use the transactions data from completejourney. Note how transaction_timestamp is a &lt;dttm&gt; type, which stands for a date-time data type. library(completejourney) glimpse(transactions_sample) ## Rows: 75,000 ## Columns: 11 ## $ household_id &lt;chr&gt; &quot;2261&quot;, &quot;2131&quot;, &quot;511&quot;, &quot;400&quot;, &quot;918&quot;, &quot;718&quot;, &quot;868&quot;, &quot;1688&quot;, &quot;467&quot;, &quot;1947&quot;, &quot;1694&quot;, &quot;568&quot;, … ## $ store_id &lt;chr&gt; &quot;309&quot;, &quot;368&quot;, &quot;316&quot;, &quot;388&quot;, &quot;340&quot;, &quot;324&quot;, &quot;323&quot;, &quot;450&quot;, &quot;31782&quot;, &quot;32004&quot;, &quot;446&quot;, &quot;446&quot;, &quot;… ## $ basket_id &lt;chr&gt; &quot;31625220889&quot;, &quot;32053127496&quot;, &quot;32445856036&quot;, &quot;31932241118&quot;, &quot;32074655895&quot;, &quot;32614612029&quot;,… ## $ product_id &lt;chr&gt; &quot;940996&quot;, &quot;873902&quot;, &quot;847901&quot;, &quot;13094913&quot;, &quot;1085604&quot;, &quot;883203&quot;, &quot;9884484&quot;, &quot;1028715&quot;, &quot;896… ## $ quantity &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0,… ## $ sales_value &lt;dbl&gt; 3.86, 1.59, 1.00, 11.87, 1.29, 2.50, 3.49, 2.00, 6.55, 3.99, 2.50, 3.49, 1.50, 1.00, 1.88… ## $ retail_disc &lt;dbl&gt; 0.43, 0.90, 0.69, 2.90, 0.00, 0.49, 0.00, 1.79, 4.44, 0.00, 0.49, 0.50, 0.89, 0.00, 0.12,… ## $ coupon_disc &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ coupon_match_disc &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ week &lt;int&gt; 5, 10, 13, 8, 10, 15, 10, 33, 2, 16, 47, 18, 18, 23, 37, 40, 33, 35, 43, 41, 49, 27, 16, … ## $ transaction_timestamp &lt;dttm&gt; 2017-01-28 14:06:53, 2017-02-28 22:31:57, 2017-03-26 13:22:21, 2017-02-18 13:13:10, 2017… 20.3 Getting current date &amp; time To get current date and time information: Sys.timezone() ## [1] &quot;America/New_York&quot; Sys.Date() ## [1] &quot;2022-06-14&quot; Sys.time() ## [1] &quot;2022-06-14 16:32:13 EDT&quot; If using the lubridate package: today() ## [1] &quot;2022-06-14&quot; now() ## [1] &quot;2022-06-14 16:32:13 EDT&quot; 20.4 Creating dates When date and time data are imported into R they will often default to a character string. This requires us to convert strings to dates. We may also have multiple strings that we want to merge to create a date variable. 20.4.1 Convert strings to dates To convert a character string that is already in a date format (YYYY-MM-DD) into a date object, we can use one of the lubridate parsing functions. One of the many benefits of the lubridate package is that it automatically recognizes the common separators used when recording dates (“-“, “/”, “.”, and “”). As a result, you only need to focus on specifying the order of the date elements to determine the parsing function applied. The different parsing functions available in lubridate include: Date-time parsing functions. Order of elements in date-time Parsing function year, month, day ymd() year, day, month ydm() month, day, year mdy() day, month, year dmy() hour, minute hm() hour, minute, second hms() year, month, day, hour, minute, second ymd_hms() For example, consider the three different character strings that follow. We can convert each one to a date or date-time object with the relevant lubridate parsing function. x &lt;- c(&quot;2015-07-01&quot;, &quot;2015-08-01&quot;, &quot;2015-09-01&quot;) y &lt;- c(&quot;07/01/2015&quot;, &quot;08/01/2015&quot;, &quot;09/01/2015&quot;) z &lt;- c(&quot;2015-07-01 12:59:59&quot;, &quot;2015-08-01 02:45:22&quot;, &quot;2015-09-01 15:05:12&quot;) ymd(x) ## [1] &quot;2015-07-01&quot; &quot;2015-08-01&quot; &quot;2015-09-01&quot; mdy(y) ## [1] &quot;2015-07-01&quot; &quot;2015-08-01&quot; &quot;2015-09-01&quot; ymd_hms(z) ## [1] &quot;2015-07-01 12:59:59 UTC&quot; &quot;2015-08-01 02:45:22 UTC&quot; &quot;2015-09-01 15:05:12 UTC&quot; You can also use lubridate functions within dplyr functions. Say, for example, that our transaction timestamp in our data was not already a dttm date type. non_dates &lt;- transactions_sample %&gt;% mutate(date = as.character(transaction_timestamp)) glimpse(non_dates) ## Rows: 75,000 ## Columns: 12 ## $ household_id &lt;chr&gt; &quot;2261&quot;, &quot;2131&quot;, &quot;511&quot;, &quot;400&quot;, &quot;918&quot;, &quot;718&quot;, &quot;868&quot;, &quot;1688&quot;, &quot;467&quot;, &quot;1947&quot;, &quot;1694&quot;, &quot;568&quot;, … ## $ store_id &lt;chr&gt; &quot;309&quot;, &quot;368&quot;, &quot;316&quot;, &quot;388&quot;, &quot;340&quot;, &quot;324&quot;, &quot;323&quot;, &quot;450&quot;, &quot;31782&quot;, &quot;32004&quot;, &quot;446&quot;, &quot;446&quot;, &quot;… ## $ basket_id &lt;chr&gt; &quot;31625220889&quot;, &quot;32053127496&quot;, &quot;32445856036&quot;, &quot;31932241118&quot;, &quot;32074655895&quot;, &quot;32614612029&quot;,… ## $ product_id &lt;chr&gt; &quot;940996&quot;, &quot;873902&quot;, &quot;847901&quot;, &quot;13094913&quot;, &quot;1085604&quot;, &quot;883203&quot;, &quot;9884484&quot;, &quot;1028715&quot;, &quot;896… ## $ quantity &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0,… ## $ sales_value &lt;dbl&gt; 3.86, 1.59, 1.00, 11.87, 1.29, 2.50, 3.49, 2.00, 6.55, 3.99, 2.50, 3.49, 1.50, 1.00, 1.88… ## $ retail_disc &lt;dbl&gt; 0.43, 0.90, 0.69, 2.90, 0.00, 0.49, 0.00, 1.79, 4.44, 0.00, 0.49, 0.50, 0.89, 0.00, 0.12,… ## $ coupon_disc &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ coupon_match_disc &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ week &lt;int&gt; 5, 10, 13, 8, 10, 15, 10, 33, 2, 16, 47, 18, 18, 23, 37, 40, 33, 35, 43, 41, 49, 27, 16, … ## $ transaction_timestamp &lt;dttm&gt; 2017-01-28 14:06:53, 2017-02-28 22:31:57, 2017-03-26 13:22:21, 2017-02-18 13:13:10, 2017… ## $ date &lt;chr&gt; &quot;2017-01-28 14:06:53&quot;, &quot;2017-02-28 22:31:57&quot;, &quot;2017-03-26 13:22:21&quot;, &quot;2017-02-18 13:13:10… We can easily convert this to a date-time by using the appropriate parsing function: non_dates %&gt;% mutate(date = ymd_hms(date)) %&gt;% glimpse() ## Rows: 75,000 ## Columns: 12 ## $ household_id &lt;chr&gt; &quot;2261&quot;, &quot;2131&quot;, &quot;511&quot;, &quot;400&quot;, &quot;918&quot;, &quot;718&quot;, &quot;868&quot;, &quot;1688&quot;, &quot;467&quot;, &quot;1947&quot;, &quot;1694&quot;, &quot;568&quot;, … ## $ store_id &lt;chr&gt; &quot;309&quot;, &quot;368&quot;, &quot;316&quot;, &quot;388&quot;, &quot;340&quot;, &quot;324&quot;, &quot;323&quot;, &quot;450&quot;, &quot;31782&quot;, &quot;32004&quot;, &quot;446&quot;, &quot;446&quot;, &quot;… ## $ basket_id &lt;chr&gt; &quot;31625220889&quot;, &quot;32053127496&quot;, &quot;32445856036&quot;, &quot;31932241118&quot;, &quot;32074655895&quot;, &quot;32614612029&quot;,… ## $ product_id &lt;chr&gt; &quot;940996&quot;, &quot;873902&quot;, &quot;847901&quot;, &quot;13094913&quot;, &quot;1085604&quot;, &quot;883203&quot;, &quot;9884484&quot;, &quot;1028715&quot;, &quot;896… ## $ quantity &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0,… ## $ sales_value &lt;dbl&gt; 3.86, 1.59, 1.00, 11.87, 1.29, 2.50, 3.49, 2.00, 6.55, 3.99, 2.50, 3.49, 1.50, 1.00, 1.88… ## $ retail_disc &lt;dbl&gt; 0.43, 0.90, 0.69, 2.90, 0.00, 0.49, 0.00, 1.79, 4.44, 0.00, 0.49, 0.50, 0.89, 0.00, 0.12,… ## $ coupon_disc &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ coupon_match_disc &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ week &lt;int&gt; 5, 10, 13, 8, 10, 15, 10, 33, 2, 16, 47, 18, 18, 23, 37, 40, 33, 35, 43, 41, 49, 27, 16, … ## $ transaction_timestamp &lt;dttm&gt; 2017-01-28 14:06:53, 2017-02-28 22:31:57, 2017-03-26 13:22:21, 2017-02-18 13:13:10, 2017… ## $ date &lt;dttm&gt; 2017-01-28 14:06:53, 2017-02-28 22:31:57, 2017-03-26 13:22:21, 2017-02-18 13:13:10, 2017… 20.4.2 Create dates by merging data Sometimes parts of your date-time are collected in separate elements/columns. For example, in the following data that comes from a data frame supplied by the nycflights13 package, the year, month, day, hour, and minute values are all separated into their own columns. library(nycflights13) flights %&gt;% select(year, month, day, hour, minute) ## # A tibble: 336,776 × 5 ## year month day hour minute ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 5 15 ## 2 2013 1 1 5 29 ## 3 2013 1 1 5 40 ## 4 2013 1 1 5 45 ## 5 2013 1 1 6 0 ## 6 2013 1 1 5 58 ## 7 2013 1 1 6 0 ## 8 2013 1 1 6 0 ## 9 2013 1 1 6 0 ## 10 2013 1 1 6 0 ## # … with 336,766 more rows To create a date-time value from this sort of input, use make_date() for dates and make_datetime() for date-times: flights %&gt;% select(year, month, day, hour, minute) %&gt;% mutate(departure = make_datetime(year, month, day, hour, minute)) ## # A tibble: 336,776 × 6 ## year month day hour minute departure ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; ## 1 2013 1 1 5 15 2013-01-01 05:15:00 ## 2 2013 1 1 5 29 2013-01-01 05:29:00 ## 3 2013 1 1 5 40 2013-01-01 05:40:00 ## 4 2013 1 1 5 45 2013-01-01 05:45:00 ## 5 2013 1 1 6 0 2013-01-01 06:00:00 ## 6 2013 1 1 5 58 2013-01-01 05:58:00 ## 7 2013 1 1 6 0 2013-01-01 06:00:00 ## 8 2013 1 1 6 0 2013-01-01 06:00:00 ## 9 2013 1 1 6 0 2013-01-01 06:00:00 ## 10 2013 1 1 6 0 2013-01-01 06:00:00 ## # … with 336,766 more rows 20.4.3 Knowledge check Using any of the parsing functions to convert your birthday to a date object. Now save the date of your birthday into 3 separate character string variables: year, month, day and convert these three separate variables into a single date with make_datetime() 20.5 Extract &amp; manipulate parts of dates lubridate also provides several accessor function that allow you to easily extract and manipulate individual elements of a date. The available accessor functions include: Date-time component accessor functions. Date component Accessor function Year year() Month month() Week week() Day of year yday() Day of month mday() Day of week wday() Hour hour() Minute minute() Second second() Time zone tz() To extract an individual element of the date variable you simply use the accessor function desired. Note that the accessor variables have additional arguments that can be used to show the name of the date element in full or abbreviated form. x &lt;- c(&quot;2015-07-01&quot;, &quot;2015-08-01&quot;, &quot;2015-09-01&quot;) year(x) ## [1] 2015 2015 2015 # default is numerical value month(x) ## [1] 7 8 9 # show abbreviated name month(x, label = TRUE) ## [1] Jul Aug Sep ## Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; Oct &lt; Nov &lt; Dec # show unabbreviated name month(x, label = TRUE, abbr = FALSE) ## [1] July August September ## 12 Levels: January &lt; February &lt; March &lt; April &lt; May &lt; June &lt; July &lt; August &lt; September &lt; October &lt; ... &lt; December wday(x, label = TRUE, abbr = FALSE) ## [1] Wednesday Saturday Tuesday ## Levels: Sunday &lt; Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; Friday &lt; Saturday This can become handy when you want to perform quick data mining on certain aspects of data. Say we want to get the number of transactions by day for the month of December. By combining lubridate and dplyr functions we can easily filter for December transactions and count the number of transactions for each day. transactions_sample %&gt;% filter(month(transaction_timestamp) == 12) %&gt;% count(day(transaction_timestamp)) ## # A tibble: 30 × 2 ## `day(transaction_timestamp)` n ## &lt;int&gt; &lt;int&gt; ## 1 1 207 ## 2 2 258 ## 3 3 327 ## 4 4 201 ## 5 5 218 ## 6 6 195 ## 7 7 181 ## 8 8 189 ## 9 9 228 ## 10 10 272 ## # … with 20 more rows 20.5.1 Knowledge check Get the day of the week that you were born on. The default will print out a number but you can use the label parameter to get the name of the day (i.e. Sun, Mon, …, Sat). Using transactions_sample compute the total sales by the day of the week. Which day produces the largest sales? Which day produces the smallest sales? 20.6 Calculations with dates Since R stores date and time objects as numbers, this allows you to perform various calculations such as logical comparisons, computing the duration between different dates, and adding/subtracting periods (months, days, hours, minutes, etc.) to dates and times. x &lt;- today() x ## [1] &quot;2022-06-14&quot; y &lt;- ymd(&quot;2018-06-01&quot;) x &gt; y ## [1] TRUE x - y ## Time difference of 1474 days 20.6.1 Durations In R, when you subtract two dates, you get a difftime object. A difftime class object records a time span of seconds, minutes, hours, days, or weeks. This ambiguity can make difftimes a little painful to work with, so lubridate provides an alternative which always uses seconds as the duration but will summarize that duration to the nearest “useful” period (years, weeks, days). x &lt;- ymd_hm(&quot;2018-06-01 12:10&quot;) y &lt;- ymd_hm(&quot;2016-03-21 13:54&quot;) z &lt;- ymd_hm(&quot;2018-05-21 13:54&quot;) # base R difftime results x - y ## Time difference of 801.93 days x - z ## Time difference of 10.928 days # lubridate::as.duration difftime results as.duration(x - y) ## [1] &quot;69286560s (~2.2 years)&quot; as.duration(x - z) ## [1] &quot;944160s (~1.56 weeks)&quot; Working with durations can be useful when performing data mining as we can perform normal statistical procedures with durations. For example, we can compute the duration between the first and last transaction date for each household to find those households that have been shopping with us the longest. transactions_sample %&gt;% group_by(household_id) %&gt;% summarize( first_date = min(transaction_timestamp), last_date = max(transaction_timestamp) ) %&gt;% mutate(difference = as.duration(last_date - first_date)) %&gt;% arrange(desc(difference)) ## # A tibble: 2,377 × 4 ## household_id first_date last_date difference ## &lt;chr&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;Duration&gt; ## 1 1378 2017-01-01 14:04:55 2017-12-31 18:07:18 31464143s (~52.02 weeks) ## 2 58 2017-01-01 10:14:16 2017-12-31 12:46:33 31458737s (~52.02 weeks) ## 3 1899 2017-01-01 10:48:12 2017-12-31 11:01:59 31450427s (~52 weeks) ## 4 2337 2017-01-01 13:33:43 2017-12-31 13:32:48 31449545s (~52 weeks) ## 5 1130 2017-01-01 10:38:18 2017-12-31 10:35:42 31449444s (~52 weeks) ## 6 2445 2017-01-01 17:36:15 2017-12-31 17:27:35 31449080s (~52 weeks) ## 7 290 2017-01-01 16:30:07 2017-12-31 16:14:42 31448675s (~52 weeks) ## 8 304 2017-01-01 17:45:07 2017-12-31 17:24:25 31448358s (~52 weeks) ## 9 1563 2017-01-01 16:09:32 2017-12-31 15:23:24 31446832s (~52 weeks) ## 10 2194 2017-01-01 11:34:53 2017-12-31 09:22:43 31441670s (~51.99 weeks) ## # … with 2,367 more rows In addition to as.duration(), lubridate provides several duration functions (dseconds, dhours, dyears, etc.) that will convert a value to the number of seconds for the duration period of interest. dseconds(55) ## [1] &quot;55s&quot; dminutes(25) ## [1] &quot;1500s (~25 minutes)&quot; ddays(30) ## [1] &quot;2592000s (~4.29 weeks)&quot; dweeks(3) ## [1] &quot;1814400s (~3 weeks)&quot; dyears(4) ## [1] &quot;126230400s (~4 years)&quot; This makes for efficient data mining as we do not need to worry about spending time to correctly compute duration periods, we simply use the right duration function of interest. For example, we can build upon the example above and filter our transactions for only those customers that have been shopping with us for at least 25 weeks. transactions_sample %&gt;% group_by(household_id) %&gt;% summarize( first_date = min(transaction_timestamp), last_date = max(transaction_timestamp) ) %&gt;% mutate(difference = as.duration(last_date - first_date)) %&gt;% filter(difference &gt;= dweeks(25)) ## # A tibble: 1,996 × 4 ## household_id first_date last_date difference ## &lt;chr&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;Duration&gt; ## 1 1 2017-01-07 13:55:24 2017-12-14 10:13:38 29449094s (~48.69 weeks) ## 2 100 2017-02-15 18:38:13 2017-11-17 21:12:00 23769227s (~39.3 weeks) ## 3 1000 2017-01-04 18:30:59 2017-12-26 17:50:21 30755962s (~50.85 weeks) ## 4 1001 2017-02-09 18:02:42 2017-12-28 13:44:31 27805309s (~45.97 weeks) ## 5 1002 2017-03-16 12:39:10 2017-12-24 13:09:28 24456618s (~40.44 weeks) ## 6 1003 2017-03-05 17:17:40 2017-12-02 09:37:27 23473187s (~38.81 weeks) ## 7 1004 2017-01-21 01:50:10 2017-12-26 20:51:39 29358089s (~48.54 weeks) ## 8 1005 2017-01-04 18:18:18 2017-12-17 14:27:52 29966974s (~49.55 weeks) ## 9 1007 2017-01-14 20:33:42 2017-10-17 22:59:46 23851564s (~39.44 weeks) ## 10 1008 2017-03-15 06:40:12 2017-12-03 08:56:03 22734951s (~37.59 weeks) ## # … with 1,986 more rows The duration functions (dxxx()) represent an exact number of seconds and does not consider the differences due to leap year and daylight savings time adjustments. It’s important to note that the duration functions do not capture differences in time due to leap year or daylight savings time adjustments. For example, if we add two hours to the given date-time provided in one_am we go from 1am to 4am. Why? Because this is the date-time that daylight savings occurred. one_am &lt;- ymd_hms(&quot;2018-03-11 01:00:00&quot;, tz = &quot;America/New_York&quot;) one_am ## [1] &quot;2018-03-11 01:00:00 EST&quot; one_am + dhours(2) ## [1] &quot;2018-03-11 04:00:00 EDT&quot; This is where working with periods can help us. 20.6.2 Periods Periods are time spans but don’t have a fixed length in seconds, instead they work with calendar friendly time periods that account for unique instances like daylight savings time and leap years. That allows them work in a more intuitive way. Like durations, periods can be created with a number of friendly constructor functions. seconds(55) ## [1] &quot;55S&quot; minutes(25) ## [1] &quot;25M 0S&quot; days(30) ## [1] &quot;30d 0H 0M 0S&quot; weeks(3) ## [1] &quot;21d 0H 0M 0S&quot; years(4) ## [1] &quot;4y 0m 0d 0H 0M 0S&quot; When we add periods to any existing date-time, it will add calendar and/or clock “correct” time periods. For example, if we add two hours to our daylight savings time we created previously, it will result in two “normal” clock hours being added to our time even though daylight savings occurred at 2am. # daylight savings date one_am ## [1] &quot;2018-03-11 01:00:00 EST&quot; # add two hours one_am + hours(2) ## [1] &quot;2018-03-11 03:00:00 EDT&quot; 20.6.3 Knowledge check Compute the last (most recent) transaction date in our transactions_sample data. Now identify shoppers that haven’t made a transaction within 3 months of this date. 20.7 Exercises Compute the total sales_value of all transactions made Thanksgiving Day during the year of our available transactions (November 23, 2017). What is the average time of date across all transactions? Which household shops the earliest in the day (on average) and which household shops the latest? 20.8 Additional resources For additional resources on learning and dealing with dates I recommend the following: R for Data Science, Chapter 16 Dates and times made easy with lubridate Date and time classes in R "],["lab-3.html", "21 Lab", " 21 Lab TBD "],["overview-4.html", "22 Overview 22.1 Learning objectives 22.2 Tasks 22.3 Course readings", " 22 Overview Being able to create visualizations (graphical representations) of data is a key step in data analysis. In this module you will learn to use the ggplot2 library to create the meaningful, elegant, and finely tuned data visualizations. You will also learn to combine data transformation, manipulation, and visualization to explore your data and start extracting key insights. 22.1 Learning objectives By the end of this module you should be able to: Build meaning visualizations with ggplot2. Identify which plots to use to explore different types of variables. Combine descriptive statistics and visualization to identify summaries, relationships, differences, and abnormalities in the data. 22.2 Tasks TBD 22.3 Course readings TBD "],["lesson-5a-introduction-to-ggplot2.html", "23 Lesson 5a: Introduction to ggplot2 23.1 Learning objectives 23.2 Prerequisites 23.3 Grammar of Graphics 23.4 The Basics 23.5 Aesthetic Mappings 23.6 Specifying Geometric Shapes 23.7 Statistical Transformations 23.8 Position Adjustments 23.9 Managing Scales 23.10 Coordinate Systems 23.11 Facets 23.12 Labels &amp; Annotations 23.13 Exercises 23.14 Additional Resources on ggplot2", " 23 Lesson 5a: Introduction to ggplot2 Being able to create visualizations (graphical representations) of data is a key step in being able to communicate information and findings to others. In this lesson, you will learn to use the ggplot2 library to make beautiful plots or charts of your data. Although R does provide built-in plotting functions, the ggplot2 library implements the Grammar of Graphics. This makes it particularly effective for describing how visualizations should represent data, and has turned it into the preeminent plotting library in R. Learning this library will allow you to make nearly any kind of (static) data visualization, customized to your exact specifications. 23.1 Learning objectives By the end of this lesson you will be able to: Plot data using various geometric shapes and statistical transformations. Manipulate and control the aesthetics of your plots (i.e. colors, scales). Put together various ggplot parameters to produce a refined plot with a title, subtitle, proper axis titles, etc. 23.2 Prerequisites To reproduce the code throughout this lesson you will need to load the ggplot2 package along with a couple others. library(ggplot2) # for plotting capabilities library(completejourney) # for data library(dplyr) # for additional data wrangling This lesson will also use the Complete Journey data; but before we start we’ll join the transactions data with the products and household demographics data: df &lt;- transactions_sample %&gt;% inner_join(products) %&gt;% inner_join(demographics) glimpse(df) ## Rows: 42,058 ## Columns: 24 ## $ household_id &lt;chr&gt; &quot;400&quot;, &quot;718&quot;, &quot;868&quot;, &quot;1694&quot;, &quot;2154&quot;, &quot;1631&quot;, &quot;2194&quot;, &quot;432&quot;, &quot;553&quot;, &quot;762&quot;, &quot;2198&quot;, &quot;306&quot;, … ## $ store_id &lt;chr&gt; &quot;388&quot;, &quot;324&quot;, &quot;323&quot;, &quot;446&quot;, &quot;343&quot;, &quot;293&quot;, &quot;32004&quot;, &quot;368&quot;, &quot;447&quot;, &quot;31862&quot;, &quot;343&quot;, &quot;382&quot;, &quot;… ## $ basket_id &lt;chr&gt; &quot;31932241118&quot;, &quot;32614612029&quot;, &quot;32074722463&quot;, &quot;40715237008&quot;, &quot;32873345772&quot;, &quot;35730415119&quot;,… ## $ product_id &lt;chr&gt; &quot;13094913&quot;, &quot;883203&quot;, &quot;9884484&quot;, &quot;989069&quot;, &quot;1064213&quot;, &quot;1108168&quot;, &quot;904774&quot;, &quot;1041476&quot;, &quot;11… ## $ quantity &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ sales_value &lt;dbl&gt; 11.87, 2.50, 3.49, 2.50, 1.50, 1.88, 1.00, 1.00, 1.60, 1.20, 1.99, 0.67, 3.19, 0.00, 2.99… ## $ retail_disc &lt;dbl&gt; 2.90, 0.49, 0.00, 0.49, 0.89, 0.12, 0.39, 0.00, 0.00, 0.38, 0.00, 0.71, 0.00, 0.00, 3.10,… ## $ coupon_disc &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,… ## $ coupon_match_disc &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,… ## $ week &lt;int&gt; 8, 15, 10, 47, 18, 37, 40, 33, 35, 43, 41, 27, 25, 3, 23, 2, 8, 30, 35, 16, 50, 42, 19, 3… ## $ transaction_timestamp &lt;dttm&gt; 2017-02-18 13:13:10, 2017-04-05 18:14:17, 2017-03-02 17:45:37, 2017-11-13 17:41:04, 2017… ## $ manufacturer_id &lt;chr&gt; &quot;4421&quot;, &quot;5072&quot;, &quot;1102&quot;, &quot;69&quot;, &quot;972&quot;, &quot;69&quot;, &quot;69&quot;, &quot;608&quot;, &quot;69&quot;, &quot;759&quot;, &quot;1720&quot;, &quot;1046&quot;, &quot;162… ## $ department &lt;chr&gt; &quot;MEAT&quot;, &quot;DRUG GM&quot;, &quot;GROCERY&quot;, &quot;MEAT-PCKGD&quot;, &quot;GROCERY&quot;, &quot;GROCERY&quot;, &quot;GROCERY&quot;, &quot;GROCERY&quot;, &quot;… ## $ brand &lt;fct&gt; National, National, National, Private, National, Private, Private, National, Private, Nat… ## $ product_category &lt;chr&gt; &quot;BEEF&quot;, &quot;FIRST AID PRODUCTS&quot;, &quot;BAKED SWEET GOODS&quot;, &quot;BREAKFAST SAUSAGE/SANDWICHES&quot;, &quot;BAG S… ## $ product_type &lt;chr&gt; &quot;ANGUS BEEF&quot;, &quot;BANDAGE/TAPE&quot;, &quot;SNACK CAKE - MULTI PACK&quot;, &quot;LINKS - RAW&quot;, &quot;BAGGED POPPED PO… ## $ package_size &lt;chr&gt; NA, NA, &quot;8 OZ&quot;, &quot;12 OZ&quot;, &quot;10 OZ&quot;, &quot;12 PK&quot;, &quot;16 OZ&quot;, &quot;10.25 OZ&quot;, &quot;8 OZ&quot;, &quot;6 OZ&quot;, NA, &quot;.3 O… ## $ age &lt;ord&gt; 35-44, 45-54, 65+, 35-44, 45-54, 19-24, 55-64, 19-24, 45-54, 35-44, 35-44, 19-24, 25-34, … ## $ income &lt;ord&gt; 150-174K, 25-34K, 35-49K, 15-24K, 35-49K, Under 15K, 25-34K, 25-34K, 75-99K, 125-149K, 25… ## $ home_ownership &lt;ord&gt; Homeowner, Homeowner, Homeowner, Probable Renter, Homeowner, Renter, Homeowner, NA, Homeo… ## $ marital_status &lt;ord&gt; Married, Married, Married, Unmarried, Married, Unmarried, Unmarried, Unmarried, NA, Marri… ## $ household_size &lt;ord&gt; 3, 5+, 2, 1, 2, 4, 1, 1, 2, 2, 1, 4, 2, 1, 4, 2, 1, 5+, 1, 2, 2, 3, 1, 3, 2, 2, 1, 4, 2, … ## $ household_comp &lt;ord&gt; 2 Adults Kids, 2 Adults Kids, 2 Adults No Kids, 1 Adult No Kids, 2 Adults No Kids, 1 Adul… ## $ kids_count &lt;ord&gt; 1, 3+, 0, 0, 0, 3+, 0, 0, 0, 1, 0, 2, 1, 0, 2, 0, 0, 3+, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 1,… 23.3 Grammar of Graphics Just as the grammar of language helps us construct meaningful sentences out of words, the Grammar of Graphics helps us to construct graphical figures out of different visual elements. This grammar gives us a way to talk about parts of a plot: all the circles, lines, arrows, and words that are combined into a diagram for visualizing data. Originally developed by Leland Wilkinson, the Grammar of Graphics was adapted by Hadley Wickham to describe the components of a plot, including the data being plotted the geometric objects (circles, lines, etc.) that appear on the plot a set of mappings from variables in the data to the aesthetics (appearance) of the geometric objects a statistical transformation used to calculate the data values used in the plot a position adjustment for locating each geometric object on the plot a scale (e.g., range of values) for each aesthetic mapping used a coordinate system used to organize the geometric objects the facets or groups of data shown in different plots Wickham further organizes these components into layers, where each layer has a single geometric object, statistical transformation, and position adjustment. Following this grammar, you can think of each plot as a set of layers of images, where each image’s appearance is based on some aspect of the data set. All together, this grammar enables us to discuss what plots look like using a standard set of vocabulary. And similar to how tidyr and dplyr provide efficient data transformation and manipulation, ggplot2 provides more efficient ways to create specific visual images. 23.4 The Basics In order to create a plot, you: Call the ggplot() function which creates a blank canvas Specify aesthetic mappings, which specifies how you want to map variables to visual aspects. In this case we are simply mapping the quantity and sales_value variables to the x- and y-axes. You then add new layers that are geometric objects which will show up on the plot. In this case we add geom_point to add a layer with points (dots) elements as the geometric shapes to represent the data. # create canvas (left) ggplot(df) # variables of interest mapped (middle) ggplot(df, aes(x = quantity, y = sales_value)) # data plotted (right) ggplot(df, aes(x = quantity, y = sales_value)) + geom_point() When you add the geom layer you use the addition (+) operator. As you add new layers you will always use + to add onto your visualization. Just as we’ve seen in the previous lessons, we can continue to use the pipe operator with ggplot. For example, say we want to only plot transactions where the quantity of items purchased was over 100. We can simply apply a filter() statement and then pipe the results directly into ggplot. Just remember to use the + operator to add new layers to ggplot. It is very common for people to pipe into ggplot and then continue to use the %&gt;% operator to add new ggplot layers. This will result in an error! df %&gt;% filter(quantity &gt; 100) %&gt;% ggplot(aes(x = quantity, y = sales_value)) + geom_point() 23.4.1 Knowledge check Filter for all transactions in the produce department and plot the quantity on the x-axis and sales_value on the y-axis. Filter for all transactions in the meat departments and plot the quantity on the x-axis and sales_value on the y-axis. Filter for all transactions in the fuel departments and plot the quantity on the x-axis and sales_value on the y-axis. What trends do you notice between the above departments? 23.5 Aesthetic Mappings The aesthetic mappings take properties of the data and use them to influence visual characteristics, such as position, color, size, shape, or transparency. Each visual characteristic can thus encode an aspect of the data and be used to convey information. All aesthetics for a plot are specified in the aes() function call (later in this lesson you will see that each geom layer can have its own aes specification). For example, we can add a mapping from the variable household_size to a color characteristic: df %&gt;% ggplot(aes(x = quantity, y = sales_value, color = household_size)) + geom_point() Note that using the aes() function will cause the visual channel to be based on the data specified in the argument. For example, using aes(color = \"blue\") won’t cause the geometry’s color to be “blue”, but will instead cause the visual channel to be mapped from the vector c(\"blue\") — as if we only had a single type of engine that happened to be called “blue”. If you wish to apply an aesthetic property to an entire geometry, you can set that property as an argument within the geom_xxx_() method, outside of the aes() call: ggplot(df, aes(x = quantity, y = sales_value)) + geom_point(color = &quot;blue&quot;) 23.5.1 Knowledge check Filter for all transactions in the grocery department (department == “GROCERY”) and plot the quantity on the x-axis and sales_value on the y-axis. Color all the points in the above plot red. Now color the points in #1 based on the household_size variable. Use size to change the size of all points to 0.5. Now map the household_size variable to size within aes(). 23.6 Specifying Geometric Shapes Building on these basics, ggplot2 can be used to build almost any kind of plot you may want. These plots are declared using functions that follow from the Grammar of Graphics. The most obvious distinction between plots is what geometric objects (geoms) they include. ggplot2 supports a number of different types of geoms, including: geom_point for drawing individual points (e.g., a scatter plot) geom_line for drawing lines (e.g., for a line charts) geom_smooth for drawing smoothed lines (e.g., for simple trends or approximations) geom_bar for drawing bars (e.g., for bar charts) geom_histogram for drawing binned values (e.g. a histogram) geom_polygon for drawing arbitrary shapes geom_map for drawing polygons in the shape of a map! (You can access the data to use for these maps by using the map_data() function). Each of these geometries will leverage the aesthetic mappings supplied although the specific visual properties that the data will map to will vary. For example, you can map data to the shape of a geom_point (e.g., if they should be circles or squares), or you can map data to the linetype of a geom_line (e.g., if it is solid or dotted), but not vice versa. Almost all geoms require an x and y mapping at the bare minimum. # Left column: x and y mapping needed! ggplot(df, aes(x = quantity, y = sales_value)) + geom_point() ggplot(df, aes(x = quantity, y = sales_value)) + geom_smooth() # Right column: no y mapping needed! ggplot(df, aes(x = household_size)) + geom_bar() ggplot(df, aes(x = sales_value)) + geom_histogram() What makes this really powerful is that you can add multiple geometries to a plot, thus allowing you to create complex graphics showing multiple aspects of your data. # plot with both points and smoothed line ggplot(df, aes(x = quantity, y = sales_value)) + geom_point() + geom_smooth() Of course the aesthetics for each geom can be different, so you could show multiple lines on the same plot (or with different colors, styles, etc). It’s also possible to give each geom a different data argument, so that you can show multiple data sets in the same plot. For example, we can plot both points and a smoothed line for the same x and y variable but specify unique colors within each geom: ggplot(df, aes(x = quantity, y = sales_value)) + geom_point(color = &quot;blue&quot;) + geom_smooth(color = &quot;red&quot;) So as you can see if we specify an aesthetic within ggplot it will be passed on to each geom that follows. Or we can specify certain aesthetics within each geom, which allows us to only show certain characteristics for that specific layer (i.e. geom_point). # color aesthetic passed to each geom layer (left) ggplot(df, aes(x = quantity, y = sales_value, color = household_size)) + geom_point() + geom_smooth(se = FALSE) # color aesthetic specified for only the geom_point layer (right) ggplot(df, aes(x = quantity, y = sales_value)) + geom_point(aes(color = household_size)) + geom_smooth(se = FALSE) 23.6.1 Knowledge check Filter all transactions where department == “PRODUCE”. Use a boxplot to plot the sales_value across each household composition (household_comp). Does it look like the household composition impacts the amount spent on produce? Using the same filtered data as in #1, create a scatter plot to assess quantity versus sales_value and color the points based on household composition. Plot a histogram of the retail_discount values based on all transactions. 23.7 Statistical Transformations If you look at the bar chart below displaying the frequency of store IDs, you’ll notice that the the y axis was defined for us as the count of observations for each household_size. This count isn’t part of the data set (it’s not a column in the df data set), but is instead a statistical transformation that the geom_bar automatically applies to the data. In particular, it applies the stat_count transformation. ggplot(df, aes(x = household_size)) + geom_bar() ggplot2 supports many different statistical transformations. For example, the “identity” transformation will leave the data “as is”. You can specify which statistical transformation a geom uses by passing it as the stat argument. For example, consider our data already had the count as a variable: household_size_count &lt;- count(df, household_size) household_size_count ## # A tibble: 5 × 2 ## household_size n ## &lt;ord&gt; &lt;int&gt; ## 1 1 12433 ## 2 2 16148 ## 3 3 6280 ## 4 4 3461 ## 5 5+ 3736 We can use stat = \"identity\" within geom_bar to plot our bar height values to this variable. Also, note that we now include n for our y variable: ggplot(household_size_count, aes(x = household_size, y = n)) + geom_bar(stat = &quot;identity&quot;) We can also call stat_ functions directly to add additional layers. For example, below is a graph that visualizes the relationship between the total sales value for each basket (aka checkout) across the different income levels. We create a scatterplot of total sales value for each for each basket and then use stat_summary to plot the median total sales value. It appears that there is a slight increase in median checkout sales value as income increases. df %&gt;% group_by(income, basket_id) %&gt;% summarize(total_sale = sum(sales_value)) %&gt;% ggplot(aes(income, total_sale)) + geom_point(color = &quot;blue&quot;) + stat_summary(fun = &quot;median&quot;, geom = &quot;point&quot;, color = &quot;red&quot;, size = 2) 23.7.1 Knowledge check Filter our transactions for only those items in the meat department and use geom_bar() to plot the number of transactions for each product_category. Filter our transactions for only those items in the meat department and you did above but now compute the number of transactions for each product_category using filter::count(). Use geom_count() to create the same plot as you got in #1. Create a scatter plot comparing retail_disc to sales_value. Now, instead of using geom_point() use stat_summary_bin(fun = “mean”, geom = “bar”, orientation = ‘y’). What does this tell you about the relationship between retail_disc and sales_value? 23.8 Position Adjustments In addition to a default statistical transformation, each geom also has a default position adjustment which specifies a set of “rules” as to how different components should be positioned relative to each other. This position is noticeable in a geom_bar if you map a different variable to the color visual characteristic. Below is a bar chart that uses color to visualize the proportion of brand purchases (Private represents the stores special private brand and National represents commonly available brands). ggplot(df, aes(x = household_size, fill = brand)) + geom_bar() The geom_bar by default uses a position adjustment of \"stack\", which makes each rectangle’s height proprotional to its value and stacks them on top of each other. We can use the position argument to specify what position adjustment rules to follow: # position = &quot;dodge&quot;: values next to each other ggplot(df, aes(x = household_size, fill = brand)) + geom_bar(position = &quot;dodge&quot;) # position = &quot;fill&quot;: percentage chart ggplot(df, aes(x = household_size, fill = brand)) + geom_bar(position = &quot;fill&quot;) Check the documentation for each particular geom to learn more about its positioning adjustments. 23.8.1 Knowledge check Create a bar plot that visualizes the number of transactions across all weeks. Now add a fill parameter to assess if the ratio of brand was consistent across these weeks. 23.9 Managing Scales Whenever you specify an aesthetic mapping, ggplot uses a particular scale to determine the range of values that the data should map to. Thus when you specify # color the data by household_size ggplot(df, aes(x = quantity, y = sales_value, color = household_size)) + geom_point() ggplot automatically adds a scale for each mapping to the plot: # same as above, with explicit scales ggplot(df, aes(x = quantity, y = sales_value, color = household_size)) + geom_point() scale_x_continuous() + scale_y_continuous() + scale_colour_discrete() Each scale can be represented by a function with the following name: scale_, followed by the name of the aesthetic property, followed by an _ and the name of the scale. A continuous scale will handle things like numeric data (where there is a continuous set of numbers), whereas a discrete scale will handle things like colors (since there is a small list of distinct colors). If you are trying to use a continuous scale, you must make sure the data is in numeric format and not as a factor or character. While the default scales will work fine, it is possible to explicitly add different scales to replace the defaults. For example, you can use a scale to change the direction of an axis: # total area size and spend relationship, ordered in reverse ggplot(df, aes(x = quantity, y = sales_value)) + geom_point() + scale_x_reverse() + scale_y_reverse() Similarly, you can use scale_x_log10() and scale_x_sqrt() to transform your scale. You can also use scales to format your axes. For example, you can make the y-axis have a percent scale. ggplot(df, aes(x = household_size, fill = brand)) + geom_bar(position = &quot;fill&quot;) + scale_y_continuous(breaks = seq(0, 1, by = .2), labels = scales::percent) A common parameter to change is which set of colors to use in a plot. While you can use the default coloring, a more common option is to leverage the pre-defined palettes from colorbrewer.org. These color sets have been carefully designed to look good and to be viewable to people with certain forms of color blindness. We can leverage color brewer palletes by specifying the scale_color_brewer() function, passing the pallete as an argument. # left: default color brewer ggplot(df, aes(x = quantity, y = sales_value, color = household_size)) + geom_point() + scale_color_brewer() # right: specifying color palette ggplot(df, aes(x = quantity, y = sales_value, color = household_size)) + geom_point() + scale_color_brewer(palette = &quot;Set3&quot;) Note that you can get the palette name from the colorbrewer website by looking at the scheme query parameter in the URL. Or see the diagram here and hover the mouse over each palette for the name. You can also specify continuous color values by using a gradient scale, or manually specify the colors you want to use as a named vector. 23.9.1 Knowledge check Create a scatter plot that plots quantity vs sales_value across all transactions. View this plot. Now transform the x-axis to a log transformation using scale_x_log10(). What new insights does this provide? Now add labels = scales::comma inside scale_x_log10(). How does this change the formatting? Create a scatter plot that plots sales_value vs retail_disc across all transactions. View this plot. Now transform the x-axis and y-axis to a log transformation using scale_x_log10() and scale_y_log10(). What new insights does this provide? Can you figure out how to change the x and y-axes to be formatted as dollar units? 23.10 Coordinate Systems The next term from the Grammar of Graphics that can be specified is the coordinate system. As with scales, coordinate systems are specified with functions that all start with coord_ and are added as a layer. There are a number of different possible coordinate systems to use, including: coord_cartesian the default cartesian coordinate system, where you specify x and y values (e.g. allows you to zoom in or out). coord_flip a cartesian system with the x and y flipped coord_fixed a cartesian system with a “fixed” aspect ratio (e.g., 1.78 for a “widescreen” plot) coord_polar a plot using polar coordinates coord_quickmap a coordinate system that approximates a good aspect ratio for maps. See documentation for more details. # zoom in with coord_cartesian (left) ggplot(df, aes(x = quantity, y = sales_value)) + geom_point() + coord_cartesian(xlim = c(0, 10000), ylim = c(0, 50)) # flip x and y axis with coord_flip (right) ggplot(df, aes(x = household_size)) + geom_bar() + coord_flip() 23.10.1 Knowledge check Produce a regular bar plot based on the department variable. Now change this plot from a vertical bar plot to a horizontal bar plot using coord_flip(). 23.11 Facets Facets are ways of grouping a data plot into multiple different pieces (subplots). This allows you to view a separate plot for each value in a categorical variable. You can construct a plot with multiple facets by using the facet_wrap() function. This will produce a “row” of subplots, one for each categorical variable (the number of rows can be specified with an additional argument): ggplot(df, aes(x = quantity, y = sales_value)) + geom_point() + facet_grid(~ marital_status) You can also use facet_grid to facet your data by more than one categorical variable. Note that we use a tilde (~) in our facet functions. With facet_grid the variable to the left of the tilde will be represented in the rows and the variable to the right will be represented across the columns. ggplot(df, aes(x = quantity, y = sales_value)) + geom_point() + facet_grid(household_size ~ marital_status) 23.11.1 Knowledge check Filter for all transactions in the meat department and create a scatter plot comparing quantity to sales_value. Build onto the plot in #1 by faceting by product_category using facet_wrap(~product_category). What does this plot tell you? Build onto the plot in #1 by faceting by product_category and brand using facet_grid(product_category~brand). What does this plot tell you? What’s the difference between facet_wrap and facet_grid? 23.12 Labels &amp; Annotations Textual labels and annotations (on the plot, axes, geometry, and legend) are an important part of making a plot understandable and communicating information. Although not an explicit part of the Grammar of Graphics (the would be considered a form of geometry), ggplot makes it easy to add such annotations. You can add titles and axis labels to a chart using the labs() function (not labels, which is a different R function!): hshld_by_basket &lt;- df %&gt;% group_by(basket_id, household_size) %&gt;% summarize(quantity = sum(quantity), sales_value = sum(sales_value)) ggplot(hshld_by_basket, aes(x = quantity, y = sales_value, color = household_size)) + geom_point() + labs(title = &quot;Customer Net Spend by Basket&quot;, subtitle = &quot;Household transaction data covering 2016-2017.&quot;, x = &quot;Quantity of items in each basket&quot;, y = &quot;Total net spend per basket ($)&quot;, color = &quot;Household size&quot;) It is also possible to add labels into the plot itself (e.g., to label each point or line) by adding a new geom_text or geom_label to the plot; effectively, you’re plotting an extra set of data which happen to be the variable names: top_baskets &lt;- hshld_by_basket %&gt;% ungroup() %&gt;% slice_max(sales_value, n = 10) ggplot(hshld_by_basket, aes(x = quantity, y = sales_value)) + geom_point(aes(color = household_size)) + geom_label(data = top_baskets, aes(label = basket_id)) However, note that many labels overlap one-another in the center of the plot and some labels hang off the page. We can use the geom_label_repel function from the ggrepel package to help position labels. library(ggrepel) ggplot(hshld_by_basket, aes(x = quantity, y = sales_value)) + geom_point(aes(color = household_size)) + geom_label_repel(data = top_baskets, aes(label = basket_id)) 23.12.1 Knowledge check # exercise data hshld_df &lt;- df %&gt;% group_by(household_id) %&gt;% summarize( total_sales = sum(sales_value), total_quantity = sum(quantity) ) %&gt;% mutate(dollar_per_item = total_sales / total_quantity) Using the data above: Create a bar plot of the top 10 households that spend the most per item purchased? Refine this plot by adding a title and better x and y-axis names. Create a scatter plot that compares total_quantity to total_sales. Log transform the x and y axis. Refine this plot by adding a title and better x and y-axis names. 23.13 Exercises Identify all different products that contain “pizza” in their product_type description. Use a bar plot to assess whether married versus unmarried customers purchase more pizza products. Create a scatter plot to assess the quantity versus sales_value of pizza products sold. Now facet the scatter plot in #3 by product_category. Do the results surprise you? 23.14 Additional Resources on ggplot2 This gets you started with ggplot2; however, this a lot more to learn. The following resources provide additional avenues to learn more: gglot2 Documentation (particularly the function reference) ggplot2 Cheat Sheet (see also here) Data Visualization portion of R for Data Science Book ggplot2 book A Layered Grammar of Graphics ggplot2 extensions that will give you even more capabilities such as interactive plots, mapping, etc! "],["lesson-5b-handling-factors.html", "24 Lesson 5b: Handling factors 24.1 Learning objectives 24.2 Prerequisites 24.3 Creating factors &amp; inspecting factors 24.4 Modifying factor order 24.5 Modifying factor levels 24.6 Exercises 24.7 Additional Resources", " 24 Lesson 5b: Handling factors So far we’ve learned to work with several different data types such as strings and dates. However, one data type we haven’t discussed much are factors. Factors are used to represent categorical data and can be unordered or ordered. One can think of a factor as an integer vector where each integer has a label. In fact, factors are built on top of integer vectors using two attributes: the class(), “factor”, which makes them behave differently from regular integer vectors, and the levels(), which defines the set of allowed values. Factors are important in statistical modeling and can be treated specially depending on the model used. Moreover, factors can influence how visualizations are displayed which is why we are introducing this lesson during this module. Consequently, understanding how to manage factors is important and this lesson will provide you the basics of managing categorical data as factors. 24.1 Learning objectives By the end of this lesson you will be able to: Describe what factors are and why we use them. Create factors from character strings. Modify factor levels and orders to help in your data wrangling. 24.2 Prerequisites For this lesson you will need to load the tidyverse suite of packages. The primary emphasis will be on the forcats package which is automatically loaded when you load tidyverse. forcats provides tools for dealing with categorical variables (and it’s an anagram of factors!). library(ggplot2) # for plotting capabilities library(completejourney) # for data library(dplyr) # for additional data wrangling 24.3 Creating factors &amp; inspecting factors 24.3.1 Some basics Imagine that you have a variable that records month: m1 &lt;- c(&quot;Aug&quot;, &quot;Oct&quot;, &quot;Jan&quot;, &quot;Mar&quot;) Using a string to record this variable has two problems: There are only twelve possible months, and there’s nothing saving you from typos: m2 &lt;- c(&quot;Aug&quot;, &quot;Oct&quot;, &quot;Jam&quot;, &quot;Mar&quot;) It doesn’t sort in a useful way: sort(m1) ## [1] &quot;Aug&quot; &quot;Jan&quot; &quot;Mar&quot; &quot;Oct&quot; You can fix both of these problems with a factor. To create a factor you must start by creating a list of the valid levels: month_levels &lt;- c( &quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot; ) Now you can create a factor: (my_months &lt;- factor(m1, levels = month_levels)) ## [1] Aug Oct Jan Mar ## Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec sort(my_months) ## [1] Jan Mar Aug Oct ## Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec And any values not in the set will be silently converted to NA: # misspelled month is forced to NA factor(m2, levels = month_levels) ## [1] Aug Oct &lt;NA&gt; Mar ## Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec We can use class() as we have in the past to verify the vector is a factor and use levels to extract the levels of the factor: class(my_months) ## [1] &quot;factor&quot; levels(my_months) ## [1] &quot;Jan&quot; &quot;Feb&quot; &quot;Mar&quot; &quot;Apr&quot; &quot;May&quot; &quot;Jun&quot; &quot;Jul&quot; &quot;Aug&quot; &quot;Sep&quot; &quot;Oct&quot; &quot;Nov&quot; &quot;Dec&quot; 24.3.2 Factors in data frames When factors are stored in a tibble, you will notice them by the &lt;fct&gt; heading; however, you can’t see their levels. One way to see them is with count(): promotions_sample %&gt;% count(display_location) ## # A tibble: 10 × 2 ## display_location n ## &lt;fct&gt; &lt;int&gt; ## 1 0 205534 ## 2 1 4266 ## 3 2 16396 ## 4 3 14902 ## 5 4 6615 ## 6 5 34216 ## 7 6 15207 ## 8 7 9689 ## 9 9 51368 ## 10 A 2342 When we plot a factor variable ggplot will plot the values in the order of the factor: ggplot(promotions_sample, aes(display_location)) + geom_bar() By default, ggplot2 will drop levels that don’t have any values… promotions_sample %&gt;% count(display_location) %&gt;% filter(n &gt; 5000) %&gt;% ggplot(aes(display_location, n)) + geom_col() … but you can force them to display with scale_x_discrete(drop = FALSE): promotions_sample %&gt;% count(display_location) %&gt;% filter(n &gt; 5000) %&gt;% ggplot(aes(display_location, n)) + geom_col() + scale_x_discrete(drop = FALSE) 24.3.3 Ordinal factors Sometimes factors will be ordinal. You will notice this by the &lt;ord&gt; description in a tibble: demographics ## # A tibble: 801 × 8 ## household_id age income home_ownership marital_status household_size household_comp kids_count ## &lt;chr&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; ## 1 1 65+ 35-49K Homeowner Married 2 2 Adults No Kids 0 ## 2 1001 45-54 50-74K Homeowner Unmarried 1 1 Adult No Kids 0 ## 3 1003 35-44 25-34K &lt;NA&gt; Unmarried 1 1 Adult No Kids 0 ## 4 1004 25-34 15-24K &lt;NA&gt; Unmarried 1 1 Adult No Kids 0 ## 5 101 45-54 Under 15K Homeowner Married 4 2 Adults Kids 2 ## 6 1012 35-44 35-49K &lt;NA&gt; Married 5+ 2 Adults Kids 3+ ## 7 1014 45-54 15-24K &lt;NA&gt; Married 4 2 Adults Kids 2 ## 8 1015 45-54 50-74K Homeowner Unmarried 1 1 Adult No Kids 0 ## 9 1018 45-54 35-49K Homeowner Married 5+ 2 Adults Kids 3+ ## 10 1020 45-54 25-34K Homeowner Married 2 2 Adults No Kids 0 ## # … with 791 more rows And we can verify with class(): class(demographics$age) ## [1] &quot;ordered&quot; &quot;factor&quot; Ordinal factors are pretty much the same as regular factors, its just an explicit way of saying that this factor has levels that should retain an order. We can probably agree that a factor for colors has no order. There is no inherent order of our colors. Consequently, the levels of this factor will simply be based on the alphabetical order of discrete values in the colors vector: colors &lt;- c(&#39;blue&#39;, &#39;green&#39;, &#39;blue&#39;, &#39;yellow&#39;, &#39;blue&#39;, &#39;green&#39;) factor(colors) ## [1] blue green blue yellow blue green ## Levels: blue green yellow Or we could specify the levels explicitly: options &lt;- c(&#39;blue&#39;, &#39;red&#39;, &#39;yellow&#39;, &#39;green&#39;) factor(colors, levels = options) ## [1] blue green blue yellow blue green ## Levels: blue red yellow green However, some factors do have inherent ordering. For example, say we have a vector of the sizes of a container: sizes &lt;- c(&#39;small&#39;, &#39;large&#39;, &#39;large&#39;, &#39;medium&#39;, &#39;small&#39;, &#39;large&#39;) factor(sizes) ## [1] small large large medium small large ## Levels: large medium small In this case, we may want to ensure that our levels follow an explicit ordinal hierarchy: options &lt;- c(&#39;small&#39;, &#39;medium&#39;, &#39;large&#39;) factor(sizes, levels = options, ordered = TRUE) ## [1] small large large medium small large ## Levels: small &lt; medium &lt; large This ordinal nature typically does not have a large impact in exploratory data analysis but as we progress into statistical modeling and machine learning it does play a bigger role. 24.3.4 Knowledge check What kind of factor is the home_ownership variable in the demographics data? What are the levels of this factor? Are all levels represented in this data? In other words, is there at least one or more observations for each level? Create a bar plot for this variable where all levels are illustrated in the plot. 24.4 Modifying factor order It’s often useful to change the order of the factor levels. This is most often beneficial for visualizations but can also become useful when dealing with machine learning algorithm such as one of the generalized linear regression family models where the first level of a factor becomes the default baseline. As an example, say we wish to create a bar chart of homeownership frequency. By default, the bars will be plotted in the order of the levels; however, if we wish to change the order of the levels with explicit values than we can use fct_relevel inside of the mutate function to change the levels. p1 &lt;- ggplot(demographics, aes(home_ownership)) + geom_bar() + ggtitle(&#39;Original order&#39;) + coord_flip() new_levels &lt;- c(&quot;Unknown&quot;, &quot;Probable Renter&quot;, &quot;Probable Homeowner&quot;, &quot;Renter&quot;, &quot;Homeowner&quot;) p2 &lt;- demographics %&gt;% mutate(home_ownership = fct_relevel(home_ownership, levels = new_levels)) %&gt;% ggplot(aes(home_ownership)) + geom_bar() + ggtitle(&#39;New order&#39;) + coord_flip() gridExtra::grid.arrange(p1, p2, nrow = 1) A similar function is fct_reorder. Consider the following chart which plots the top 20 levels of prod_desc with the largest average net_spend_amt. spend_by_dept &lt;- transactions_sample %&gt;% inner_join(products, by = &quot;product_id&quot;) %&gt;% group_by(department) %&gt;% summarize(total_spend = sum(sales_value)) ggplot(spend_by_dept, aes(total_spend, department)) + geom_point() In the above chart, there is no natural order of the levels; rather, we wish to communicate those categories with the largest values in a rank-order fashion. To highlight the pattern in this chart we can improve it by reordering the levels of department based on the values of total_spend using fct_reorder(). ggplot(spend_by_dept, aes(total_spend, fct_reorder(department, total_spend))) + geom_point() When reording factor levels for plots its best to do it within the ggplot call. When you want to reorder factor levels more permanently it is best to do it within mutate. 24.4.1 Knowledge check Create a bar plot for the age variable in the demographics data. Now create a bar plot where the bars are ordered based on the count of observations and not the original factor level order. 24.5 Modifying factor levels Sometimes you need to do more than just reorder factors. This may include recoding values, combining different levels, and dropping unused levels. Consider our total sales by department. Let’s coerce our department’s to a factor. spend_by_dept &lt;- mutate(spend_by_dept, department = factor(department)) spend_by_dept ## # A tibble: 26 × 2 ## department total_spend ## &lt;fct&gt; &lt;dbl&gt; ## 1 AUTOMOTIVE 12.6 ## 2 CHEF SHOPPE 73.9 ## 3 CNTRL/STORE SUP 1 ## 4 COSMETICS 1087. ## 5 COUPON 30.2 ## 6 DELI 7838. ## 7 DRUG GM 29563. ## 8 FLORAL 1231. ## 9 FROZEN GROCERY 14.8 ## 10 FUEL 16529. ## # … with 16 more rows If we wanted to rename categories we can use fct_recode. This allows us to rename one or more factor levels at a time. spend_by_dept %&gt;% mutate(department = fct_recode( department, &quot;STORE SUPPLIES&quot; = &quot;CNTRL/STORE SUP&quot;, &quot;OVER COUNTER PHARMA&quot; = &quot;DRUG GM&quot; )) %&gt;% ggplot(aes(total_spend, fct_reorder(department, total_spend))) + geom_point() fct_recode will leave levels that aren’t explicitly mentioned as is, and will warn you if you accidentally refer to a level that doesn’t exist. To combine groups, you can assign multiple old levels to the same new level: spend_by_dept %&gt;% mutate(department = fct_recode( department, &quot;MEAT &amp; SEAFOOD&quot; = &quot;MEAT&quot;, &quot;MEAT &amp; SEAFOOD&quot; = &quot;SEAFOOD&quot; )) ## # A tibble: 26 × 2 ## department total_spend ## &lt;fct&gt; &lt;dbl&gt; ## 1 AUTOMOTIVE 12.6 ## 2 CHEF SHOPPE 73.9 ## 3 CNTRL/STORE SUP 1 ## 4 COSMETICS 1087. ## 5 COUPON 30.2 ## 6 DELI 7838. ## 7 DRUG GM 29563. ## 8 FLORAL 1231. ## 9 FROZEN GROCERY 14.8 ## 10 FUEL 16529. ## # … with 16 more rows If you want to collapse a lot of levels, fct_collapse is a useful variant of fct_recode. For each new variable, you can provide a vector of old levels. spend_by_dept %&gt;% mutate(department = fct_collapse( department, &quot;MEAT &amp; SEAFOOD&quot; = c(&quot;MEAT&quot;, &quot;MEAT-PCKGD&quot;, &quot;SEAFOOD&quot;, &quot;SEAFOOD-PCKGD&quot;) )) ## # A tibble: 26 × 2 ## department total_spend ## &lt;fct&gt; &lt;dbl&gt; ## 1 AUTOMOTIVE 12.6 ## 2 CHEF SHOPPE 73.9 ## 3 CNTRL/STORE SUP 1 ## 4 COSMETICS 1087. ## 5 COUPON 30.2 ## 6 DELI 7838. ## 7 DRUG GM 29563. ## 8 FLORAL 1231. ## 9 FROZEN GROCERY 14.8 ## 10 FUEL 16529. ## # … with 16 more rows Sometimes you just want to lump together all the small groups to make a plot or table simpler. That’s the job of fct_lump(), which progressively lumps together the smallest groups until it reaches the specified number of desired groups (n). transactions_sample %&gt;% inner_join(products, by = &quot;product_id&quot;) %&gt;% mutate(department = fct_lump(department, n = 10)) %&gt;% group_by(department) %&gt;% summarize(total_spend = sum(sales_value)) %&gt;% ggplot(aes(total_spend, fct_reorder(department, total_spend))) + geom_point() 24.5.1 Knowledge check Create a bar plot of the income variable in demographics. Now recode the levels of the income variable so that you have 3 income levels: Low income = &lt; $50K income Middle income = between $50-125K Upper income = &gt; $125K 24.6 Exercises Filter the products data for “BREAD” category. Now lump all bread product_types into 10 categories. Which category has the most observations? How many observations fall in the “Other” category? Now plot the result from #1 with a bar plot and reorder the bars so they are ordered by number of observations. 24.7 Additional Resources If you want to learn more about factors, I recommend reading Amelia McNamara and Nicholas Horton’s paper, Wrangling categorical data in R. This paper lays out some of the history discussed in stringsAsFactors: An unauthorized biography and stringsAsFactors = , and compares the tidy approaches to categorical data outlined in this lesson with base R methods. "],["lesson-5c-visual-data-exploration.html", "25 Lesson 5c: Visual data exploration 25.1 Learning objectives 25.2 Prerequisites 25.3 Univariate Distributions 25.4 Bivariate relationships 25.5 Multivariate relationships 25.6 Data quality 25.7 Exercise", " 25 Lesson 5c: Visual data exploration Data visualization is a critical tool in the data analysis process. Visualization tasks can range from generating fundamental distribution plots to understanding the interplay of complex influential variables in machine learning algorithms. In lesson 5a we learned how to make plots with ggplot2. In this lesson we build upon this skill and focus on the use of visualization for initial data exploration. Visual data exploration is a mandatory initial step whether or not more formal analysis follows. When combined with descriptive statistics, visualization provides an effective way to identify summaries, structure, relationships, differences, and abnormalities in the data. Often times no elaborate analysis is necessary as all the important conclusions required for a decision are evident from simple visual examination of the data. Other times, data exploration will be used to help guide the data cleaning, feature selection, and sampling process. Regardless, visual data exploration is about investigating the characteristics of your data set. To do this, we typically create numerous plots in an interactive fashion. This lesson will show you how to create plots that answer some of the fundamental questions we typically have of our data. 25.1 Learning objectives By the end of this lesson you will be able to create and modify plots to understand: Single variable distributions. Relationships between two or more variables. How missing data behaves throughout a data set. 25.2 Prerequisites We’ll illustrate the key ideas by primarily focusing on refined Ames housing data provided by the AmesHousing package. We’ll use tidyverse to provide some basic data manipulation capabilities along with ggplot2 for plotting. # main package required library(tidyverse) # data used ames &lt;- AmesHousing::make_ames() ames ## # A tibble: 2,930 × 81 ## MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape Land_Contour Utilities Lot_Config Land_Slope ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 One_Story_1946_an… Resident… 141 31770 Pave No_A… Slightly… Lvl AllPub Corner Gtl ## 2 One_Story_1946_an… Resident… 80 11622 Pave No_A… Regular Lvl AllPub Inside Gtl ## 3 One_Story_1946_an… Resident… 81 14267 Pave No_A… Slightly… Lvl AllPub Corner Gtl ## 4 One_Story_1946_an… Resident… 93 11160 Pave No_A… Regular Lvl AllPub Corner Gtl ## 5 Two_Story_1946_an… Resident… 74 13830 Pave No_A… Slightly… Lvl AllPub Inside Gtl ## 6 Two_Story_1946_an… Resident… 78 9978 Pave No_A… Slightly… Lvl AllPub Inside Gtl ## 7 One_Story_PUD_194… Resident… 41 4920 Pave No_A… Regular Lvl AllPub Inside Gtl ## 8 One_Story_PUD_194… Resident… 43 5005 Pave No_A… Slightly… HLS AllPub Inside Gtl ## 9 One_Story_PUD_194… Resident… 39 5389 Pave No_A… Slightly… Lvl AllPub Inside Gtl ## 10 Two_Story_1946_an… Resident… 60 7500 Pave No_A… Regular Lvl AllPub Inside Gtl ## # … with 2,920 more rows, and 70 more variables: Neighborhood &lt;fct&gt;, Condition_1 &lt;fct&gt;, Condition_2 &lt;fct&gt;, ## # Bldg_Type &lt;fct&gt;, House_Style &lt;fct&gt;, Overall_Qual &lt;fct&gt;, Overall_Cond &lt;fct&gt;, Year_Built &lt;int&gt;, Year_Remod_Add &lt;int&gt;, ## # Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;, Exterior_1st &lt;fct&gt;, Exterior_2nd &lt;fct&gt;, Mas_Vnr_Type &lt;fct&gt;, Mas_Vnr_Area &lt;dbl&gt;, ## # Exter_Qual &lt;fct&gt;, Exter_Cond &lt;fct&gt;, Foundation &lt;fct&gt;, Bsmt_Qual &lt;fct&gt;, Bsmt_Cond &lt;fct&gt;, Bsmt_Exposure &lt;fct&gt;, ## # BsmtFin_Type_1 &lt;fct&gt;, BsmtFin_SF_1 &lt;dbl&gt;, BsmtFin_Type_2 &lt;fct&gt;, BsmtFin_SF_2 &lt;dbl&gt;, Bsmt_Unf_SF &lt;dbl&gt;, ## # Total_Bsmt_SF &lt;dbl&gt;, Heating &lt;fct&gt;, Heating_QC &lt;fct&gt;, Central_Air &lt;fct&gt;, Electrical &lt;fct&gt;, First_Flr_SF &lt;int&gt;, ## # Second_Flr_SF &lt;int&gt;, Low_Qual_Fin_SF &lt;int&gt;, Gr_Liv_Area &lt;int&gt;, Bsmt_Full_Bath &lt;dbl&gt;, Bsmt_Half_Bath &lt;dbl&gt;, … We also demonstrate some useful functions from a few other packages throughout the lesson. To replicate all the code in this lesson you’ll need to install the following additional packages: pkgs &lt;- c(&quot;AmesHousing&quot;, &quot;ggridges&quot;, &quot;GGally&quot;, &quot;glue&quot;,&quot;gridExtra&quot;, &quot;hexbin&quot;, &quot;naniar&quot;, &quot;treemap&quot;, &quot;viridis&quot;) install.packages(pkgs) This refined data set has: glue::glue( &#39;rows: {dim(ames)[1]}\\n&#39;, &#39;columns: {dim(ames)[2]}&#39; ) ## rows: 2930 ## columns: 81 The glue package is a great tool for combining strings with R expressions! 25.3 Univariate Distributions Before moving on to more sophisticated visualizations that enable multidimensional investigation, it is important to be able to understand how an individual variable is distributed. Visually understanding the distribution allows us to describe many features of a variable. A variable is continuous if it can take any of an infinite set of ordered values. There are several different plots that can effectively communicate the different features of continuous variables. Features we are generally interested in include: Measures of location Measures of spread Asymmetry Outliers Gaps 25.3.1 Histograms Histograms are often overlooked, yet they are a very efficient means for communicating these features of continuous variables. Formulated by Karl Pearson, histograms display numeric values on the x-axis where the continuous variable is broken into intervals (aka bins) and the the y-axis represents the frequency of observations that fall into that bin. Histograms quickly signal what the most common observations are for the variable being assessed (the higher the bar the more frequent those values are observed in the data); they also signal the shape of your data by illustrating if the observed values cluster towards one end or the other of the distribution. To get a quick sense of how our home sale prices are distributed across the 2,930 observations in the stores data we can generate a simple histogram by applying ggplot’s geom_histogram() function. This histogram tells us several important features about our variable: ggplot(ames, aes(x = Sale_Price)) + geom_histogram() + scale_x_continuous(labels = scales::dollar) This plot conveys several important features about the distribution of Sale_Price: Measures of location: the most common values of Sale_Price are around $160K (the median); Measures of spread: Sale_Price ranges from near zero to over $700K; Skewness: Sale_Price is skewed right (a common feature of financial data); Outliers: there appears to be some abnormally large values of Sale_Price. Gaps: We see a gap exists between sale price values around $625K and $750K. Some statistical modeling techniques (i.e. linear regression) are sensitive to outliers and assume that the data are (at least approximately) symmetric. Histograms, as well as some other visualizations we’ll cover, are invaluable tools for identifying there and other potential problems with our data. By default, geom_histogram() will divide the values of a continuous variable into 30 equally sized bins. Since Sale_Price ranges from $12,789–$755,000, 30 equally sized bins implies a bin width of \\(\\frac{\\left\\lfloor{\\$755,000 - \\$12,789}\\right\\rfloor}{30} = \\$24,740\\). So in the above plot, the first bar represents the frequency of Sale_Price in the range of (roughly) \\(\\left[\\$12,500, \\ \\$37,500\\right]\\), the second bar represents the frequency of Sale_Price in the range of (roughly) \\(\\left[\\$37,500, \\ \\$62,300\\right]\\), and so on. The number of bins (or equivalently, the bin width) in a histogram is a tuning parameter that should be adjusted specifically for each application. Adjusting the bin width is somewhat subjective and needs to be done carefully (usually by eye). If the bin width is “too small”, the histogram will overfit the data and display too much noise. If the bin width is “too large”, the histogram will underfit the data and important features (like outliers) will be missed. In geom_histogram(), we can use the binwidth and bins arguments to control the bin width and number of bins, respectively (but only one argument needs to be specified). By adjusting one of these parameters, we can change the crudeness of the histogram estimate of the variable’s density. For instance, in the default histogram, the bin with the largest frequency ranged from $136,000–$161,000; however, as demonstrated below, we can often do better by manually adjusting the bin width. # Histograms with various bin widths # Too crude (i.e., under fitting) p1 &lt;- ggplot(ames, aes(Sale_Price)) + geom_histogram(binwidth = 100000) + ggtitle(&quot;Bin width = $100,000&quot;) # Less crude p2 &lt;- ggplot(ames, aes(Sale_Price)) + geom_histogram(binwidth = 50000) + ggtitle(&quot;Bin width = $50,000&quot;) # Just right? p3 &lt;- ggplot(ames, aes(Sale_Price)) + geom_histogram(binwidth = 5000) + ggtitle(&quot;Bin width = $5,000&quot;) # Too flexible (i.e., over fitting) p4 &lt;- ggplot(ames, aes(Sale_Price)) + geom_histogram(binwidth = 1000) + ggtitle(&quot;Bin width = $1,000&quot;) # Display plots in a grid grid.arrange(p1, p2, p3, p4, ncol = 2) The most common value of Sale_Price (i.e., the statistical mode) is $135,000 (this is roughly where the peak of each histogram occurs). R does not have a built-in function for finding the most common value in a set of observations, but we can easily find it using a combination of which.max() and table(). which.max(table(ames$Sale_Price)) ## 135000 ## 270 We can also find the most frequent bin by combining cut_width (cut_interval and cut_number are additional options) with count. We see that the most frequent bin when using increments of $5,000 is $128,000 - $132,000. ames %&gt;% count(cut_width(Sale_Price, width = 5000)) %&gt;% arrange(desc(n)) ## # A tibble: 106 × 2 ## `cut_width(Sale_Price, width = 5000)` n ## &lt;fct&gt; &lt;int&gt; ## 1 (1.28e+05,1.32e+05] 137 ## 2 (1.42e+05,1.48e+05] 130 ## 3 (1.32e+05,1.38e+05] 125 ## 4 (1.38e+05,1.42e+05] 125 ## 5 (1.22e+05,1.28e+05] 118 ## 6 (1.52e+05,1.58e+05] 103 ## 7 (1.48e+05,1.52e+05] 101 ## 8 (1.18e+05,1.22e+05] 99 ## 9 (1.58e+05,1.62e+05] 92 ## 10 (1.72e+05,1.78e+05] 92 ## # … with 96 more rows We can assess the applicability of a log transformation by adding scale_x_log() to our ggplot visual. This log transformed histogram provides a couple new insights: It appears the log transformation helps our variable meet normality assumptions. More on this in a second. It appears there new potential outliers that we did not see earlier. There are several observations where the sale price is very low. In fact, there are 11 observations where the sale price is below $50,000 and 2 that are below $15,000! ggplot(ames, aes(Sale_Price)) + geom_histogram(bins = 100) + scale_x_log10(labels = scales::dollar) 25.3.2 Quantile-quantile plots Let’s consider the issue of normality. If you really want to look at normality, then Q-Q plots are a great visual to assess. This graph plots the cumulative values we have in our data against the cumulative probability of a particular distribution (the default is a normal distribution). In essence, this plot compares the actual value against the expected value that the score should have in a normal distribution. If the data are normally distributed the plot will display a straight (or nearly straight) line of dots. If the data deviates from normality then the plot will display strong curvature or “snaking.” Looking at the Q-Q plots below, notice how the normal Q-Q plot for Sale_Price starts to bend up. This indicates that the distribution of Sale_Price has a heavier right tail than expected from a normal distribution (i.e., Sale_Price is more positively skewed than what would be expected from normally distributed data). The normal Q-Q plot for log(Sale_Price) is a bit more well behaved (aside from the two potential outliers and slight downward curvature in the bottom left of the graph). # non-log transformed p1 &lt;- ggplot(ames, aes(sample = Sale_Price)) + stat_qq() # log transformed p2 &lt;- ggplot(ames, aes(sample = log(Sale_Price))) + stat_qq() gridExtra::grid.arrange(p1, p2, ncol = 2) Histograms (and kernel density estimates), and Q-Q plots are great at visually summarizing the distribution of a continuous variable; especially a variable with many unique values. Unfortunately, histograms are less suitable for continuous data with few unique values, and are not great at displaying outliers (the same goes for Q-Q plots). Fortunately, a simple display of a few descriptive statistics offers a perfect compliment to histograms; this display is referred to as a box plot. 25.3.3 Box plots We previously mentioned how we obtained a new insight regarding new potential outliers that we did not see earlier. So far our histogram identified potential outliers at the lower end and upper end of the sale price amount spectrum. Unfortunately histograms are not very good at delineating outliers. Rather, we can use a boxplot which does a better job identifying specific outliers. Boxplots are an alternative way to illustrate the distribution of a variable and is a concise way to illustrate the standard quantiles and outliers of data. As the below figure indicates, the box itself extends, left to right, from the 1st quartile to the 3rd quartile. This means that it contains the middle half of the data. The line inside the box is positioned at the median. The lines (whiskers) coming out either side of the box extend to 1.5 interquartile ranges (IQRs) from the quartiles. These generally include most of the data outside the box. More distant values, called outliers, are denoted separately by individual points. Now we have a more analytically specific approach to identifying outliers. Figure 25.1: Information a boxplot provides. There are two efficient graphs to get an indication of potential outliers in our data. The classic boxplot on the left will identify points beyond the whiskers which are beyond \\(1.5 \\times IQR\\) from the first and third quantile. This illustrates there are several additional observations that we may need to assess as outliers that were not evident in our histogram. However, when looking at a boxplot we lose insight into the shape of the distribution. A violin plot on the right provides us a similar chart as the boxplot but we lose insight into the quantiles of our data and outliers are not plotted (hence the reason I plot geom_point prior to geom_violin). Violin plots will come in handy later when we start to visualize multiple distributions along side each other. p1 &lt;- ggplot(ames, aes(&quot;var&quot;, Sale_Price)) + geom_boxplot(outlier.alpha = .25) + scale_y_log10( labels = scales::dollar, breaks = quantile(ames$Sale_Price) ) p2 &lt;- ggplot(ames, aes(&quot;var&quot;, Sale_Price)) + geom_point() + geom_violin() + scale_y_log10( labels = scales::dollar, breaks = quantile(ames$Sale_Price) ) gridExtra::grid.arrange(p1, p2, ncol = 2) The boxplot starts to answer the question of what potential outliers exist in our data. Outliers in data can distort predictions and affect their accuracy. Consequently, its important to understand if outliers are present and, if so, which observations are considered outliers. Boxplots provide a visual assessment of potential outliers while the outliers package provides a number of useful functions to systematically extract these outliers. The most useful function is the scores function, which computes normal, t, chi-squared, IQR and MAD scores of the given data which you can use to find observation(s) that lie beyond a given value. Here, I use the outliers::score function to extract those observations beyond the whiskers in our boxplot and then use a stem-and-leaf plot to assess them. A stem-and-leaf plot is a special table where each data value is split into a “stem” (the first digit or digits) and a “leaf” (usually the second digit). Since the decimal point is located 5 digits to the right of the “|” the last stem of “7” and and first leaf of “5” means an outlier exists at around $750,000. The first stem of “0” and and third leaf of “3” means an outlier exists at around $30,000. This is a concise way to see approximately where our outliers are. In fact, I can now see that I have several lower end outliers ranging from $11K-$60K and several more upper end outliers ranging from $450K-$760K. # ID outliers outliers &lt;- outliers::scores(log(ames$Sale_Price), type = &quot;iqr&quot;, lim = 1.5) # stem plot of outliers stem(ames$Sale_Price[outliers]) ## ## The decimal point is 5 digit(s) to the right of the | ## ## 0 | 1134444445555555666666666666 ## 1 | ## 2 | ## 3 | ## 4 | 56666777788899 ## 5 | 000445566889 ## 6 | 1123 ## 7 | 56 # order outlier values sort(ames$Sale_Price[outliers]) ## [1] 12789 13100 34900 35000 35311 37900 39300 40000 44000 45000 46500 50000 50138 51689 52000 52500 ## [17] 55000 55000 55000 55993 57625 58500 58500 59000 60000 60000 60000 61000 451950 455000 457347 460000 ## [33] 462000 465000 466500 468000 470000 475000 475000 479069 485000 492000 500000 500067 501837 535000 538000 545224 ## [49] 552000 555000 556581 582933 584500 591587 610000 611657 615000 625000 745000 755000 As demonstrated, several plots exist for examining univariate continuous variables. Several examples were provided here but still more exist (i.e. frequency polygon, bean plot, shifted histograms). There is some general advice to follow such as histograms being poor for small data sets, dot plots being poor for large data sets, histograms being poor for identifying outlier cut-offs, box plots being good for outliers but obscuring multimodality. Consequently, it is important to draw a variety of plots. Moreover, it is important to adjust parameters within plots (i.e. bin width, axis transformation for skewed data) to get a comprehensive picture of the variable of concern. In practice, it is often helpful (encouraged, in fact) to look at many types of graphics for a particular variable. For continuous variables, it is often useful to look at histograms, box plots, Q-Q plots, and a number of other graphics (depending on the situation). We can combine multiple plots to display a wealth of important information about continuous variables, especially as it relates to typical analytic tasks (e.g., density, correlation, outliers, etc.) p1 &lt;- ggplot(ames, aes(x = Sale_Price)) + geom_histogram(aes(y = ..density..), size = 1.5) + geom_density(color = &quot;purple2&quot;) + scale_x_log10(labels = scales::dollar) + ggtitle(&quot;Illustrates a skewed distribution&quot;) p2 &lt;- ggplot(ames, aes(x = &quot;&quot;, y = Sale_Price)) + geom_jitter(alpha = 0.1) + geom_boxplot() + coord_flip() + scale_y_log10(labels = scales::dollar) + ggtitle(&quot;Illustrates a outliers&quot;) p3 &lt;- ggplot(ames, aes(sample = Sale_Price)) + geom_qq(alpha = 0.5) + ggtitle(&quot;Illustrates whether we can assume normality&quot;) ames$Index &lt;- seq_len(nrow(ames)) # add row number as a new column p4 &lt;- ggplot(ames, aes(x = Index, y = Sale_Price)) + geom_line(alpha = 0.5) + ggtitle(&quot;Illustrates pattern based on index&quot;) grid.arrange(p1, p2, p3, p4, ncol = 2) 25.3.4 Categorical variables A categorical variable is a variable that can take on one of a limited, and usually fixed, number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property (i.e. gender, grade, manufacturer). There are several different features we are generally interested in with categorical variables (i.e. counts, proportions, imbalances, mislabeled levels). There are a few different plots that can effectively communicate these features of categorical variables. 25.3.5 Bar charts In a typical bar chart, each bar represents a different category (e.g. male or female) and the height of each bar represents the frequency (or proportion) of observations within each category. By default, the x-axis typically represents categories; however, as we will see, it is often useful to “flip” bar charts horizontally for readability, especially when the number of categories is large. We have already seen bar charts in the form of a histogram—the difference there being that the bars (i.e., categories) were created by binning a numeric variable into (roughly) equal sized buckets. If we look at the general zoning classification (MS_Zoning) for each property sold in our ames data set, we see that the majority of all properties fall within one category. We can use ggplot2’s geom_bar() function to construct simple bar charts. By default, geom_bar() simply counts the observations within each category and displays them in a vertical bar chart. ggplot(ames, aes(MS_Zoning)) + geom_bar() + theme(axis.text.x = element_text(angle = 55, hjust = 1)) MS_Zoning is an example of a nominal categorical variable; a categorical variable for which there is no logical ordering of the categories (as opposed to low, medium, and high, for example). To get better clarity of nominal variables we can make some refinements. We can also draw bar charts manually using geom_col(). To do this, we first need to compute the length of each bar. For frequencies, we can use dplyr::count() to tally the number of observations within each category. We can then use dplyr::mutate() to convert frequencies to proportions. With this approach, we can use reorder() to easily reorder the bar categories from most frequent to least (or vice versa). For readability, we can also apply coord_flip() to rotate the bar chart (or any ggplot2 figure) on its side. These refinements are demonstrated in the code chunk below. # Bar chart of frequencies p1 &lt;- ames %&gt;% count(MS_Zoning) %&gt;% ggplot(aes(reorder(MS_Zoning, n), n)) + geom_col() + coord_flip() + labs(x = &quot;MS_Zoning&quot;, y = &quot;Frequency&quot;) + ggtitle(&quot;Total count&quot;) # Bar chart of proportions p2 &lt;- ames %&gt;% count(MS_Zoning) %&gt;% mutate(pct = n / sum(n)) %&gt;% # convert to proportions ggplot(aes(reorder(MS_Zoning, pct), pct)) + geom_col() + coord_flip() + # now x becomes y labs(x = &quot;MS_Zoning&quot;, y = &quot;Relative frequency&quot;) + scale_y_continuous(labels = scales::percent) + ggtitle(&quot;Percent of whole&quot;) # Dispay both plots side by side grid.arrange(p1, p2, ncol = 2) Now we can see that properties zoned as residential low density make up nearly 80% of all observations . We also see that properties zoned as agricultural (A_agr), industrial (I_all), commercial (C_all), and residential high density make up a very small amount of observations. In fact, below we see that these imbalanced category levels each make up less than 1% of all observations. ames %&gt;% count(MS_Zoning) %&gt;% mutate(pct = n / sum(n)) %&gt;% arrange(desc(pct)) ## # A tibble: 7 × 3 ## MS_Zoning n pct ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Residential_Low_Density 2273 0.776 ## 2 Residential_Medium_Density 462 0.158 ## 3 Floating_Village_Residential 139 0.0474 ## 4 Residential_High_Density 27 0.00922 ## 5 C_all 25 0.00853 ## 6 A_agr 2 0.000683 ## 7 I_all 2 0.000683 Severely imbalanced categories can cause problems in statistical modeling, so it makes sense to sometimes combine the infrequent levels into an other category. Here we use n = 3 to retain the top three most frequent categories/levels, and condense the remaining into an other category. You can see that other still represents less than 2% of all observations. ames %&gt;% mutate(MS_Zoning = forcats::fct_lump(MS_Zoning, n = 3)) %&gt;% count(MS_Zoning) %&gt;% mutate(pct = n / sum(n)) %&gt;% ggplot(aes(reorder(MS_Zoning, pct), pct)) + geom_col() + coord_flip() In some cases, the categories of a categorical variable have a natural ordering (though, the difference between any two categories is not meaningful)—these are called ordinal variables. For example, the Kitchen_Qual variable in ames categorizes kitchen quality into five buckets: table(ames$Kitchen_Qual) ## ## Excellent Fair Good Poor Typical ## 205 70 1160 1 1494 ggplot(ames, aes(Kitchen_Qual)) + geom_bar() Here, we might consider ordering the bars using their natural order: Poor &lt; Fair &lt; Typical &lt; Good &lt; Excellent with forcats::fct_relevel(). It is easier to see that most homes have average to slightly above average quality kitchens. ames %&gt;% mutate(Kitchen_Qual = forcats::fct_relevel( Kitchen_Qual, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;) ) %&gt;% ggplot(aes(Kitchen_Qual)) + geom_bar() Often our data will have categorical variables that are numerically encoded (typically as integers). For example, in the ames data set, the month each home was sold (Mo_Sold) was encoded using the integers 1–12. Sometimes being numerically encoded allows us to compute stats. For example we see that the median and mean values are around 6 implying that our distribution is centered around June (month 6) with most homes selling between May (month 4) and August (month 8). summary(ames$Mo_Sold) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 4.00 6.00 6.22 8.00 12.00 But for visuals (and in some cases, modeling), we often want to make sure R treats such variables as categorical as this makes interpreting the plots a little easier. # Keeping Mo_Sold as an integer p1 &lt;- ggplot(ames, aes(Mo_Sold)) + geom_bar() + xlab(&quot;Month sold (numeric)&quot;) # Converting Mo_Sold to a factor p2 &lt;- ggplot(ames, aes(as.factor(Mo_Sold))) + geom_bar() + xlab(&quot;Month sold (factor)&quot;) # Display both plots side by side grid.arrange(p1, p2, nrow = 2) 25.3.6 Dot plots Basic bar charts are great when the number of categories is small. As the number of categories increases, the bars can become squished together and distract attention from the main insights of the visual. Cleveland dot plots (or just dot plots) and lollipop charts, like bar charts, are useful for visualizing discrete distributions (e.g., tables or the frequencies of different categories) while being more economical in ink. For example, if we can use geom_point() to construct a dot plot of the relative frequencies of home sales across the 28 within the Ames housing data set. p1 &lt;- ames %&gt;% count(Neighborhood) %&gt;% mutate(pct = n / sum(n)) %&gt;% ggplot(aes(pct, reorder(Neighborhood, pct))) + geom_point() + labs(x = &quot;Relative frequency&quot;, y = &quot;Neighborhood&quot;) Similar to a dot plot, a lollipop chart minimizes the visual ink but uses a line to draw the readers attention to the specific x -axis value of each category. To create a lollipop chart, we use geom_segment() to add lines to the previous plot; we explicitly state that we want the lines to start at x = 0 and extend to the corresponding relative frequency with xend = pct. We also need to include y = Neighborhood and yend = Neighborhood so that we get one line segment for each neighborhood. p2 &lt;- p1 + geom_segment( aes(x = 0, xend = pct, y = Neighborhood, yend = Neighborhood), size = 0.15 ) gridExtra::grid.arrange(p1, p2, nrow = 1) This makes it clear that some neighborhoods have minimal representation (i.e. Green Hills, Landmark). And we can always verify this numerically. ames %&gt;% count(Neighborhood) %&gt;% mutate(pct = n / sum(n)) %&gt;% arrange(pct) ## # A tibble: 28 × 3 ## Neighborhood n pct ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Landmark 1 0.000341 ## 2 Green_Hills 2 0.000683 ## 3 Greens 8 0.00273 ## 4 Blueste 10 0.00341 ## 5 Northpark_Villa 23 0.00785 ## 6 Veenker 24 0.00819 ## 7 Bloomington_Heights 28 0.00956 ## 8 Briardale 30 0.0102 ## 9 Meadow_Village 37 0.0126 ## 10 Clear_Creek 44 0.0150 ## # … with 18 more rows This could inform us to lump all neighborhoods that appear fewer than 1% of the time into a single category. ames %&gt;% mutate(Neighborhood = fct_lump_prop(Neighborhood, 0.01)) %&gt;% count(Neighborhood) %&gt;% mutate(pct = n / sum(n)) %&gt;% ggplot(aes(pct, reorder(Neighborhood, pct))) + geom_point() + labs(x = &quot;Relative frequency&quot;, y = &quot;Neighborhood&quot;) + geom_segment( aes(x = 0, xend = pct, y = Neighborhood, yend = Neighborhood), size = 0.15 ) 25.3.7 Pie charts Don’t use them. 25.3.8 Knowledge check Assess the distribution of square footage (Gr_Liv_Area) across the homes sold. Is this feature normally distributed or is it skewed? Do there appear to be some outliers in this feature? What are the most common square footages across the homes. Assess the distribution of the age of the homes across the homes sold. To do this focus on the feature Year_Built. Is this feature normally distributed or is it skewed? Do there appear to be some outliers in this feature? What are the most common bins of years that homes were built. Assess the Overall_Qual of homes sold. What percentages of homes fall in each of the quality levels? 25.4 Bivariate relationships Beyond understanding the distribution of each individual variable, we often want to investigate associations and relationships between variables. When visualizing the relationship between two or more variables we are generally interested in identifying potential associations, outliers, clusters, gaps, barriers, and change points. 25.4.1 Scatter plots The easiest way to assess the relationship between two continuous variables is to use a scatter plot, one of the most important statistical graphics. A scatter plot graphs two continuous variables directly against each other (one variable on each axis). Below we assess the relationship between sale price (Sale_Price) and the square footage of above ground living area (Gr_Liv_Area). To limit the effect of over plotting, a common issue with scatter plot of large data sets, we lower the opacity of the individual points with alpha. ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point(alpha = 0.1) + scale_x_continuous(labels = scales::comma) + scale_y_continuous(labels = scales::dollar) It is fairly evident from our scatter plot that there is a generally positive association between Gr_Liv_Area and Sale_Price (this does not imply any causal association between them). Five potential outliers with Gr_liv_Area &gt; 4000 are also apparent. We also gain a general understanding of the joint density of these two variables (e.g., dense regions/clusters of data points indicate reasonable combinations of Gr_liv_Area and Sale_Price). For example, it is not very likely to find a home that will sell for more than $400K that less than 1.5K (sq. ft.) of above ground living area. The relationship between Gr_Liv_Area and Sale_Price appears to be fairly linear, but the variability in Sale_Price increases with Gr_Liv_Area (probably due to the positive skewness associated with both variables); this non-constant variance is called heteroscedasticiy. To help guide our eyes in interpreting trends between two variables, we can add parametric and/or non-parametric trend lines, which we can do so with geom_smooth(). Here we add two different trend lines: method = \"lm\" draw a trend line of the form \\(\\beta_0 + \\beta_1\\)Gr_Liv_Area, where the \\(y\\)-intercept (\\(\\beta_0\\)) and the slope (\\(\\beta_1\\)) are estimated from the observed data; method = \"auto\" use the number of observations to choose an appropriate non-parametric smoother (i.e., let the trend be dictated by the observed data). Note that for both trend lines we used se = FALSE to suppress plotting +/-2 standard error bands. Our plot shows that for homes with less than 2,250 square feet the relationship is fairly linear; however, beyond 2,250 square feet we see strong deviations from linearity. For reference, we also included the same plot, but with both axes on the \\(log_{10}\\) scale. Notice how this transformation helps to alleviate, to some extent, the heteroskedacticy noted earlier. # Better colors for the trend lines set1 &lt;- RColorBrewer::brewer.pal(n = 9, name = &quot;Set1&quot;) # Scatter plot with trend lines p1 &lt;- ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point(alpha = 0.1) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = set1[1]) + geom_smooth(method = &quot;auto&quot;, se = FALSE, color = set1[2]) # Scatter plot with trend lines (log10 scale) p2 &lt;- ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point(alpha = 0.1) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = set1[1]) + geom_smooth(method = &quot;auto&quot;, se = FALSE, color = set1[2]) + scale_x_log10() + scale_y_log10() # Display plots side by side grid.arrange(p1, p2, ncol = 2) Scatter plots can also several distinct clustering or gaps. We can add a density plot onto our scatter plot to highlight multiple clusters or how the density of points spread out. We can also change our plot to a hexbin plot, that replaces bunches of points with a larger hexagonal symbol. This provides us with a heatmap-like plot to signal highly concentrated regions. It also does a better job identifying gaps in our data where no observations exist. This is illustrated below which graphs Sale_Price versus Garage_Area. By incorporating a 2-D KDE (middle) we draw attention to the higher density areas which appear to be located at homes with Garage_Area = 0, and homes where 250 &lt; Garage_Area &lt; 500. Similar observations can be drawn from the hexagonal heat map (right). # Scatter plot p1 &lt;- ggplot(ames, aes(x = Garage_Area, y = Sale_Price)) + geom_point(alpha = 0.2) # Scatter plot with 2-D KDE p2 &lt;- ggplot(ames, aes(x = Garage_Area, y = Sale_Price)) + geom_point(alpha = 0.1) + stat_density2d(aes(fill = stat(level)), geom = &quot;polygon&quot;) + viridis::scale_fill_viridis(option = &quot;A&quot;) + theme(legend.position = &quot;none&quot;) # heat map of hexagonal bin counts p3 &lt;- ggplot(ames, aes(x = Garage_Area, y = Sale_Price)) + geom_hex(bins = 50, show.legend = FALSE) + viridis::scale_fill_viridis(option = &quot;D&quot;) # &quot;D&quot; is the default # Display plots side by side grid.arrange(p1, p2, p3, ncol = 3) 25.4.2 Strip plots A scatter plot of a continuous variable against a categorical variable is referred to as a strip plot. Strip plots can be useful in practice, but it is generally advisable to use box plots (and there extensions) instead. Below we plot Sale_Price against the number of above ground bedrooms (Bedroom_AbvGr). Due to the size of this data set, the strip plot (top left) suffers from over plotting. We can use geom_jitter() to add a some random variation to points within each category (top right), which allows us to see where heavier concentrations of points exist. Alternatively, we can use box plots and violin plots to compare the distributions of Sale_Price to Bedroom_AbvGr (bottom row). # Convert to an ordered factor ames$Bedroom_AbvGr &lt;- as.ordered(ames$Bedroom_AbvGr) # Strip plot p1 &lt;- ggplot(ames, aes(x = Bedroom_AbvGr, y = Sale_Price)) + geom_point(alpha = 0.1) + ggtitle(&quot;Scatter plot&quot;) # Strip plot (with jittering) p2 &lt;- ggplot(ames, aes(x = Bedroom_AbvGr, y = Sale_Price)) + geom_jitter(alpha = 0.1, width = 0.2) + ggtitle(&quot;Strip plot&quot;) # Box plots p3 &lt;- ggplot(ames, aes(x = Bedroom_AbvGr, y = Sale_Price)) + geom_boxplot() + ggtitle(&quot;Box plot&quot;) # Violin plots p4 &lt;- ggplot(ames, aes(x = Bedroom_AbvGr, y = Sale_Price)) + geom_violin() + ggtitle(&quot;Violin plot&quot;) # Display plots side by side grid.arrange(p1, p2, p3, p4, nrow = 2) 25.4.3 Overlays &amp; faceting An alternative approach to view the distribution of a continuous variable across multiple categories includes overlaying distribution plots. For example, we could assess the net sales value of households across the different sizes of households. We can do this with a frequency polygon (left), which display the outline of a histogram. However, since some quality levels have very low counts it is tough to see the distribution of net sales within each category. A better approach is to overlay density plots which allows us to see how each household size’s distribution differs from one another. p1 &lt;- ggplot(ames, aes(x = Sale_Price, color = Overall_Qual)) + geom_freqpoly() p2 &lt;- ggplot(ames, aes(x = Sale_Price, color = Overall_Qual, fill = Overall_Qual)) + geom_density(alpha = .15) gridExtra::grid.arrange(p1, p2, nrow = 2) When there are many levels in a categorical variable, overlaid plots become difficult to decipher. Rather than overlay plots, we can also use small multiples to compare the distribution of a continuous variable. Ridge plots provide a form of small multiples by partially overlapping distribution plots. They can be quite useful for visualizing changes in continuous distributions over discrete variable levels. In this example I use the ggridges package which provides an add-on geom_density_ridges for ggplot. This provides a clear picture how the sale price distribution changes (and increases) as the overall quality of the home increases. ggplot(ames, aes(x = Sale_Price, y = Overall_Qual)) + ggridges::geom_density_ridges() + scale_x_continuous(labels = scales::dollar) We can also use faceting to understand how two or more categorical variables are associated with each other. For example, below we assess the quality of kitchens (Kitchen_Qual) for homes that sold above average (i.e., Sale_Price &gt; mean(Sale_Price)) or below average (i.e., Sale_Price &gt; mean(Sale_Price)). Not surprisingly, we see that that sold for above average tended to have higher quality kitchens. ames %&gt;% mutate( Group = ifelse( Sale_Price &gt; mean(Sale_Price), yes = &quot;Sold above avg.&quot;, no = &quot;Sold below avg.&quot;), Kitchen_Qual = forcats::fct_relevel( Kitchen_Qual, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;) ) %&gt;% ggplot(aes(Kitchen_Qual)) + geom_bar() + facet_wrap(~ Group) 25.5 Multivariate relationships In most analyses, data are usually multivariate by nature, and the analytics are designed to capture and measure multivariate relationships. Visual exploration should therefore also incorporate this important aspect. We can extend the above basic principles and add in additional features to assess multidimensional relationships. One approach is to add additional variables with features such as color, shape, or size. 25.5.1 Layering variables For example, we can compare Sale_Price with Gr_Liv_Area using a simple scatter plot. We can also include information on other variables by using color, shape, and point size. For instance, we can use a different point shape and color to indicate homes with and without central air conditioning (Central_Air). The below plot illustrates that there are far more homes with central air; furthermore, the homes without central air tend to have less square footage and lower sale prices. ggplot(ames, aes( x = Gr_Liv_Area, y = Sale_Price, color = Central_Air, shape = Central_Air )) + geom_point(alpha = 0.3) + theme(legend.position = &quot;top&quot;) Although we lowered the opacity of each point, the over crowded plot makes it difficult to distinguish the relationships between homes with and without air conditioning. Another approach is to use facetting which provides more clarity between the differences in air and non-air conditioned homes. ames %&gt;% mutate(Central_Air = fct_recode( Central_Air, &quot;With A/C&quot; = &quot;Y&quot;, &quot;Without A/C&quot; = &quot;N&quot; )) %&gt;% ggplot(aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_hex(bins = 50, show.legend = FALSE) + facet_wrap(~Central_Air) + viridis::scale_fill_viridis(option = &quot;D&quot;) However, as before, when there are many levels in a categorical variable it becomes hard to compare differences by only incorporating color or shape features. An alternative is to create small multiples. Below we compare the relationship between Sale_Price and Gr_Liv_Area and how this relationship differs across the different house styles (House_Style). ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point(alpha = 0.2) + scale_x_log10() + scale_y_log10(labels = scales::dollar) + facet_wrap(~ House_Style, nrow = 2) + theme_bw() We can start to add several of the features discussed in this lesson to highlight multivariate features. For example, here we assess the relationship between sales price and above ground square footage for homes with and without central air conditioning and across the different housing styles. For each house style and central air category we can see where the values are clustered and how the linear relationship changes. For all home styles, houses with central air have a higher selling price with a steeper slope than those without central air. Also, those plots without density markings and linear lines for the no central air category (red) tell us that there are no more than one observation in these groups; so this identifies gaps across multivariate categories of interest. ggplot(ames, aes(Gr_Liv_Area, Sale_Price, color = Central_Air, shape = Central_Air)) + geom_point(alpha = 0.2) + geom_density2d(alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_x_log10() + scale_y_log10(labels = scales::dollar) + facet_wrap(~ House_Style, nrow = 2) + ggtitle(&quot;Sale Price vs. Above Ground Sq.Ft&quot;, subtitle = &quot;How does central air and house style influence this relationship?&quot;) + theme_bw() 25.5.2 Parallel coordinate plots Parallel coordinate plots (PCPs) are also a great way to visualize continuous variables across multiple variables. In PCPs, a vertical axis is drawn for each variable. Then each observation is represented by drawing a line that connects its values on the different axis, creating a multivariate profile. To create a PCP, we can use the ggparcoord() function from the GGally package. By default, GGally::ggparcoord() will standardize the variables based on a z-score distribution; however, there are many options for scaling (see ?GGally::ggparcoord). A benefit of using PCPs is that you can visualize observations across both continuous and categorical variables. In the example below we include Overall_Qual, an ordered factor with ten levels “Very Poor” &lt; “Poor” &lt; “Fair” &lt; … &lt; “Excellent” &lt; “Very Excellent” having integer values of 1–10. When including a factor variable, GGally::ggparcoord() will use the factor integer levels as their corresponding value, so it is important to appropriately order any factors you want to include. # Variables of interest variables &lt;- c(&quot;Sale_Price&quot;, &quot;Year_Built&quot;, &quot;Year_Remod_Add&quot;, &quot;Overall_Qual&quot;) # Parallel coordinate plot ames %&gt;% select(variables) %&gt;% GGally::ggparcoord(alpha = 0.05, scale = &quot;center&quot;) The darker bands in the above plot illustrate several features. The observations with higher sales prices tend to be built in more recent years, be remodeled in recent years and be categorized in the top half of the overall quality measures. In contracts, homes with lower sales prices tend to be more out-dated (based on older built and remodel dates) and have lower quality ratings. We also see some homes with exceptionally old build dates that have much newer remodel dates but still have just average quality ratings. We can make this more explicit by adding a new variable to indicate if a sale price is above average. We can then tell GGally::ggparcood() to group by this new variable. Now we clearly see that above average sale prices are related to much newer homes. ames %&gt;% select(variables) %&gt;% mutate(Above_Avg = Sale_Price &gt; mean(Sale_Price)) %&gt;% GGally::ggparcoord( alpha = 0.05, scale = &quot;center&quot;, columns = 1:4, groupColumn = &quot;Above_Avg&quot; ) 25.5.3 Mosaic plots Mosaic plots are a graphical method for visualizing data from two or more qualitative variables. In this visual the graphics area is divided up into rectangles proportional in size to the counts of the combinations they represent. Below we use abbreviate to shorten the labels but we can pull a few insights from the below: Left plot: we can see that far more homes that sold above the average sale price (TRUE) have attached garages (Attc) whereas far more homes that sold below average sale price have detached garages (Dtch). Right plot: we can see that far more homes that sold below the average sale price have 1 car garages where almost all homes that sold above the average sale price have 2-3 car garages. ames2 &lt;- ames %&gt;% mutate( Above_Avg = Sale_Price &gt; mean(Sale_Price), Garage_Type = abbreviate(Garage_Type), Garage_Qual = abbreviate(Garage_Qual) ) par(mfrow = c(1, 2)) mosaicplot(Above_Avg ~ Garage_Type, data = ames2, las = 1) mosaicplot(Above_Avg ~ Garage_Type + Garage_Cars, data = ames2, las = 1) 25.5.4 Tree maps Tree maps are also a useful visualization aimed at assessing the hierarchical structure of data. Tree maps are primarily used to assess a numeric value across multiple categories. It can be useful to assess the counts or proportions of a categorical variable nested within other categorical variables. For example, we can use a tree map to visualize the above right mosaic plot that illustrates the number of homes sold above and below average sales price with different garage characteristics. Again, we can see in the tree map that houses with above average prices tend to have attached 2 and 3-car garages. Houses sold below average price have more attached 1-car garages and also have far more detached garages. ames %&gt;% mutate(Above_Below = ifelse(Sale_Price &gt; mean(Sale_Price), &quot;Above Avg&quot;, &quot;Below Avg&quot;)) %&gt;% count(Garage_Type, Garage_Cars, Above_Below) %&gt;% treemap::treemap( index = c(&quot;Above_Below&quot;, &quot;Garage_Type&quot;, &quot;Garage_Cars&quot;), vSize = &quot;n&quot; ) 25.5.5 Heat maps A heat map is essentially a false color image of a matrix or data set. Heat maps can be extremely useful in identifying clusters of values; a common use is in plotting a correlation matrix. In the example below we select all the numeric variables in ames, compute the correlation matrix between them, and visualize the results with a heat map. Bright/dark spots represent clusters of observations with similar correlations. We can see that Sale_Price (3rd row from bottom) has a relatively weak linear association with variables such as BsmtFin_Sf_1, Bsmt_Unf_SF, Longitude, and Enclosed_Porch. The larger correlations values for Sale_Price align with variables such as Garage_Cars, Garage_Area, and First_Flr_SF, etc. ames %&gt;% select_if(is.numeric) %&gt;% # select all the numeric columns cor() %&gt;% # compute the correlation matrix heatmap( symm = TRUE, # since correlation matrices are symmetric! col = viridis::inferno(nrow(ames)) ) 25.5.6 Generalized pairs plot When dealing with a small data set (or subset of a large data set), it can be useful to construct a matrix of plots comparing two-way relationships across a number of variables. In the code chunk below we (i) select Sale_Price and all variables names that contain \"sf\" (i.e., all square footage variables), (ii) scale all variables, and (iii) display scatter plots and correlation values with the GGally::ggpairs() function. This can help us identify which features may have a potential relationship/association via numeric and visual means. In this example we see a pretty good relationship between Sale_Price and the Total_Bsmt_SF and First_Flr_SF features along with a couple others. ames %&gt;% select(Sale_Price, contains(&quot;sf&quot;)) %&gt;% # select column names containing &quot;sf&quot; GGally::ggpairs() 25.5.7 Knowledge check Analyze the relationship between the quality features (i.e. Overall_Qual, Kitchen_Qual, Bsmt_Qual, Exter_Qual) and Sale_Price. Which feature of these features appear to have a relationship? Assess the relationship between Sale_Price, Gr_Liv_Area, and Overall_Qual. How would you explain this relationship? Which neighborhoods tend to have larger homes (Gr_Liv_Area)? Which of these neighborhoods have higher than average Sale_Price and which have lower than average Sale_Price. Are there homes that appear to be much larger than the rest of the homes in their neighborhood? 25.6 Data quality Data quality is an important issue for any project involving analyzing data. Data quality issues deserves an entire book in its own right, and a good reference is the The Quartz guide to bad data. In this section, we discuss one topic of particular importance: visualizing missing data. It is important to understand the distribution of missing values (i.e., NA) is any data set. So far, we have been using a pre-processed version of the Ames housing data set (via the AmesHousing::make_ames() function). However, if we use the raw Ames housing data (via AmesHousing::ames_raw), there are actually 13,997 missing values—there is at least one missing values in each row of the original data! ames_raw &lt;- AmesHousing::ames_raw sum(is.na(ames_raw)) ## [1] 13997 It is important to understand the distribution of missing values in a data set in order to determine 1) if any variable(s) should be eliminated prior to analysis, or 2) if any values need to be imputed9. We can use the naniar package to assess missing data. For example, the below plot allows us to easily see where the majority of missing values occur (i.e., in the variables Pool QC, Misc Feature, Alley, Fence, and Fireplace Qu). Due to their high frequency of missingness, these variables would likely need to be removed prior to statistical analysis, or imputed (i.e., filled in with intelligent guesses). library(naniar) gg_miss_var(ames_raw) We can also spot obvious patterns of missingness. For example, plotting missing values across all variables and observations allow us to identify when missing values appear to occur within the same observations across certain features. For example, the below shows us that when missing values appear for one of the garage features nearly all other garage variables are also missing. vis_miss(ames_raw) Digging a little deeper into these variables, we might notice that Garage_Cars and Garage_Area contain the value 0 whenever the other Garage_xx variables have missing values (i.e. a value of NA). This might be because they did not have a way to identify houses with no garages when the data were originally collected; and therefore, all houses with no garage were identified by including nothing. Since this missingness is informative, it would be appropriate to impute NA with a new category level (e.g., \"None\") for these garage variables. Circumstances like this tend to only arise upon careful descriptive and visual examination of the data! ames_raw %&gt;% filter(is.na(`Garage Type`)) %&gt;% select(contains(&quot;garage&quot;)) ## # A tibble: 157 × 7 ## `Garage Type` `Garage Yr Blt` `Garage Finish` `Garage Cars` `Garage Area` `Garage Qual` `Garage Cond` ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;NA&gt; NA &lt;NA&gt; 0 0 &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; NA &lt;NA&gt; 0 0 &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; NA &lt;NA&gt; 0 0 &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; NA &lt;NA&gt; 0 0 &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; NA &lt;NA&gt; 0 0 &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; NA &lt;NA&gt; 0 0 &lt;NA&gt; &lt;NA&gt; ## 7 &lt;NA&gt; NA &lt;NA&gt; 0 0 &lt;NA&gt; &lt;NA&gt; ## 8 &lt;NA&gt; NA &lt;NA&gt; 0 0 &lt;NA&gt; &lt;NA&gt; ## 9 &lt;NA&gt; NA &lt;NA&gt; 0 0 &lt;NA&gt; &lt;NA&gt; ## 10 &lt;NA&gt; NA &lt;NA&gt; 0 0 &lt;NA&gt; &lt;NA&gt; ## # … with 147 more rows We can visually assess combinations of missingness and intersections of missingness amongst variables with the gg_miss_upset function. For example, the below shows the relationships across missing values in our raw Ames data. We see that there are 917 observations where Pool QC, Misc Feature, Alley, and Fence are all missing. We can also see that when garage features are missing they are pretty much missing across all garage features (versus just one or two garage features missing values). You can control how many features to visualize with nsets. By default it will pull the 5 features with the most missing values. ames_raw %&gt;% gg_miss_upset(nsets = 10) Data can be missing for different reasons. Perhaps the values were never recoded (or lost in translation), or it was recorded an error (a common feature of data enetered by hand). Regardless, it is important to identify and attempt to understand how missing values are distributed across a data set as it can provide insight into how to deal with these observations. 25.6.1 Knowledge check Using the nycflights13::flights data: 1. Identify which features have missing values? 2. How many missing values are in each of these features? 3. Do there appear to be a relationship between the missing values for these features? Do they tend to be missing at random or do they tend to be missing in unison? 25.7 Exercise Using the completejourney data: Use an inner join to join transactions_sample, products, and demographics data. Are there any missing values in this data? Assess the distribution of the sales value, quantity, and discount features. Are these features normally distributed or are they skewed? Do there appear to have outliers? What are the most common bins of values for these features and are there gaps across the value spectrum? Assess the distribution of departments for each transaction. Are there certain departments that have very few observations? Assess the relationship between sales_value, quantity, and department. How would you explain this relationship? Does brand have any influence in these relationships? Which departments tend to have larger sales values? Which of these departments have higher than average sales values and which have lower than average sales values. Are there departments that appear to much larger quantities of transactions than the rest of the departments? References "],["lab-4.html", "26 Lab", " 26 Lab TBD "],["overview-5.html", "27 Overview 27.1 Learning objectives 27.2 Tasks 27.3 Course readings", " 27 Overview Don’t repeat yourself (DRY) is a software development principle aimed at reducing repetition. Formulated by Andy Hunt and Dave Thomas in their book The Pragmatic Programmer, the DRY principle states that “every piece of knowledge must have a single, unambiguous, authoritative representation within a system.” This principle has been widely adopted to imply that you should not duplicate code. Although the principle was meant to be far grander than that, there’s plenty of merit behind this slight misinterpretation. Removing duplication is an important part of writing efficient code and reducing potential errors. First, reduced duplication of code can improve computing time and reduces the amount of code writing required. Second, less duplication results in less creating and saving of unnecessary objects. Inefficient code invariably creates copies of objects you have little interest in other than to feed into some future line of code; this wrecks havoc on properly managing your objects as it basically results in a global environment charlie foxtrot! Less duplication also results in less editing. When changes to code are required, duplicated code becomes tedious to edit and invariably mistakes or fat-fingering occur in the cut-and-paste editing process which just lengthens the editing that much more. Thus, minimizing duplication by writing efficient code is important to becoming a data analyst and in this module you learn to improve your code efficiency with control flow, iteration, and functions. 27.1 Learning objectives By the end of this module you should be able to: Use a variety of control statements to control the flow of code evaluation. Simplify repetitive code with R’s looping statements. Understand when and how to write a function. 27.2 Tasks TBD 27.3 Course readings TBD "],["lesson-6a-conditional-statements.html", "28 Lesson 6a: Conditional statements 28.1 Learning objectives 28.2 Prerequisites 28.3 if statement 28.4 ifelse statement 28.5 Multiple conditions 28.6 Vectorized approaches 28.7 With data frames 28.8 Exercises", " 28 Lesson 6a: Conditional statements Conditional statements are a key part of any programming language. Their purpose is to evaluate if some condition holds and then execute a particular task depending on the result. In this lesson we’ll perform some basic if and if-else conditional assessments and then build on those by adding multiple conditional assessments and also look at vectorized approaches. 28.1 Learning objectives By the end of this lesson you will be able to: Apply basic if and if...else statements. Add multiple conditional assessments. Apply non-vectorized and vectorized conditional statements. Incorporate these approaches for data mining data frames. 28.2 Prerequisites Many base R functions exist to perform control statements; however, we can also perform extensions of control statements using functions in the tidyverse package. library(tidyverse) To illustrate various control tasks we will use the Complete Journey customer transaction data. transactions &lt;- completejourney::transactions_sample 28.3 if statement The conditional if statement is used to test an expression. If the test_expression is TRUE, the statement gets executed. But if it’s FALSE, nothing happens. # syntax of if statement if (test_expression) { statement } Say, for example, that we want R to print a message depending on the value of x: set.seed(10) x &lt;- runif(1) If the condition is TRUE then the statement within the brackets is executed: if (x &gt; .5) { paste(&quot;x equals&quot;, round(x, 2), &quot;which is greater than 0.5&quot;) } ## [1] &quot;x equals 0.51 which is greater than 0.5&quot; However, if the condition is FALSE then the statement is not executed: if (x &gt; .7) { paste(&quot;x equals&quot;, round(x, 2), &quot;which is greater than 0.7&quot;) } Now say you have a vector of values that you would like to test. One would think you would just perform the following; however, you can see that you will get an error: x &lt;- c(8, 3, -2, 5) if (x &lt; 0) { print(&quot;x contains a negative number&quot;) } ## Error in if (x &lt; 0) {: the condition has length &gt; 1 This is because if() is not vectorized and it seeks to assess a single logical condition. To assess if a condition holds among multiple inputs, use any() or all(). any() looks to see if at least one value in the vector meets the condition: any(x &lt; 0) ## [1] TRUE if(any(x &lt; 0)) { print(&quot;x contains a negative number&quot;) } ## [1] &quot;x contains a negative number&quot; While all() looks to see if all values in the vector meets the condition: all(x &lt; 0) ## [1] FALSE if (all(x &lt; 0)) { print(&quot;x contains a negative number&quot;) } There are actually two ways to write this if statement; since the body of the statement is only one line you can write it with or without curly braces. I recommend getting in the habit of using curly braces, that way if you build onto if statements with additional functions in the body or add an else statement later you will not run into issues with unexpected code procedures. # without curly braces if(any(x &lt; 0)) print(&quot;x contains negative numbers&quot;) ## [1] &quot;x contains negative numbers&quot; # with curly braces produces same result if (any(x &lt; 0)) { print(&quot;x contains negative numbers&quot;) } ## [1] &quot;x contains negative numbers&quot; 28.3.1 Knowledge check Fill in the following code chunk so that: Ff month has value 1-9 the file name printed out will be “data/month-0X.csv” What happens if the month value is 10-12? month &lt;- 4 if (month _____) { paste0(&quot;data/&quot;, &quot;Month-0&quot;, month, &quot;.csv&quot;) } 28.4 ifelse statement The conditional if...else statement is used to test an expression similar to the if statement. However, rather than nothing happening if the test_expression is FALSE, the else part of the function will be evaluated. # syntax of if...else statement if (test_expression) { statement 1 } else { statement 2 } The following extends the if example illustrated in the previous section. Here, the if...else statement tests if any values in a vector are negative; if TRUE it produces one output and if FALSE it produces the else output. # this test results in statement 1 being executed x &lt;- c(8, 3, -2, 5) if (any(x &lt; 0)) { print(&quot;x contains negative numbers&quot;) } else { print(&quot;x contains all positive numbers&quot;) } ## [1] &quot;x contains negative numbers&quot; # this test results in statement 2 (or the else statement) being executed y &lt;- c(8, 3, 2, 5) if (any(y &lt; 0)) { print(&quot;y contains negative numbers&quot;) } else { print(&quot;y contains all positive numbers&quot;) } ## [1] &quot;y contains all positive numbers&quot; 28.4.1 Knowledge check Fill in the following code chunk so that: if month has value 1-9 the file name printed out will be “data/month-0X.csv” if month has value 10-12 the file name printed out will be “data/month-1X.csv” test it out for when month equals 4, 6, 10, &amp; 12 month &lt;- 4 if (month _____) { paste0(&quot;data/&quot;, &quot;Month-0&quot;, month, &quot;.csv&quot;) } else { paste0(&quot;data/&quot;, &quot;Month-&quot;, month, &quot;.csv&quot;) } 28.5 Multiple conditions We can continue to expand an if...else statement to assess more than just binary conditions by incorporating else if steps: x &lt;- 0 if (x &lt; 0) { print(&quot;x is a negative number&quot;) } else if (x &gt; 0) { print(&quot;x is a positive number&quot;) } else { print(&quot;x is zero&quot;) } ## [1] &quot;x is zero&quot; Note how we extend by following else with if(). But we should always end with an else. 28.5.1 Knowledge check Fill in the following code chunk so that: if month has value 1-9 the file name printed out will be “data/month-0X.csv” if month has value 10-12 the file name printed out will be “data/month-1X.csv” if month is an invalid month number (not 1-12), the result printed out is “Invalid month” test it out for when month equals 6, 10, &amp; 13 month &lt;- 4 if (month _____) { paste0(&quot;data/&quot;, &quot;Month-0&quot;, month, &quot;.csv&quot;) } else if (month _____) { paste0(&quot;data/&quot;, &quot;Month-&quot;, month, &quot;.csv&quot;) } else { print(&quot;_____&quot;) } 28.6 Vectorized approaches So far, we have focused on controlling the flow based on a single conditional statement. Basically, given one element we assess if that element meets a certain condition or multiple elements we simply assess if the condition holds for all/any elements: Figure 28.1: Illustration of non-vectorized conditional statements. However, what if we want to assess the condition against each element and execute code if that condition is TRUE for each element (aka vectorized)? Figure 28.2: Illustration of vectorized conditional statements. We can vectorize an if...else statement a couple of ways. One option is to use the ifelse() function built in base R: (x &lt;- c(runif(5), NA)) ## [1] 0.306769 0.426908 0.693102 0.085136 0.225437 NA ifelse(x &gt; .5, &quot;greater than&quot;, &quot;less than&quot;) ## [1] &quot;less than&quot; &quot;less than&quot; &quot;greater than&quot; &quot;less than&quot; &quot;less than&quot; NA Second, we can use dplyr::if_else(), which provides a little more stability in output type and flexibility in what to do with missing values: if_else(x &gt; .5, &quot;greater than&quot;, &quot;less than&quot;, missing = &quot;something else&quot;) ## [1] &quot;less than&quot; &quot;less than&quot; &quot;greater than&quot; &quot;less than&quot; &quot;less than&quot; ## [6] &quot;something else&quot; However, in both cases, they can only assess binary conditional statements. If we want to vectorize multiple conditions (i.e. if...else...if...else) then the best approach is to use dplyr::case_when(). The syntax for case_when() can be a little confusing at first. Basically, a conditional expression looks like condition ~ code to execute. Consequently: x &lt; .3 is the first condition to assess and if it is TRUE then the result is what comes after the ~. If the first condition is not met, then case_when will evaluate the second, third, etc. until it finds a condition that is true. If you include TRUE ~ some_expression at the end then any element that does not meet a prior condition will be considered TRUE and the expression will be evaluated. So in the below code, any element that doesn’t meet the prior conditions will be lumped into an “out of bounds” category. set.seed(123) (x &lt;- c(runif(10), NA, Inf, 1.25)) ## [1] 0.287578 0.788305 0.408977 0.883017 0.940467 0.045556 0.528105 0.892419 0.551435 0.456615 ## [11] NA Inf 1.250000 dplyr::case_when( x &lt; .3 ~ &quot;low&quot;, x &lt; .7 ~ &quot;medium&quot;, x &lt; .9 ~ &quot;medium high&quot;, x &lt;=1.0 ~ &quot;high&quot;, is.na(x) ~ &quot;missing&quot;, TRUE ~ &quot;out of bounds&quot; ) ## [1] &quot;low&quot; &quot;medium high&quot; &quot;medium&quot; &quot;medium high&quot; &quot;high&quot; ## [6] &quot;low&quot; &quot;medium&quot; &quot;medium high&quot; &quot;medium&quot; &quot;medium&quot; ## [11] &quot;missing&quot; &quot;out of bounds&quot; &quot;out of bounds&quot; 28.6.1 Knowledge check Re-write the below code using a vectorized approach. Test the results with month &lt;- 1:13. month &lt;- 1:13 if (month %in% 1:9) { paste0(&quot;data/&quot;, &quot;Month-0&quot;, month, &quot;.csv&quot;) } else if (month %in% 10:12) { paste0(&quot;data/&quot;, &quot;Month-&quot;, month, &quot;.csv&quot;) } else { print(&quot;Invalid month&quot;) } 28.7 With data frames So how can we leverage these skills when performing exploratory data analysis? Most common is to use use ifelse(), if_else(), and case_when() within dplyr::mutate(). For example, say we want to create a new variable that classifies transactions above $10 as “high value” otherwise they are “low value”. We can use the dplyr::if_else function within mutate to perform this. # I use select simply to reduce the size of the data set so you can easily # see results transactions %&gt;% select(household_id, basket_id, sales_value) %&gt;% mutate(value = if_else(sales_value &gt; 10, &#39;high value&#39;, &#39;low value&#39;)) ## # A tibble: 75,000 × 4 ## household_id basket_id sales_value value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2261 31625220889 3.86 low value ## 2 2131 32053127496 1.59 low value ## 3 511 32445856036 1 low value ## 4 400 31932241118 11.9 high value ## 5 918 32074655895 1.29 low value ## 6 718 32614612029 2.5 low value ## 7 868 32074722463 3.49 low value ## 8 1688 34850403304 2 low value ## 9 467 31280745102 6.55 low value ## 10 1947 32744181707 3.99 low value ## # … with 74,990 more rows dplyr::if_else is preferred to the base R ifelse function because it allows you to work with missing values more conveniently. When we want to perform multiple if_else statements within mutate we can embed multiple if_else statements within each other. Unfortunately, this gets pretty confusing quickly so a more convenient approach is to use case_when. For example, we can create a new variable that results in the following: Large purchase: quantity &gt; 20 or sales_value &gt; 10 Medium purchase: quantity &gt; 10 or sales_value &gt; 5 small purchase: quantity &gt; 0 or sales_value &gt; 0 Alternative transaction: all other transactions # I use select simply to reduce the size of the data set so you can easily # see results transactions %&gt;% select(household_id, basket_id, quantity, sales_value) %&gt;% mutate( value = case_when( quantity &gt; 20 | sales_value &gt; 10 ~ &quot;Large purchase&quot;, quantity &gt; 10 | sales_value &gt; 5 ~ &quot;Medium purchase&quot;, quantity &gt; 0 | sales_value &gt; 0 ~ &quot;Small purchase&quot;, TRUE ~ &quot;Alternative transaction&quot; ) ) ## # A tibble: 75,000 × 5 ## household_id basket_id quantity sales_value value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2261 31625220889 1 3.86 Small purchase ## 2 2131 32053127496 1 1.59 Small purchase ## 3 511 32445856036 1 1 Small purchase ## 4 400 31932241118 2 11.9 Large purchase ## 5 918 32074655895 1 1.29 Small purchase ## 6 718 32614612029 1 2.5 Small purchase ## 7 868 32074722463 1 3.49 Small purchase ## 8 1688 34850403304 1 2 Small purchase ## 9 467 31280745102 2 6.55 Medium purchase ## 10 1947 32744181707 1 3.99 Small purchase ## # … with 74,990 more rows 28.7.1 Knowledge check Fill in the blanks below to assign each transaction to a power rating of 1, 2, 3, or 4 based on the sales_value variable: power_rating = 1: if sales_value &lt; 25th percentile power_rating = 2: if sales_value &lt; 50th percentile power_rating = 3: if sales_value &lt; 75th percentile power_rating = 4: if sales_value &gt;= 75th percentile Hint: use quantile(x, perc_value) transactions %&gt;% select(household_id, basket_id, quantity, sales_value) %&gt;% mutate( power_rating = case_when( ______ ~ 1, ______ ~ 2, ______ ~ 3, ______ ~ 4 ) ) 28.8 Exercises Using the Complete Journey transactions_sample data: Create a new column titled total_disc that is the sum of all discounts applied to each transaction. Create a new column disc_rating that classifies each transaction as: ‘none’: if total_disc == 0 ‘low’: if total_disc &lt; 25th percentile ‘medium’: if total_disc &lt; 75th percentile ‘high’: if total_disc &gt;= 75th percentile ‘other’: for any transaction that doesn’t meet any of the above conditions How many transactions are in each of the above disc_rating categories? "],["lesson-6b-iteration-statements.html", "29 Lesson 6b: Iteration statements 29.1 Learning objectives 29.2 for loops 29.3 Controlling sequences 29.4 Repeating code for undefined number of iterations 29.5 Iteration with functional programming 29.6 Exercises", " 29 Lesson 6b: Iteration statements Often, we need to execute repetitive code statements a particular number of times. Or, we may even need to execute code for an undetermined number of times until a certain condition no longer holds. There are multiple ways we can achieve this and in this lesson we will cover several of the more common approaches to perform iteration. 29.1 Learning objectives By the end of this module you will be able to: Apply for, while, and repeat to execute repetitive code statements. Incorporate break and next to control looping statements. Use functional programming to perform repetitive code statements. 29.2 for loops The for loop is used to execute repetitive code statements for a particular number of times. The general syntax is provided below where i is the counter and as i assumes each sequential value the code in the body will be performed for that ith value. # syntax of for loop for (i in sequence) { &lt;do stuff here with i&gt; } There are three main components of a for loop to consider: Sequence: the sequence of values to iterate over. Body: apply some function(s) to the object we are iterating over. Output: You must specify what to do with the result. This may include printing out a result or modifying the object in place. For example, the following for loop iterates through each value (2018, 2011, …, 2022) and performs the paste and print functions inside the curly brackets. Note how we use year in place of i. Its always good to be more descriptive with your iteration terms so its clearer to others what you are referring to. years &lt;- c(2018:2022) for (year in years) { output &lt;- paste(&quot;The year is&quot;, year) print(output) } ## [1] &quot;The year is 2018&quot; ## [1] &quot;The year is 2019&quot; ## [1] &quot;The year is 2020&quot; ## [1] &quot;The year is 2021&quot; ## [1] &quot;The year is 2022&quot; In the above example, year refers directly to the value of the element in years. However, sometimes we want to refer to the index value rather than the value. To do this, we primarily use seq_along. seq_along() is a function that creates a vector that contains a sequence of numbers from 1 to the length of the object: seq_along(years) ## [1] 1 2 3 4 5 To use seq_along with a for loop we just specify it in the sequence portion of the for loop: for (index in seq_along(years)) { output &lt;- paste0(&quot;Element &quot;, index, &quot;: &quot;, years[index]) print(output) } ## [1] &quot;Element 1: 2018&quot; ## [1] &quot;Element 2: 2019&quot; ## [1] &quot;Element 3: 2020&quot; ## [1] &quot;Element 4: 2021&quot; ## [1] &quot;Element 5: 2022&quot; 29.2.1 Knowledge check We can see all data sets that we have in the “data” folder with list.files(): library(here) monthly_data_files &lt;- here(&quot;data/monthly_data&quot;) list.files(monthly_data_files) ## [1] &quot;Month-01.csv&quot; &quot;Month-02.csv&quot; &quot;Month-03.csv&quot; &quot;Month-04.csv&quot; &quot;Month-05.csv&quot; &quot;Month-06.csv&quot; ## [7] &quot;Month-07.csv&quot; &quot;Month-08.csv&quot; &quot;Month-09.csv&quot; &quot;Month-10.csv&quot; &quot;Month-11.csv&quot; Say we wanted to import one of these files into R: # here&#39;s a single file (first_df &lt;- list.files(monthly_data_files)[1]) ## [1] &quot;Month-01.csv&quot; # create path and import this single file df &lt;- readr::read_csv(here(monthly_data_files, first_df)) # create a new name for file (new_name &lt;- stringr::str_sub(first_df, end = -5)) ## [1] &quot;Month-01&quot; # dynamically rename file assign(new_name, df) Can you incorporate these procedures into a for loop to import all the data files? for(data_file in _____) { # 1: import data df &lt;- readr::read_csv(_____) # 2: remove &quot;.csv&quot; from file name new_name &lt;- _____ # 3: dynamically rename file assign(_____, _____) } 29.3 Controlling sequences There are two ways to control the progression of a loop: next: terminates the current iteration and advances to the next. break: exits the entire for loop. Both are used in conjunction with if statements. For example, this for loop will iterate for each element in year; however, when it gets to the element that equals the year of covid (2020) it will break out and end the for loop process. covid = 2020 for (year in years) { if (year == covid) break print(year) } ## [1] 2018 ## [1] 2019 The next argument is useful when we want to skip the current iteration of a loop without terminating it. On encountering next, the R parser skips further evaluation and starts the next iteration of the loop. In this example, the for loop will iterate for each element in year; however, when it gets to the element that equals covid it will skip the rest of the code execution simply jump to the next iteration. covid = 2020 for (year in years) { if (year == covid) next print(year) } ## [1] 2018 ## [1] 2019 ## [1] 2021 ## [1] 2022 29.3.1 Knowledge check The following code identifies the month of the data set from our monthly data files in the last knowledge check: # data files (data_files &lt;- list.files(monthly_data_files)) ## [1] &quot;Month-01.csv&quot; &quot;Month-02.csv&quot; &quot;Month-03.csv&quot; &quot;Month-04.csv&quot; &quot;Month-05.csv&quot; &quot;Month-06.csv&quot; ## [7] &quot;Month-07.csv&quot; &quot;Month-08.csv&quot; &quot;Month-09.csv&quot; &quot;Month-10.csv&quot; &quot;Month-11.csv&quot; # extract month number as.numeric(stringr::str_extract(data_files, &quot;\\\\d+&quot;)) ## [1] 1 2 3 4 5 6 7 8 9 10 11 Modify the following for loop with a next or break statement to: only import Month-01 through Month-07 only import Month-08 through Month-10 # Modify this code chunk with you next/break statement for(data_file in data_files) { # steps to import each data set df &lt;- readr::read_csv(paste0(&quot;data/&quot;, data_file)) new_name &lt;- stringr::str_sub(data_file, end = -5) assign(new_name, df) rm(df) } 29.4 Repeating code for undefined number of iterations Sometimes we need to execute code for an undetermined number of times until a certain condition no longer holds. There are two very similar options to do this: while loop repeat loop 29.4.1 while loop With a while loop we: Test condition first Then execute code while (condition) { &lt;do stuff&gt; } For example, the probability of flipping 10 coins and getting all heads or tails is \\((\\frac{1}{2})^{10} = 0.0009765625\\) (1 in 1024 tries). Let’s implement this and see how many times it’ll take to accomplish this feat. The following while statement will check if the number of unique values in flip are 1, which implies that we flipped all heads or tails. If it is not equal to 1 then we repeat the process of flipping 10 coins and incrementing the number of tries. When our condition statement length(unique(flip)) != 1 is FALSE then we exit the while loop. # create a coin coin &lt;- c(&quot;heads&quot;, &quot;tails&quot;) # set number of tries to zero n_tries &lt;- 0 # this will be used to imitate a flip of 10 coins flip &lt;- NULL while(length(unique(flip)) != 1) { # flip coin 10x flip &lt;- sample(coin, 10, replace = TRUE) # add to the number of tries n_tries &lt;- n_tries + 1 } n_tries ## [1] 553 29.4.2 repeat loop With a repeat loop we: Execute code first Then test condition repeat { &lt;do stuff&gt; if(condition) break } We can perform the same exercise as above to assess how many times it takes to flip 10 coins and get all heads or tails. The main difference here is repeat performs the action and then we incorporate a conditional statement to see if the flip is all heads or tails (length(unique(flip)) == 1) and if so we break out of the loop. Notice that you need to incorporate the conditional statement in repeat, otherwise it will continue looping indefinitely! coin &lt;- c(&quot;heads&quot;, &quot;tails&quot;) n_tries &lt;- 0 repeat { # flip coin 10x flip &lt;- sample(coin, 10, replace = TRUE) # add to the number of tries n_tries &lt;- n_tries + 1 # if current flip contains all heads or tails break if (length(unique(flip)) == 1) { print(n_tries) break } } ## [1] 63 29.4.3 Knowledge check An elementary example of a random walk is the random walk on the integer number line, \\(\\mathbb{Z}\\), which starts at 0 and at each step moves +1 or −1 with equal probability. Fill in the incomplete code chunk below perform a random walk starting at value 0, with each step either adding or subtracting 1. Have your random walk stop if the value it exceeds 100 or if the number of steps taken exceeds 10,000. value &lt;- 0 step &lt;- 0 repeat { # randomly add or subtract 1 random_step &lt;- sample(c(-1, 1), 1) value &lt;- value + _______ # count step step &lt;- step + __ # break once our walk exceeds 100 if (______ == 100 | _____ &gt; 10000) { print(step) break } } 29.5 Iteration with functional programming Iteration can be summed up as FOR EACH ____ DO ____. For example, in the previous knowledge checks we saw this code: data_files &lt;- data_files &lt;- list.files(monthly_data_files) for(data_file in data_files) { # steps to import each data set df &lt;- readr::read_csv(paste0(&quot;data/&quot;, data_file)) new_name &lt;- stringr::str_sub(data_file, end = -5) assign(new_name, df) rm(df) } The intent of this approach is FOR EACH file DO importing procedure. However, sometimes when reading for loops it’s tough to focus on the primary intent. Figure 29.1: Intent of iteration statements. Functional programming turns this idea into a function which, as we’ll see in later examples, can help to make iteration more efficient, strict, and explicit! Figure 29.2: Functional programming view of iteration statements. The purrr package provides functional programming tools that: for each element of x apply function f and provide consistent output The most common use of purrr functions focus around the family of map() functions. The family of map functions provided by purrr consist of vectorized functions which minimize your need to explicitly create loops. The initial functions we’ll explore include map() outputs a list. map_lgl() outputs a logical vector. map_int() outputs an integer vector. map_dbl() outputs a double vector. map_chr() outputs a character vector. map_df() outputs a data frame. These functions all behave in a similar manner - they each take a vector as input, applies a function to each piece, and then returns a new vector that’s the same length as the input. The primary difference is in the object they return. For example, say we wanted to iterate over the Complete Journey demographics data frame and compute the number of unique values for each column. We could do this with a for loop, which would look like this: cols &lt;- colnames(completejourney::demographics) distinct_values &lt;- vector(mode = &#39;integer&#39;, length = length(cols)) for (column in seq_along(cols)) { n_dist &lt;- n_distinct(completejourney::demographics[column]) distinct_values[column] &lt;- n_dist } distinct_values ## [1] 801 6 12 5 3 5 4 4 However, admittedly, this is a bit busy and its tough to see what the primary intent of this code is. Alternatively, we could apply purrr::map, which in this example will iterate over each column of completejourney::demographics and apply the n_distinct function. # tidyverse automatically loads purrr library(tidyverse) completejourney::demographics %&gt;% map(n_distinct) ## $household_id ## [1] 801 ## ## $age ## [1] 6 ## ## $income ## [1] 12 ## ## $home_ownership ## [1] 5 ## ## $marital_status ## [1] 3 ## ## $household_size ## [1] 5 ## ## $household_comp ## [1] 4 ## ## $kids_count ## [1] 4 Notice how the above results in a named list. If instead we wanted a vector to be returned we can us map_int: completejourney::demographics %&gt;% map_int(n_distinct) ## household_id age income home_ownership marital_status household_size ## 801 6 12 5 3 5 ## household_comp kids_count ## 4 4 We can apply other map functions to our input as well; we simply need to think about what we expect the output to be and that directs us to use the relevant map function: # logical output completejourney::demographics %&gt;% map_lgl(is.factor) ## household_id age income home_ownership marital_status household_size ## FALSE TRUE TRUE TRUE TRUE TRUE ## household_comp kids_count ## TRUE TRUE # integer output completejourney::demographics %&gt;% map_int(~ length(unique(.))) ## household_id age income home_ownership marital_status household_size ## 801 6 12 5 3 5 ## household_comp kids_count ## 4 4 Notice the last function applied looks different. The map functions provide a few shortcuts that you can use with the .f argument in order to save a little typing. The syntax for creating an anonymous function in R is quite verbose so purrr provides a convenient shortcut: a one-sided formula where . inside of unique points where the data should be evaluated. # traditional anonymous function approach completejourney::demographics %&gt;% map_int(function(x) length(unique(x))) ## household_id age income home_ownership marital_status household_size ## 801 6 12 5 3 5 ## household_comp kids_count ## 4 4 # one-sided formula approach completejourney::demographics %&gt;% map_int(~ length(unique(.))) ## household_id age income home_ownership marital_status household_size ## 801 6 12 5 3 5 ## household_comp kids_count ## 4 4 This, along with chaining multiple map functions together, can make for very efficient data mining. To provide a toy example, the following uses the diamonds data set provided by ggplot2 and: breaks the data set into separate data frames based on the cut variable, applies a linear regression model to each data frame, extracts the summary of each linear regression model, and computes the \\(R^2\\) for each model. Don’t worry, in the next module you’ll learn more about applying statistical models to your data. For now, just work through each line of code to get insight into the input and output of each map step. diamonds %&gt;% split(.$cut) %&gt;% map(~lm(price ~ carat, data = .)) %&gt;% map(summary) %&gt;% map_dbl(~.$r.squared) ## Fair Good Very Good Premium Ideal ## 0.73839 0.85095 0.85816 0.85563 0.86709 29.5.1 Knowledge check With the built-in airquality data set, use the most appropriate map functions to answer these three questions: How many n_distinct values are in each column? Are there any missing values in this data set? What is the standard deviation for each variable? If you want to get deeper into functional programming with purrr check out Section 21.5 of R for Data Science. 29.6 Exercises Use purrr::map_dfr to import each of the monthly_data/Month-XX.csv data files and combine into one single data frame. Check the current class of each column i.e. (class(df$Account_ID)). Since the time stamp variable has two classes, you can’t condense this down to an atomic vector. How many unique values exist in each column? "],["lesson-6c-writing-functions.html", "30 Lesson 6c: Writing functions 30.1 Learning objectives 30.2 When to write functions 30.3 Function components 30.4 Function arguments 30.5 Checking arguments and other conditions 30.6 Lazy evaluation 30.7 Status updates 30.8 Distributing your functions 30.9 Exercises 30.10 Additional resources", " 30 Lesson 6c: Writing functions R is a functional programming language, meaning that everything you do is basically built on functions. However, moving beyond simply using pre-built functions to writing your own functions is when your capabilities really start to take off and your code development/writing takes on a new level of efficiency. Functions allow you to reduce code duplication by automating a generalized task to be applied recursively. Whenever you catch yourself repeating a function or copy and pasting code there is a good change that you should write a function to eliminate the redundancies. Unfortunately, due to their abstractness, grasping the idea of writing functions (let alone writing them well) can take some time. However, in this lesson I provide you with the basic knowledge of how functions operate in R to get you started on the right path. 30.1 Learning objectives By the end of this lesson you will be able to: Identify when you should re-write code into a function. Understand the general components of functions. Allow users of your functions to specify function arguments in different ways. Incorporate messages, warnings, and error handling into your functions. Use evaluation rules so arguments can be used only when necessary. Save &amp; source your own functions for reuse. 30.2 When to write functions This section is taken directly from R for Data Science by Garrett Grolemund and Hadley Wickham and is a great example of how to first start thinking about functions. You should consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code). For example, take a look at this code. What does it do? df &lt;- data.frame( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) df$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE)) df$b &lt;- (df$b - min(df$b, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$b, na.rm = TRUE)) df$c &lt;- (df$c - min(df$c, na.rm = TRUE)) / (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE)) df$d &lt;- (df$d - min(df$d, na.rm = TRUE)) / (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE)) You might be able to puzzle out that this rescales each column to have a range from 0 to 1. But did you spot the mistake? I made an error when copying-and-pasting the code for df$b: I forgot to change an a to a b. Extracting repeated code out into a function is a good idea because it prevents you from making this type of mistake. To write a function you need to first analyze the code. How many inputs does it have? (df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE)) This code only has one input: df$a. (It’s a little suprisingly that TRUE is not an input: you can explore why in the exercise below). To make the single input more clear, it’s a good idea to rewrite the code using temporary variables with a general name. Here this function only takes one vector of input, so I’ll call it x: x &lt;- 1:10 (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)) ## [1] 0.00000 0.11111 0.22222 0.33333 0.44444 0.55556 0.66667 0.77778 0.88889 1.00000 There is some duplication in this code. We’re computing the range of the data three times, but it makes sense to do it in one step: rng &lt;- range(x, na.rm = TRUE) (x - rng[1]) / (rng[2] - rng[1]) ## [1] 0.00000 0.11111 0.22222 0.33333 0.44444 0.55556 0.66667 0.77778 0.88889 1.00000 Pulling out intermediate calculations into named variables is a good practice because it makes it more clear what the code is doing. Now that I’ve simplified the code, and checked that it still works, I can turn it into a function: rescale01 &lt;- function(x) { rng &lt;- range(x, na.rm = TRUE) (x - rng[1]) / (rng[2] - rng[1]) } rescale01(c(0, 5, 10)) ## [1] 0.0 0.5 1.0 30.3 Function components With the exception of primitive functions all R functions have three parts: body(): the code inside the function formals(): the list of arguments used to call the function environment(): the mapping of the location(s) of the function’s variables In most of the subsequent examples, we will be using the transaction table from the Complete Journey data. transactions &lt;- completejourney::transactions_sample To illustrate the function components we talked about, let’s build a function that finds the total sales for a store, for a given week. In the function that follows, the body of the function includes the summation equation that needs to be calculated, and then rounded to two decimals. The formals (or arguments) required for the function are the store_id, and the specific week for which we need the sales. And the environment shows that function operates in the global environment. sales &lt;- function(store, week) { # aggregation function store_sales &lt;- aggregate( transactions$sales_value, by = list( store_id = transactions$store_id, week = transactions$week), FUN = &quot;sum&quot; ) # get result for relevant store &amp; week valid_store &lt;- store_sales$store_id == store valid_week &lt;- store_sales$week == week result &lt;- store_sales[valid_store &amp; valid_week, &#39;x&#39;] # round and return result round(result, digits = 2) } body(sales) ## { ## store_sales &lt;- aggregate(transactions$sales_value, by = list(store_id = transactions$store_id, ## week = transactions$week), FUN = &quot;sum&quot;) ## valid_store &lt;- store_sales$store_id == store ## valid_week &lt;- store_sales$week == week ## result &lt;- store_sales[valid_store &amp; valid_week, &quot;x&quot;] ## round(result, digits = 2) ## } formals(sales) ## $store ## ## ## $week environment(sales) ## &lt;environment: R_GlobalEnv&gt; 30.3.1 Knowledge check Identify the arguments, body, and environment of: read.csv dplyr::add_count sum Do one of the above functions behave a bit differently when you try to get the body and/or environment? 30.4 Function arguments To perform the sales() function we can call the arguments in different ways. # using argument names sales(store = 309, week = 31) ## [1] 29.76 # same as above but without using names (aka &quot;positional matching&quot;) sales(309, 31) ## [1] 29.76 # if using names you can change the order sales(week = 31, store = 309) ## [1] 29.76 # if not using names you must insert arguments in proper order # in this e.g. the function assumes store_id = 317, and week = 27 sales(317, 27) ## [1] 17.1 Note that when building a function you can also set default values for arguments. In our original sales(), we did not provide any default values, so if we do not supply all the argument parameters an error will be returned. However, if we set default values then the function will use the stated default if any parameters are missing: # missing the week argument sales(309) ## Error in sales(309): argument &quot;week&quot; is missing, with no default # creating default argument values sales &lt;- function(store, week = 52) { # aggregation function store_sales &lt;- aggregate( transactions$sales_value, by = list( store_id = transactions$store_id, week = transactions$week), FUN = &quot;sum&quot; ) # get result for relevant store &amp; week valid_store &lt;- store_sales$store_id == store valid_week &lt;- store_sales$week == week result &lt;- store_sales[valid_store &amp; valid_week, &#39;x&#39;] # round and return result round(result, digits = 2) } # function will use default values sales(309) ## [1] 6.49 # specifying a different week value sales(309, 48) ## [1] 25.14 30.4.1 Knowledge check Define a function titled ratio that takes arguments x and y and returns their ratio, \\(\\frac{x}{y}\\). Now add a third variable digits that allows you to round the output to a specified decimal. Now set the default to 2. 30.5 Checking arguments and other conditions We’ve created a function that allows users to specify their inputs. But how do we ensure they provide us with the right kind of inputs? Or what if we want to be able to provide some sort of feedback to the user? There are several ways to signal conditions to function users: stop() &amp; stopifnot(): signal an error (no way for a function to continue and execution must stop) warning(): signal a warning (something has gone wrong but the function has been able to at least partially recover.) message(): signal an informative message (function works fine but user should be informed of something.) The stop functions are most commonly used to check for proper inputs but can be used to stop the function procedures because the user’s environment is not properly established. When applying stop we should always weigh the benefits of highly custom, informative error messages (via stop) versus short to the point (via stopifnot) When using the stop functions execution will stop at the first violation. sales &lt;- function(store, week = 52) { # validate arguments if (!is.numeric(store) | !is.numeric(week)) { stop(&quot;`store` and `week` arguments must be numeric values&quot;, call. = FALSE) } # aggregation function store_sales &lt;- aggregate( transactions$sales_value, by = list( store_id = transactions$store_id, week = transactions$week ), FUN = &quot;sum&quot; ) # get result for relevant store &amp; week valid_store &lt;- store_sales$store_id == store valid_week &lt;- store_sales$week == week result &lt;- store_sales[valid_store &amp; valid_week, &#39;x&#39;] # round result round(result, digits = 2) } # valid inputs sales(309, 48) ## Error in match.fun(FUN): argument &quot;FUN&quot; is missing, with no default # invalid inputs sales(309, &quot;48&quot;) ## Error: `store` and `week` arguments must be numeric values warnings() are rarely used but can be useful to signal non-terminating concerns such as: hyperparameter clashes in a grid search an extremely long compute time and the user may want to re-assess deprecated functions NAs, NULLs, Infs exist, which result in NA output For example, many of the lubridate functions throw a warning message if one of the dates in a vector are not in a standard date format that includes day, month and year: lubridate::mdy(c(&quot;2-1-2019&quot;, &quot;1995&quot;)) ## Warning: 1 failed to parse. ## [1] &quot;2019-02-01&quot; NA For example, say that we knew there was an issue with our data collection process for the last week of the year and incomplete sales data was reported. If we want to ensure users of our function are aware of this concern we could add a warning message. sales &lt;- function(store, week = 52) { # add warning message if (week == 52) { warning(&quot;Due to EOY reporting issues week 52 has incomplete sales data&quot;, call. = FALSE) } # check inputs if (!is.numeric(store) | !is.numeric(week)) { stop(&quot;`store` and `week` arguments must be numeric values&quot;, call. = FALSE) } # aggregation function store_sales &lt;- aggregate( transactions$sales_value, by = list( store_id = transactions$store_id, week = transactions$week ), FUN = &quot;sum&quot; ) # get result for relevant store &amp; week valid_store &lt;- store_sales$store_id == store valid_week &lt;- store_sales$week == week result &lt;- store_sales[valid_store &amp; valid_week, &#39;x&#39;] # round result round(result, digits = 2) } sales(309, 52) ## Warning: Due to EOY reporting issues week 52 has incomplete sales data ## [1] 6.49 message() is designed to be informational and we use them to tell the user that you’ve done something on their behalf. Examples include: Setting default arguments that may be non-trivial library(ggplot2) ggplot(iris, aes(Sepal.Length)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. how information was interpreted by the function df &lt;- readr::read_csv(here::here(&quot;data&quot;, &quot;monthly_data&quot;, &quot;Month-01.csv&quot;)) ## Rows: 54535 Columns: 10 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): Factor_C, Factor_E, Transaction_Status, Month ## dbl (5): Account_ID, Factor_A, Factor_B, Factor_D, Response ## dttm (1): Transaction_Timestamp ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Or any other reasons. In our case, we may want to signal to the user how many observations are being included in our aggregation. When providing messages like the below, its always good practice to provide an option to suppress messages. sales &lt;- function(store, week = 52, verbose = TRUE) { # add warning message if (week == 52) { warning(&quot;Due to EOY reporting issues week 52 has incomplete sales data&quot;, call. = FALSE) } # check inputs if (!is.numeric(store) | !is.numeric(week)) { stop(&quot;`store` and `week` arguments must be numeric values&quot;, call. = FALSE) } # aggregation function store_sales &lt;- aggregate( transactions$sales_value, by = list( store_id = transactions$store_id, week = transactions$week), FUN = &quot;sum&quot; ) # get n observations valid_store = transactions$store_id == store valid_week = transactions$week == week n_obs = nrow(transactions[valid_store &amp; valid_week, ]) # provide message if (verbose) { message(&quot;Store &quot;, store, &quot; had &quot;, n_obs, &quot; transactions during week &quot;, week) } # round output round(store_sales$x[which(store_sales$store_id == store &amp; store_sales$week == week)], 2) } sales(309, 50) ## Store 309 had 4 transactions during week 50 ## [1] 10.34 30.5.1 Knowledge check Take a look at the following function that we introduced at the beginning of this lesson: rescale &lt;- function(x, digits = 2){ rng &lt;- range(x, na.rm = TRUE) scaled &lt;- (x - rng[1]) / (rng[2] - rng[1]) round(scaled, digits = digits) } # test vector set.seed(123) test_vector &lt;- c(runif(20, min = 25, max = 40), NA) rescale(test_vector) ## [1] 0.27 0.82 0.40 0.92 0.98 0.00 0.53 0.93 0.56 0.45 1.00 0.45 0.69 0.58 0.07 0.94 0.22 0.00 0.31 1.00 NA Move the na.rm = TRUE option to the function arguments so that the user can control whether or not to remove missing values. Add a stop that throws an error if the user supplies non-numeric values for x. Add a warning() that tells the user NAs are present (if they use na.rm = FALSE) Add a message() that tells the user how many NAs were removed Your results should look something like this: # results in an error rescale(letters) ## Error: `x` must be a numeric atomic vector # results in warning rescale(test_vector, na.rm = FALSE) ## Warning: There are 1 NAs. Remove them with `na.rm = TRUE` ## [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA # results in message rescale(test_vector) ## 1 NAs were removed. ## [1] 0.27 0.82 0.40 0.92 0.98 0.00 0.53 0.93 0.56 0.45 1.00 0.45 0.69 0.58 0.07 0.94 0.22 0.00 0.31 1.00 30.6 Lazy evaluation R functions perform “lazy” evaluation, in which arguments are only evaluated if required in the body of the function. Take the following function as an example, x is not used in the body of the function so even passing stop to x does not result in an error because x is never evaluated. h01 &lt;- function(x) { 10 } h01(x = stop(&quot;This is an error!&quot;)) ## [1] 10 This provides you with lots of flexibility in when and how to evaluate inputs h02 &lt;- function(x, y, z) { if(missing(z)) { sum(x, y) } else { sum(x, y, z) } } # z does not exist h02(x = 1, y = 2) ## [1] 3 # z does exist h02(x = 1, y = 2, z = 3) ## [1] 6 This may not seem useful but one of the more common applications is to use NULL to indicate that a variable is not required but can be used if supplied. For example, in the following function we add a multiplier argument. When multiplier is NULL we do not do anything with it but when its not NULL we multiply our output (pv) by the supplied value. compute_pv &lt;- function(fv, r, n, multiplier = NULL) { # compute present value pv &lt;- round(fv / (1 + r)^n) # apply multiplier if its supplied if (is.null(multiplier)) { pv } else { pv * multiplier } } # w/o multiplier compute_pv(1000, .05, 10) ## [1] 614 # with multiplier compute_pv(1000, .05, 10, multiplier = 1.25) ## [1] 767.5 30.7 Status updates When functions include an iterative nature (i.e. looping), a nice feature to include in your functions, both for yourself and other analysts, is a status output to inform the user how much computation has been done (i.e. how many loops have been performed) and how much is left. 30.7.1 Option 1: Base R In base R we can create a simplistic status output by using txtProgressBar and setTxtProgressBar. There is some customization options with these such as width of progress bar, line type, etc. Also, note that it is good practice to add an argument that lets the user run the function in silent mode (provides more flexibility). f1 &lt;- function(x, n, verbose = TRUE) { status &lt;- NULL iterate &lt;- 1:n # create progress bar if(verbose) pb &lt;- txtProgressBar(min = 0, max = length(n), style = 3) # some iteration in your loop for(i in seq_along(iterate)) { # some functionality Sys.sleep(x) # update progress bar if(verbose) setTxtProgressBar(pb, i/n) } } f1(x = .2, n = 10) ## | | | 0% | |========= | 10% | |================= | 20% | |========================== | 30% | |================================== | 40% | |=========================================== | 50% | |==================================================== | 60% | |============================================================ | 70% | |===================================================================== | 80% | |============================================================================= | 90% | |======================================================================================| 100% 30.7.2 Option 2: progress package An alternative approach is the progress package. progress allows you to provide estimated time until completion, elapsed time in seconds, a spinner, etc. progress is also useful when your function is downloading many files as it will show download rate (bytes per second) and total bytes downloaded. Check out ?progress_bar for more details. f2 &lt;- function(x, n, verbose = TRUE) { iterate &lt;- 1:n # create progress bar pb &lt;- progress::progress_bar$new( format = &quot; computing [:bar] :percent eta: :eta&quot;, total = n, width = 60, force = TRUE ) for (i in seq_along(iterate)) { # some functionality Sys.sleep(x) # report progress if(verbose) pb$tick() } } f2(.3, 20) ## computing [==&gt;-----------------------------] 10% eta: 3s computing ## [====&gt;---------------------------] 15% eta: 3s computing [=====&gt;--------------------------] ## 20% eta: 4s computing [=======&gt;------------------------] 25% eta: 4s computing ## [=========&gt;----------------------] 30% eta: 4s computing [==========&gt;---------------------] ## 35% eta: 3s computing [============&gt;-------------------] 40% eta: 3s computing ## [=============&gt;------------------] 45% eta: 3s computing [===============&gt;----------------] ## 50% eta: 3s computing [=================&gt;--------------] 55% eta: 2s computing ## [==================&gt;-------------] 60% eta: 2s computing [====================&gt;-----------] ## 65% eta: 2s computing [=====================&gt;----------] 70% eta: 2s computing ## [=======================&gt;--------] 75% eta: 1s computing [=========================&gt;------] ## 80% eta: 1s computing [==========================&gt;-----] 85% eta: 1s computing ## [============================&gt;---] 90% eta: 1s computing [=============================&gt;--] 95% ## eta: 0s computing [================================] 100% eta: 0s 30.8 Distributing your functions If you want to save a function to be used at other times and within other scripts there are two main ways to do this. One way is to build a package which is discussed in more details here. Another option, and the one discussed here, is to save the function in a script. For example, we can save a script that contains a PV() function that computes the present value. Figure 30.1: PV.R script that contains a function to compute the present value. Now, if we are working in a fresh script you’ll see that we have no objects and functions in our working environment: Figure 30.2: Fresh session. If we want to use the PV function in th PV.R script we can simply read in the function by sourcing the script using source(\"PV.R\"). Now, you’ll notice that we have the PV() function in our global environment and can use it as normal. Note that if you are working in a different directory then where the PV.R file is located you’ll need to include the proper command to access the relevant directory. Figure 30.3: Sourcing the PV.R script. 30.9 Exercises Practice writing the following functions and test them on the given test vector. There is source code in the following code chunk that you can re-write into the following functions: variance() std_dev() std_error() skewness() Be sure to add a parameter that handles missing values and add appropriate warnings and messages. # create a function that computes the variance of x (1 / (length(x) - 1)) * sum((x - mean(x))^2) # create a function that computes the standard deviation of x sqrt((1 / (length(x) - 1)) * sum((x - mean(x))^2)) # create a function that computes the standard error of x var_x &lt;- (1 / (length(x) - 1)) * sum((x - mean(x))^2) sqrt(var_x / length(x)) # create a function that computes the skewness of x n &lt;- length(x) v &lt;- var(x) m &lt;- mean(x) third.moment &lt;- (1 / (n - 2)) * sum((x - m)^3) third.moment / (var(x)^(3 / 2)) Using the following test vector you should get these results: # test vector set.seed(123) x &lt;- rlnorm(100) # variance function output variance(x) ## [1] 3 # stardard dev output std_dev(x) ## [1] 1.7 # standard error output std_error(x) ## [1] 0.17 # skewness output skewness(x) ## [1] 2.3 Now store these functions in a separate script and source them in a fresh R session. 30.10 Additional resources Functions are a fundamental building block of R and writing functions is a core activity of an R programmer. It represents the key step of the transition from a mere “user” to a developer who creates new functionality for R. As a result, its important to turn your existing, informal knowledge of functions into a rigorous understanding of what functions are and how they work. A few additional resources that can help you get to the next step of understanding functions include: Hadley Wickham’s Advanced R book Roger Peng’s R Programming for Data Science book DataCamp’s Intermediate R course Coursera’s R Programming course "],["lab-5.html", "31 Lab", " 31 Lab TBD "],["overview-6.html", "32 Overview 32.1 Learning objectives 32.2 Tasks 32.3 Course readings", " 32 Overview Now that you are equipped with powerful programming tools we can finally move into the world of applied modeling. In this module we’ll build on your new tools of data wrangling and programming to fit, analyze, and understand different applied modeling techniques. First, you will gain a basic understanding of discovering relationships across variables and using models for data exploration. Then you will learn about the machine learning process and how to start applying it with R. This will give you the foundation to start building your own machine learning toolbox. 32.1 Learning objectives By the end of this module you should be able to: Identify covariate relationships within your data. Explain the modeling process and apply it within your R workflow. Apply and explain the results of a few foundational machine learning algorithms. 32.2 Tasks TBD 32.3 Course readings TBD "],["lesson-1.html", "33 Lesson 1 33.1 Learning objectives 33.2 Content A 33.3 Content B 33.4 Exercises", " 33 Lesson 1 TBD 33.1 Learning objectives 33.2 Content A 33.3 Content B 33.4 Exercises "],["lesson-2.html", "34 Lesson 2 34.1 Learning objectives 34.2 Content A 34.3 Content B 34.4 Exercises", " 34 Lesson 2 TBD 34.1 Learning objectives 34.2 Content A 34.3 Content B 34.4 Exercises "],["lab-6.html", "35 Lab", " 35 Lab TBD "],["computing-environment-1.html", "Computing Environment", " Computing Environment This book was built with the following computing environment and packages: sessioninfo::session_info(pkgs = &#39;attached&#39;) ## ─ Session info ──────────────────────────────────────────────────────────────────────────────────────────────────────── ## setting value ## version R version 4.2.0 (2022-04-22) ## os macOS Monterey 12.4 ## system x86_64, darwin17.0 ## ui RStudio ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/New_York ## date 2022-06-14 ## rstudio 2022.02.3+492 Prairie Trillium (desktop) ## pandoc 2.18 @ /usr/local/bin/ (via rmarkdown) ## ## ─ Packages ──────────────────────────────────────────────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## completejourney * 1.1.0 2019-09-28 [1] CRAN (R 4.2.0) ## dbplyr * 2.1.1 2021-04-06 [1] CRAN (R 4.2.0) ## dplyr * 1.0.9 2022-04-28 [1] CRAN (R 4.2.0) ## forcats * 0.5.1 2021-01-27 [1] CRAN (R 4.2.0) ## ggplot2 * 3.3.6 2022-05-03 [1] CRAN (R 4.2.0) ## ggrepel * 0.9.1 2021-01-15 [1] CRAN (R 4.2.0) ## gridExtra * 2.3 2017-09-09 [1] CRAN (R 4.2.0) ## here * 1.0.1 2020-12-13 [1] CRAN (R 4.2.0) ## jsonlite * 1.8.0 2022-02-22 [1] CRAN (R 4.2.0) ## lubridate * 1.8.0 2021-10-07 [1] CRAN (R 4.2.0) ## magrittr * 2.0.3 2022-03-30 [1] CRAN (R 4.2.0) ## naniar * 0.6.1 2021-05-14 [1] CRAN (R 4.2.0) ## nycflights13 * 1.0.2 2021-04-12 [1] CRAN (R 4.2.0) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.2.0) ## readr * 2.1.2 2022-01-30 [1] CRAN (R 4.2.0) ## readxl * 1.4.0 2022-03-28 [1] CRAN (R 4.2.0) ## RSQLite * 2.2.14 2022-05-07 [1] CRAN (R 4.2.0) ## stringr * 1.4.0 2019-02-10 [1] CRAN (R 4.2.0) ## tibble * 3.1.7 2022-05-03 [1] CRAN (R 4.2.0) ## tidyr * 1.2.0 2022-02-01 [1] CRAN (R 4.2.0) ## tidyverse * 1.3.1 2021-04-15 [1] CRAN (R 4.2.0) ## ## [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library ## ## ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
