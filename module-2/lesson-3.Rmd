# Lesson 2c: Importing data

```{r setup-2c, include=FALSE}
# clean up saved items that may carry over from previous lessons
rm(list = ls())

knitr::opts_chunk$set(
  fig.align = "center",
  collapse = TRUE
)
```

The first step to any data analysis process is to get the data. Data can come from many sources but two of the most common include delimited and Excel files. This section covers how to import data from these common files; plus we cover other important topics such as understanding file paths, connecting to SQL databases, and to load data from saved R object files.

## Learning objectives

Upon completing this module you will be able to:

1. Describe how imported data affects computer memory.
2. Import tabular data with R.
3. Assess some basic attributes of your imported data.
4. Import alternative data files such as SQL tables and Rdata files.

## Data & memory

R stores its data in memory - this makes it relatively quickly accessible but can cause size limitations in certain fields. In this class we will mainly work with small to moderate data sets, which means we should not run into any space limitations.

```{block, type='note'}
R does provide tooling that allows you to work with big data via distributed data (i.e. [sparklyr](https://spark.rstudio.com/)) and relational databrases (i.e. SQL).
```

R memory is session-specific, so quitting R (i.e. shutting down RStudio) removes the data from memory. A general way to conceptualize data import into and use within R:

1. Data sits in on the computer/server - this is frequently called “disk”
2. R code can be used to copy a data file from disk to the R session’s memory
3. R data then sits within R’s memory ready to be used by other R code

Here is a visualization of this process:

```{r, echo=FALSE, fig.align='center', fig.cap='Conceptualizing how R imports data.'}
knitr::include_graphics("images/import-framework.png")
```

## Delimited files

Text files are a popular way to hold and exchange tabular data as almost any data application supports exporting data to the CSV (or other text file) format. Text file formats use delimiters to separate the different elements in a line, and each line of data is in its own line in the text file. Therefore, importing different kinds of text files can follow a fairly consistent process once you’ve identified the delimiter.

There are three main groups of functions that we can use to read in text files:

* Base R functions
* *readr** package functions
* *vroom** package functions

Here, we'll focus on the middle one.

```{block, type='note'}
All three functions will import a tabular delimited file (.csv, .tsv, .txt, etc.) and convert it to a data frame in R, but each has subtle differences. You can read why we favor **readr** and **vroom** over the base R importing functions (i.e. `read.csv()`) [here](https://r4ds.had.co.nz/data-import.html#compared-to-base-r).

We will not cover the **vroom** package but it is good to know about as it can be extremely fast for very large data sets. Read more about **vroom** [here](https://www.tidyverse.org/blog/2019/05/vroom-1-0-0/).
```

The following will import a data set describing the sale of individual residential property in Ames, Iowa from 2006 to 2010 ([source](http://jse.amstat.org/v19n3/decock.pdf)).

```{block, type='note'}
You can download this data from the supplemental files in Canvas.
```


```{r ames-import}
library(readr)

ames <- read_csv('data/ames_raw.csv')
```

We can look at it in our notebook or console and we see that it is displayed in a well-organized, concise manner. More on this in a second.

```{r}
ames
```
If we check the class of our object we do see that it is a **data.frame**; however, we also see some additional information. That's because this is a special kind of data frame known as a **tibble**.

```{r}
class(ames)
```

```{block, type='video'}
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/wjdC5pDSZBk?si=7myENjYK-l3wvfhb" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
```


### Tibbles

**Tibbles** _are_ data frames, but they tweak some older behaviors of data frames to make life a little easier. There are two main differences in the usage of a tibble vs. a classic data frame: printing and subsetting.

Tibbles have a refined print method that shows only the first 10 rows, and all the columns that fit on your screen. This makes it much easier to work with large data. We can see this difference with the following:

```{r}
# printed output of a tibble
ames
```

```{r}
# printed output of a regular data frame
as.data.frame(ames)
```

The differences in the subsetting are not important at this time but the main takeaway is that tibbles are more strict and will behave more consistently then data frames for certain subsetting tasks.

```{block, type='tip'}
Read more about tibbles [here](https://r4ds.had.co.nz/tibbles.html#tibbles).
```

### File paths

This is a good time to have a discussion on file paths. It’s important to understand where files exist on your computer and how to reference those paths. There are two main approaches:

* Absolute paths
* Relative paths

An **absolute path** always contains the root elements and the complete list of directories to locate the specific file or folder. For the ames_raw.csv file, the absolute path on my computer is:

```{r abs-path}
library(here)

absolute_path <- here('data/ames_raw.csv')
absolute_path
```

I can always use the absolute path in `read_csv()`:

```{r}
ames <- read_csv(absolute_path)
```

In contrast, a relative path is a path built starting from the current location. For example, say that I am operating in a directory called “Project A”. If I’m working in “my_notebook.Rmd” and I have a “my_data.csv” file in that same directory:

```bash
# illustration of the directory layout
Project A
├── my_notebook.Rmd
└── my_data.csv
```

Then I can use this relative path to import this file: `read_csv('my_data.csv')`. This just means to look for the ‘my_data.csv’ file relative to the current directory that I am in.

Often, people store data in a “data” directory. If this directory is a subdirectory within my Project A directory:

```bash
# illustration of the directory layout
Project A
├── my_notebook.Rmd
└── data
    └── my_data.csv
```

Then I can use this relative path to import this file: `read_csv('data/my_data.csv')`. This just means to look for the ‘data’ subdirectory relative to the current directory that I am in and then look for the ‘my_data.csv’ file.

Sometimes, the data directory may not be in the current directory. Sometimes a project directory will look like the following where there is a subdirectory containing multiple notebooks and then another subdirectory containing data assets. If you are working in “notebook1.Rmd” within the notebooks subdirectory, you will need to tell R to go up one directory relative to the notebook you are working in to the main Project A directory and then go down into the data directory.

```bash
# illustration of the directory layout
Project A
├── notebooks
│   ├── notebook1.Rmd
│   ├── notebook2.Rmd
│   └── notebook3.Rmd
└── data
    └── my_data.csv
```

I can do this by using dot-notation in my relative path specification - here I use ‘..’ to imply “go up one directory relative to my current location”: `read_csv('../data/my_data.csv')`.

Note that the path specified in `read_csv()` does not need to be a local path. For example, the ames_raw.csv data is located online at https://raw.githubusercontent.com/bradleyboehmke/uc-bana-7025/main/data/ames_raw.csv. We can use `read_csv()` to import directly from this location:

```{r}
url <- 'https://raw.githubusercontent.com/bradleyboehmke/uc-bana-7025/main/data/ames_raw.csv'
ames <- read_csv(url)
```


### Metadata

Once we’ve imported the data we can get some descriptive metadata about our data frame. For example, we can get the dimensions of our data frame. Here, we see that we have 2,930 rows and 82 columns.

```{r}
dim(ames)
```

We can also see all the names of our columns:

```{r}
names(ames)
```

You may have also noticed the message each time we read in the data set that identified the delimiter and it also showed the following, which states that when we read in the data, 45 variables were read in as character strings and 37 were read in as double floating points.

```
chr (45): PID, MS SubClass, MS Zoning, Street, Alley, Lot Shape, Land Contour, Utilities,...
dbl (37): Order, Lot Frontage, Lot Area, Overall Qual, Overall Cond, Year Built, Year Rem...
```

There was also a message that stated _"use `spec()` to retrieve the full column specification for this data."_ When we do so we see that it lists all the columns and the data type that were read in as.

```{r}
spec(ames)
```

Lastly, its always good to understand if, and how many, missing values are in the data set we can do this easily by running the following, which shows 13,997 elements are missing.  That's a lot!

```{r}
sum(is.na(ames))
```

We can even apply some operators and indexing procedures we learned in previous lessons to quickly view all columns with missing values and get a total sum of the missing values within those columns.

```{block, type='note'}
In future modules we'll learn different ways we can handle these missing values.
```

```{r}
missing_values <- colSums(is.na(ames))
sort(missing_values[missing_values > 0], decreasing = TRUE)
```

### Knowledge check

```{block, type='todo'}
1. Check out the help documentation for `read_csv()` by executing `?read_csv`. What parameter in `read_csv()` allows us to specify values that represent missing values?
2. Read in this [energy_consumption.csv](https://raw.githubusercontent.com/bradleyboehmke/uc-bana-7025/main/data/energy_consumption.csv) file.
3. What are the dimensions of this data? What data type is each column?
4. Apply `dplyr::glimpse(ames)`. What information does this provide?
```

```{block, type='video'}
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Ap8wJhAWLRE?si=6dzzqSvbnrY7NdgP&amp;start=1050" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
```


## Excel files

With Excel still being the spreadsheet software of choice its important to be able to efficiently import and export data from these files. Often, many users will simply resort to exporting the Excel file as a CSV file and then import into R using `readxl::read_excel`; however, this is far from efficient. This section will teach you how to eliminate the CSV step and to import data directly from Excel using the **readxl** package.

To illustrate, we’ll import so mock grocery store products data located in a products.xlsx file.

To read in Excel data you will use the `excel_sheets()` functions. This allows you to read the names of the different worksheets in the Excel workbook and identify the specific worksheet of interest and then specify that in read_excel.

```{r excel}
library(readxl)

excel_sheets('data/products.xlsx')
```

```{block, type='warning'}
If you don’t explicitly specify a sheet then the first worksheet will be imported.
```

```{r excel-import}
products <- read_excel('data/products.xlsx', sheet = 'products data')
products
```

```{block, type='video'}
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/wjdC5pDSZBk?si=qPa4fQ-pWMCvJILQ&amp;start=168" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
```


## SQL databases

Many organizations continue to use relational databases along with SQL to interact with these data assets. 

```{block, type='note'}
If you are unfamiliar with relational databases and SQL then [this is a quick read](https://datacarpentry.org/sql-ecology-lesson/00-sql-introduction/index.html) that explains the benefits of these tools.
```

R can connect to almost any existing database type. Most common database types have R packages that allow you to connect to them (e.g., RSQLite, RMySQL, etc). Furthermore, the [DBI](https://dbi.r-dbi.org/) and [dbplyr](https://dbplyr.tidyverse.org/) packages support connecting to the widely-used open source databases [sqlite](https://sqlite.org/), [mysql](https://www.mysql.com/) and [postgresql](https://www.postgresql.org/), as well as Google’s [bigquery](https://cloud.google.com/bigquery/), and it can also be extended to other database types . 

```{block, type='tip'}
RStudio has created a [website](http://db.rstudio.com/) that provides documentation and best practices to work on database interfaces.
```

In this example I will illustrate connecting to a local sqlite database. First, let's load the libraries we'll need:

```{r}
library(dbplyr)
library(dplyr)
library(RSQLite)
```


The following illustrates with the example [Chinook Database](https://www.sqlitetutorial.net/sqlite-sample-database/), which I’ve downloaded to my data directory.

```{block, type='note'}
The following uses 2 functions that talk to the SQLite database. `dbconnect()` comes from the **DBI** package and is not something that you’ll use directly as a user. It simply allows R to send commands to databases irrespective of the database management system used. The `SQLite()` function from the **RSQLite** package allows R to interface with SQLite databases.
```

```{r}
chinook <- dbConnect(SQLite(), "data/chinook.db")
```

This command does not load the data into the R session (as the `read_csv()` function did). Instead, it merely instructs R to connect to the SQLite database contained in the chinook.db file. Using a similar approach, you could connect to many other database management systems that are supported by R including MySQL, PostgreSQL, BigQuery, etc.

Let’s take a closer look at the chinook database we just connected to:

```{r}
src_dbi(chinook)
```

Just like a spreadsheet with multiple worksheets, a SQLite database can contain multiple tables. In this case there are several tables listed in the tbls row in the output above:

* albums
* artists
* customers
* etc.

Once you've made the connection, you can use `tbl()` to read in the “tracks” table directly as a data frame.

```{r}
tracks <- tbl(chinook, 'tracks')
```

If you are familiar with SQL then you can even pass a SQL query directly in the `tbl()` call using **dbplyr**'s `sql()`. For example, the following SQL query:

1. SELECTS the name, composer, and milliseconds columns,
2. FROM the tracks table,
3. WHERE observations in the milliseconds column are greater than 200,000 and WHERE observations in the composer column are not missing (NULL)

```{r}
sql_query <- "SELECT name, composer, milliseconds FROM tracks WHERE milliseconds > 200000 and composer is not null"

long_tracks <- tbl(chinook, sql(sql_query))
long_tracks
```

```{r, echo=FALSE, message=FALSE}
dbDisconnect(chinook)
```


## Many other file types

There are many other file types that you may encounter in your career. Most of which we can import into R one way or another. For example the [**haven**](https://haven.tidyverse.org/) package reads SPSS, Stata, and SAS files, [**xml2**](https://cran.r-project.org/web/packages/xml2/index.html) allows you to read in XML, and [**rvest**](https://rvest.tidyverse.org/) helps to scrape data from HTML web pages.  

Two non-tabular file types you may experience in practice are JSON files and R object files.

### JSON files

[JSON files](https://en.wikipedia.org/wiki/JSON) are non-tabular data files that are popular in data engineering due to their space efficiency and flexibility. Here is an example JSON file:

```json
{
    "planeId": "1xc2345g",
    "manufacturerDetails": {
        "manufacturer": "Airbus",
        "model": "A330",
        "year": 1999
    },
    "airlineDetails": {
        "currentAirline": "Southwest",
        "previousAirlines": {
            "1st": "Delta"
        },
        "lastPurchased": 2013
    },
    "numberOfFlights": 4654
}
```

JSON Files can be imported using the [jsonlite](https://github.com/jeroen/jsonlite) library.

```{r json-example}
library(jsonlite)

imported_json <- fromJSON('data/json_example.json')
```

Since JSON files can have multiple dimensions, jsonlite will import JSON files as a list since that is the most flexible data structure in R.

```{r}
class(imported_json)
```

And we can view the data:

```{r}
imported_json
```


### R object files

Sometimes you may need to save data or other R objects outside of your workspace. You may want to share R data/objects with co-workers, transfer between projects or computers, or simply archive them. There are three primary ways that people tend to save R data/objects: as .RData, .rda, or as .rds files. 

```{block, type='note'}
You can read about the differences of these R objects [here](http://uc-r.github.io/exporting#export_r_objects).
```

If you have an .RData or .rda file you need to load you can do so with the following. You will not seen any output in your console or script because this simply loads the data objects within `xy.RData` into your global environment.

```{r rdata-example}
load('data/xy.RData')
```

For .rds files you can use **readr**'s `read_rds()` function:

```{r rds-example}
read_rds('data/x.rds')
```


## Exercises

```{block, type='todo'}
1. R stores its data in _______ .
2. What happens to R’s data when the R or RStudio session is terminated?
3. Load the hearts.csv data file into R using the readr library.
4. What are the dimensions of this data? What data types are the variables in this data set?
5. Use the `head()` and `tail()` functions to assess the first and last 15 rows of this data set.
6. Now import the hearts_data_dictionary.csv file, which provides some information on each variable. Do the data types of the hearts.csv variables align with the description of each variable?
```

