# Lesson 7c: Model evaluation & selection

```{r setup-7c, include=FALSE}
# clean up saved items that may carry over from previous lessons
rm(list = ls())

knitr::opts_chunk$set(
  fig.align = "center",
  fig.height = 3.5,
  fig.width = 6,
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE
)
```

The last couple of lessons gave you a good introduction to building predictive models using the tidymodels construct. This lesson is going to go deeper into the idea of model evaluation & selection. We’ll discuss how to incorporate cross-validation procedures to give you a more robust assessment of model performance. We’ll also discuss the concept of hyperparameter tuning, the bias-variance tradeoff, and how to implement a tuning strategy to find a model the maximizes generalizability.

## Learning objectives

By the end of this lesson you will be able to:

1. Perform cross-validation procedures for more robust model performance assessment.
2. Execute hyperparameter tuning to find optimal model parameter settings.

## Prerequisites

For this lesson we'll use several packages provided via **tidymodels** and we'll use the `ames` housing data.

```{r}
library(tidymodels)

ames <- AmesHousing::make_ames()
```

Let's go ahead and create our train-test split:

```{r 7c-train-test-split}
# create train/test split
set.seed(123)  # for reproducibility
split  <- initial_split(ames, prop = 0.7)
train  <- training(split)
test   <- testing(split)
```


## Resampling & cross-validation

In the previous lessons we split our data into training and testing sets and we assessed the performance of our model on the test set. Unfortunately, there are a few pitfalls to this approach:

1. If our dataset is small, a single test set may not provide realistic expectations of our model’s performance on unseen data.
2. A single test set does not provide us any insight on variability of our model’s performance.
3. Using our test set to drive our model building process can bias our results via data leakage.

Resampling methods provide an alternative approach by allowing us to repeatedly fit a model of interest to parts of the training data and test its performance on other parts of the training data.

```{r 7c-resampling, echo=FALSE, out.width='90%', out.height='90%', fig.cap="Illustration of resampling."}
knitr::include_graphics("images/resampling.svg")
```

```{block, type='note'}
This allows us to train and validate our model entirely on the training data and not touch the test data until we have selected a final “optimal” model.
```

The two most commonly used resampling methods include **k-fold cross-validation** and **bootstrap sampling**. This lesson focuses on using k-fold cross-validation.

## K-fold cross-validation

Cross-validation consists of repeating the procedure such that the training and testing sets are different each time. Generalization performance metrics are collected for each repetition and then aggregated. As a result we can get an estimate of the variability of the model’s generalization performance.

k-fold cross-validation (aka k-fold CV) is a resampling method that randomly divides the training data into *k* groups (aka folds) of approximately equal size.

```{r 7c-cv-diagram, echo=FALSE, fig.cap="Illustration of k-fold sampling across a data sets index.", out.width='90%', out.height='90%'}
knitr::include_graphics("images/cross_validation_diagram.png")
```

The model is fit on $k-1$ folds and then the remaining fold is used to compute model performance.  This procedure is repeated _k_ times; each time, a different fold is treated as the validation set. This process results in _k_ estimates of the generalization error (say $\epsilon_1, \epsilon_2, \dots, \epsilon_k$). Thus, the _k_-fold CV estimate is computed by averaging the _k_ test errors, providing us with an approximation of the error we might expect on unseen data.

```{r 7c-modeling-process-cv-diagram, echo=FALSE, out.width='90%', out.height='90%', fig.cap="Illustration of a 5-fold cross validation procedure."}
knitr::include_graphics("images/cv.png")
```

```{block, type='tip'}
In practice, one typically uses k=5 or k=10. There is no formal rule as to the size of k; however, as k gets larger, the difference between the estimated performance and the true performance to be seen on the test set will decrease.
```

To implement k-fold CV we first make a resampling object. In this example we we create a 10-fold resampling object.

```{r}
kfolds <- vfold_cv(train, v = 10)
```

We can now create our random forest model object and create a workflow object as we did in the previous lesson. To fit our model across our 10-folds we just use `fit_resamples()`.

```{r 7c-rf-cv}
# create our random forest model object
rf_mod <- rand_forest() %>%
   set_mode('regression')

# add model object and our formula spec to a workflow object
rf_wflow <- workflow() %>% 
   add_model(rf_mod) %>%
   add_formula(Sale_Price ~ .)

# fit our model across the 10-fold CV
rf_fit_cv <- rf_wflow %>%
   fit_resamples(kfolds)
```

We can then get our average 10-fold cross validation error with `collect_metrics()`:

```{r 7c-overall-cv-results}
collect_metrics(rf_fit_cv)
```

If we want to see the model evaluation metric (i.e. RMSE) for each fold we just need to unnest the `rf_fit_cv` object. 

```{block, type='tip'}
We have not discussed nested data frames but you can read about them [here](https://tidyr.tidyverse.org/articles/nest.html) 
```

```{r 7c-cv-fold-results}
rf_fit_cv %>% 
   unnest(.metrics) %>%
   filter(.metric == 'rmse')
```

## Hyperparameter tuning

Given two different models (blue line) to the same data (gray dots), which model do you prefer?

```{r 7c-bias-variance-comparison, echo=FALSE, out.width='90%', out.height='90%', fig.cap="Between model A and B, which do you think is better?"}
knitr::include_graphics("images/bias-variance-comparison.png")
```

The image above illustrates the fact that prediction errors can be decomposed into two main subcomponents we care about:

* error due to “bias”
* error due to “variance”

### Bias

Error due to ***bias*** is the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict.

It measures how far off in general a model’s predictions are from the correct value, which provides a sense of how well a model can conform to the underlying structure of the data.

High bias models (i.e. generalized linear models) are rarely affected by the noise introduced by new unseen data

```{r modeling-process-bias-model, echo=FALSE, fig.height=4, fig.width=10, fig.cap="A biased polynomial model fit to a single data set does not capture the underlying non-linear, non-monotonic data structure (left).  Models fit to 25 bootstrapped replicates of the data are underterred by the noise and generates similar, yet still biased, predictions (right).", warning=FALSE, message=FALSE}
# Simulate some nonlinear monotonic data
set.seed(123)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
df <- data.frame(x, y) %>%
  filter(x < 4.5)
# Single model fit
bias_model <- lm(y ~ I(x^3), data = df)
df$predictions <- predict(bias_model, df)
p1 <- ggplot(df, aes(x, y)) +
  geom_point(alpha = .3) +
  geom_line(aes(x, predictions), size = 1.5, color = "dodgerblue") +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) +
  ggtitle("Single biased model fit")
# Bootstrapped model fit
bootstrap_n <- 25
bootstrap_results <- NULL
for(i in seq_len(bootstrap_n)) {
  set.seed(i)  # for reproducibility
  index <- sample(seq_len(nrow(df)), nrow(df), replace = TRUE)
  df_sim <- df[index, ]
  fit <- lm(y ~ I(x^3), data = df_sim)
  df_sim$predictions <- predict(fit, df_sim)
  df_sim$model <- paste0("model", i)
  df_sim$ob <- index
  bootstrap_results <- rbind(bootstrap_results, df_sim)
}
p2 <- ggplot(bootstrap_results, aes(x, predictions, color = model)) +
  geom_line(show.legend = FALSE, size = .5) +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) +
  ggtitle("25 biased models fit to bootstrap samples")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

### Variance

Error due to ***variance*** is the variability of a model prediction for a given data point.

Many models (e.g., k-nearest neighbor, decision trees, gradient boosting machines) are very adaptable and offer extreme flexibility in the patterns that they can fit to. However, these models offer their own problems as they run the risk of overfitting to the training data.

Although you may achieve very good performance on your training data, the model will not automatically generalize well to unseen data.

```{r modeling-process-variance-model, echo=FALSE, fig.height=4, fig.width=10, fig.cap="A high variance _k_-nearest neighbor model fit to a single data set captures the underlying non-linear, non-monotonic data structure well but also overfits to individual data points (left).  Models fit to 25 bootstrapped replicates of the data are deterred by the noise and generate highly variable predictions (right).", warning=FALSE, message=FALSE}
library(caret)

# Simulate some nonlinear monotonic data
set.seed(123)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
df <- data.frame(x, y) %>%
  filter(x < 4.5)
# Single model fit
variance_model <- knnreg(y ~ x, k = 3, data = df)
df$predictions <- predict(variance_model, df)
p1 <- ggplot(df, aes(x, y)) +
  geom_point(alpha = .3) +
  geom_line(aes(x, predictions), size = 1.5, color = "dodgerblue") +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) +
  ggtitle("Single high variance model fit")
# Bootstrapped model fit
bootstrap_n <- 25
bootstrap_results <- NULL
for(i in seq_len(bootstrap_n)) {
  set.seed(i)  # for reproducibility
  index <- sample(seq_len(nrow(df)), nrow(df), replace = TRUE)
  df_sim <- df[index, ]
  fit <- knnreg(y ~ x, k = 3, data = df_sim)
  df_sim$predictions <- predict(fit, df_sim)
  df_sim$model <- paste0("model", i)
  df_sim$ob <- index
  bootstrap_results <- rbind(bootstrap_results, df_sim)
}
p2 <- ggplot(bootstrap_results, aes(x, predictions, color = model)) +
  geom_line(show.legend = FALSE) +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) +
  ggtitle("25 high variance models fit to bootstrap samples")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

```{block, type='note'}
Many high performing models (i.e. random forests, gradient boosting machines, deep learning) are very flexible in the patterns they can conform to due to the many hyperparameters they have. However, this also means they are prone to overfitting (aka can have high variance error).
```

### Hyperparameters

***Hyperparameters*** (aka tuning parameters) are the “knobs to twiddle” to control the complexity of machine learning algorithms and, therefore, the ***bias-variance trade-off***.

Some models have very few hyperparameters. For example in a K-nearest neighbor (KNN) model K (the number of neighbors) is the primary hyperparameter.

```{r modeling-process-knn-options, fig.width=10, fig.height=5, echo=FALSE, fig.cap="_k_-nearest neighbor model with differing values for _k_.", warning=FALSE, message=FALSE}
k_results <- NULL
k <- c(2, 5, 10, 20, 50, 150)

# Simulate some nonlinear monotonic data
set.seed(123)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
df <- data.frame(x, y) %>%
  filter(x < 4.5)

# Fit many different models
for(i in seq_along(k)) {
  df_sim <- df
  fit <- caret::knnreg(y ~ x, k = k[i], data = df_sim)
  df_sim$predictions <- predict(fit, df_sim)
  df_sim$model <- paste0("k = ", stringr::str_pad(k[i], 3, pad = " "))
  k_results <- rbind(k_results, df_sim)
}
ggplot() +
  geom_point(data = df, aes(x, y), alpha = .3) +
  geom_line(data = k_results, aes(x, predictions), color = "dodgerblue", size = 1.5) +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) +
  facet_wrap(~ model)
```

While other models such as gradient boosted machines (GBMs) and deep learning models can have many.

***Hyperparameter tuning*** is the process of screening hyperparameter values (or combinations of hyperparameter values) to find a model that balances bias & variance so that the model generalizes well to unseen data.

Let's illustrate by using decision trees. One of the key hyperparameters in decision trees is the depth of the tree. 

```{block, type='tip'}
This lesson does not dig into the decision tree algorithm but if you want to better understand the hyperparameters for decision trees you can read about them [here](https://bradleyboehmke.github.io/HOML/DT.html#how-deep)
```
 
Say we wanted to assess what happens when we grow the decision tree 5 levels deep. We could do this manually:

```{r 7c-dt-tree-depth5}
# create our decision tree model object
dt_mod <- decision_tree(tree_depth = 5) %>%
   set_mode('regression')

# add model object and our formula spec to a workflow object
dt_wflow <- workflow() %>% 
   add_model(dt_mod) %>%
   add_formula(Sale_Price ~ .)

# fit our model across the 10-fold CV
dt_fit_cv <- dt_wflow %>%
   fit_resamples(kfolds)

# assess results
collect_metrics(dt_fit_cv)
```

But what if we wanted to assess and compare different `tree_depth` values. Moreover, decision trees have another key hyperparameter `cost_complexity`. So what if we wanted to assess a few values of that hyperaparameter in combination with `tree_depth`? Adjusting these values manually would be painstakingly burdensome.

```{block, type = 'note'}
Again, don't worry if you have no idea what these hyperparameters mean. Just realize we want to toggle these values to try find an optimal model.
```

### Full cartesian grid search

For this we could use a ***full cartesian grid search***. A full cartesian grid search takes the values provided for each hyperparameter and assesses every combination. 

First, let's rebuild our decision tree model object; however, this time we'll create a model specification that identifies which hyperparameters we plan to tune.

```{r 7c-dt-tune}
dt_mod <- decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")
```

```{block, type='note'}
Think of `tune()` here as a placeholder. After the tuning process, we will select a single numeric value for each of these hyperparameters. For now, we specify our parsnip model object and identify the hyperparameters we will `tune()`.
```

Next, we create our tuning grid of hyperparameter values we want to assess. The function `grid_regular()` is from the **dials** package. It chooses sensible values to try for each hyperparameter; here, we asked for 3 of each. Since we have two to tune, `grid_regular()` returns $3 \times 3 = 9$ different possible tuning combinations to try.

```{block, type='tip'}
A full cartesian grid search can explode as you add more hyperparameters and values to assess. When this happens its best to start using `grid_random` or `grid_latin_hypercube` to reduce combinatorial explosion and computation time.
```


```{r 7c-create-grid}
dt_grid <- grid_regular(
   cost_complexity(),
   tree_depth(),
   levels = 3
   )

dt_grid
```

Now that we have our tuning grid and model object defined we can:

1. Create our k-fold CV object (5-fold in this example),
2. Add our model to a workflow object and specify the formula,
3. Apply `tune_grid()` to execute our hyperparameter tuning.

```{r 7c-tune-dt}
# 5-fold instead of 10-fold to reduce computation time
kfolds <- vfold_cv(train, v = 5)

# add model object and our formula spec to a workflow object
dt_wflow <- workflow() %>% 
   add_model(dt_mod) %>%
   add_formula(Sale_Price ~ .)

# fit our model across the 5-fold CV
dt_grid_search <- dt_wflow %>%
   tune_grid(
      resamples = kfolds,
      grid = dt_grid
   )
```

We can check out the hyperparameter combinations that resulted in the best model performance with `show_best()`. Here we look at the top 5 models and we can see that the top 4 all tend to perform very similarly. It appears that deeper trees with smaller cost complexity factor tend to perform best on this data set.

```{r 7c-best-results}
dt_grid_search %>%
   show_best(metric = 'rmse')
```

## Finalizing our model

If we are satisfied with our results and we want to use the best hyperparameter values for our best decision tree model, we can select it with `select_best()`:

```{r 7c-best-model}
# select best model based on RMSE metric
best_tree <- dt_grid_search %>%
   select_best(metric = 'rmse')

best_tree
```
We can then update (or “finalize”) our workflow object `dt_wflow` with the values from `select_best()`.

```{r 7c-final-workflow}
final_wflow <- dt_wflow %>%
   finalize_workflow(best_tree)
```

Finally, let’s fit this final model to the training data and use our test data to estimate the model performance we expect to see with new data. We can use the function `last_fit()` with our finalized model; this function fits the finalized model on the full training data set and evaluates the finalized model on the testing data.

```{block, type='note'}
We pass the initial train-test split object we created at the beginning of this lesson to `last_fit`.
```


```{r 7c-final-fit}
final_fit <- final_wflow %>%
   last_fit(split)

final_fit %>%
   collect_predictions() %>%
   rmse(truth = Sale_Price, estimate = .pred)
```

As we can see our test RMSE is less than our CV RMSE. This indicates that we did not overfit during our tuning procedure, which is a good thing.

Perhaps we would also like to understand what variables are important in this final model. We can use the **vip** package to estimate variable importance based on the model’s structure.

```{r 7c-vip}
library(vip)

final_fit %>% 
   extract_fit_parsnip() %>% 
   vip()
```


## Exercises

```{block, type='todo'}
Import the dataset blood_transfusion.csv:

1. The column “Class” contains the target variable. Investigate this variable. Is this a regression or classification problem?
2. Why is it relevant to add a preprocessing step to standardize the features? What `step_xxx()` function would you use to do so?
3. Perform a k-nearest neighbor model on this data with `neighbors = 10`. Be sure to add a preprocessing step to standardize the features.
4. Perform a 5-fold cross validation with the above model workflow. What is your average CV score?
5. Now perform hyperparameter tuning to understand the effect of the parameter `neighbors` on the model score. Assess 10 values for `neighbors` between the range of 1-100. Again, perform a 5-fold cross validation. Which hyperparameter value performed the best and what was the CV score?
```

## Additional resources

This module provided a very high-level introduction to predictive modeling with **tidymodels**.  To build on this knowledge you can find many great resources at https://www.tidymodels.org/.
